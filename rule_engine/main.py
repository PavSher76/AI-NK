import os
import json
import logging
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
import httpx
import openpyxl
from openpyxl.styles import Font, PatternFill
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/app/rule_engine.log')
    ]
)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –º–æ–¥–µ–ª—è–º
model_logger = logging.getLogger('model_requests')
model_logger.setLevel(logging.INFO)

app = FastAPI(title="Rule Engine Service", version="1.0.0")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://norms_user:norms_password@norms-db:5432/norms_db")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
GATEWAY_URL = os.getenv("GATEWAY_URL", "http://gateway:8080")
VLLM_URL = os.getenv("VLLM_URL", "http://vllm:8000")  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ VLLM

class RuleEngine:
    def __init__(self):
        self.db_conn = None
        self.qdrant_client = None
        self.connect_databases()
        self.load_rules()
    
    def connect_databases(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # PostgreSQL
            self.db_conn = psycopg2.connect(POSTGRES_URL)
            logger.info("Connected to PostgreSQL")
            
            # Qdrant
            self.qdrant_client = qdrant_client.QdrantClient(
                host=QDRANT_HOST,
                port=QDRANT_PORT
            )
            logger.info("Connected to Qdrant")
            
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            raise
    
    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        self.rules = {
            # –ü—Ä–∞–≤–∏–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            "formatting": {
                "document_title": {
                    "pattern": r"^[–ê-–Ø–Å][–ê-–Ø–Å\s\-\(\)\d]+$",
                    "description": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã",
                    "severity": 3
                },
                "page_numbers": {
                    "pattern": r"^\d+$",
                    "description": "–ù–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏",
                    "severity": 2
                },
                "date_format": {
                    "pattern": r"^\d{2}\.\d{2}\.\d{4}$",
                    "description": "–î–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
                    "severity": 4
                }
            },
            
            # –ü—Ä–∞–≤–∏–ª–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            "content": {
                "required_fields": {
                    "fields": ["title", "date", "author", "approver"],
                    "description": "–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è",
                    "severity": 5
                },
                "text_length": {
                    "min_length": 10,
                    "description": "–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 10 —Å–∏–º–≤–æ–ª–æ–≤",
                    "severity": 2
                }
            },
            
            # –ü—Ä–∞–≤–∏–ª–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–º
            "compliance": {
                "gost_references": {
                    "pattern": r"–ì–û–°–¢\s+[–†]?\d+\.\d+-\d+",
                    "description": "–°—Å—ã–ª–∫–∏ –Ω–∞ –ì–û–°–¢ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ",
                    "severity": 4
                },
                "sp_references": {
                    "pattern": r"–°–ü\s+\d+\.\d+\.\d+",
                    "description": "–°—Å—ã–ª–∫–∏ –Ω–∞ –°–ü –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ",
                    "severity": 4
                }
            },
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
            "technical": {
                "units": {
                    "allowed_units": ["–º–º", "—Å–º", "–º", "–∫–≥", "—Ç", "–ª", "–º¬≥", "–º¬≤", "¬∞C", "–í—Ç", "–ê", "–í"],
                    "description": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è",
                    "severity": 3
                },
                "dimensions": {
                    "pattern": r"\d+\s*(–º–º|—Å–º|–º|–∫–≥|—Ç|–ª|–º¬≥|–º¬≤|¬∞C|–í—Ç|–ê|–í)",
                    "description": "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è",
                    "severity": 3
                }
            }
        }
    
    def check_formatting_rules(self, text: str, element_type: str) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        findings = []
        
        for rule_name, rule in self.rules["formatting"].items():
            if rule_name == "document_title" and element_type == "text":
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                lines = text.split('\n')
                for i, line in enumerate(lines[:3]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏
                    if line.strip() and not re.match(rule["pattern"], line.strip()):
                        findings.append({
                            "rule": rule_name,
                            "category": "formatting",
                            "severity": rule["severity"],
                            "title": "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞",
                            "description": rule["description"],
                            "location": f"–°—Ç—Ä–æ–∫–∞ {i+1}: {line.strip()}",
                            "recommendation": "–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"
                        })
            
            elif rule_name == "date_format":
                # –ü–æ–∏—Å–∫ –¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç–µ
                dates = re.findall(r'\d{2}\.\d{2}\.\d{4}', text)
                for date in dates:
                    if not re.match(rule["pattern"], date):
                        findings.append({
                            "rule": rule_name,
                            "category": "formatting",
                            "severity": rule["severity"],
                            "title": "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã",
                            "description": rule["description"],
                            "location": f"–î–∞—Ç–∞: {date}",
                            "recommendation": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç –î–î.–ú–ú.–ì–ì–ì–ì"
                        })
        
        return findings
    
    def check_content_rules(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        findings = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        required_fields = self.rules["content"]["required_fields"]["fields"]
        missing_fields = []
        
        for field in required_fields:
            if field not in metadata or not metadata[field]:
                missing_fields.append(field)
        
        if missing_fields:
            findings.append({
                "rule": "required_fields",
                "category": "content",
                "severity": self.rules["content"]["required_fields"]["severity"],
                "title": "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è",
                "description": self.rules["content"]["required_fields"]["description"],
                "location": f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing_fields)}",
                "recommendation": "–î–æ–±–∞–≤—å—Ç–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç"
            })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        if len(text.strip()) < self.rules["content"]["text_length"]["min_length"]:
            findings.append({
                "rule": "text_length",
                "category": "content",
                "severity": self.rules["content"]["text_length"]["severity"],
                "title": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞",
                "description": self.rules["content"]["text_length"]["description"],
                "location": f"–î–ª–∏–Ω–∞: {len(text.strip())} —Å–∏–º–≤–æ–ª–æ–≤",
                "recommendation": "–î–æ–ø–æ–ª–Ω–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"
            })
        
        return findings
    
    def check_compliance_rules(self, text: str) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–º"""
        findings = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ì–û–°–¢
        gost_refs = re.findall(r'–ì–û–°–¢\s+[–†]?\d+\.\d+-\d+', text)
        for ref in gost_refs:
            if not re.match(self.rules["compliance"]["gost_references"]["pattern"], ref):
                findings.append({
                    "rule": "gost_references",
                    "category": "compliance",
                    "severity": self.rules["compliance"]["gost_references"]["severity"],
                    "title": "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –ì–û–°–¢",
                    "description": self.rules["compliance"]["gost_references"]["description"],
                    "location": f"–°—Å—ã–ª–∫–∞: {ref}",
                    "recommendation": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ì–û–°–¢"
                })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –°–ü
        sp_refs = re.findall(r'–°–ü\s+\d+\.\d+\.\d+', text)
        for ref in sp_refs:
            if not re.match(self.rules["compliance"]["sp_references"]["pattern"], ref):
                findings.append({
                    "rule": "sp_references",
                    "category": "compliance",
                    "severity": self.rules["compliance"]["sp_references"]["severity"],
                    "title": "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –°–ü",
                    "description": self.rules["compliance"]["sp_references"]["description"],
                    "location": f"–°—Å—ã–ª–∫–∞: {ref}",
                    "recommendation": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –°–ü"
                })
        
        return findings
    
    def check_technical_rules(self, text: str) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª"""
        findings = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è
        allowed_units = self.rules["technical"]["units"]["allowed_units"]
        unit_pattern = r'\b(\d+)\s*([–∞-—è—ë–ê-–Ø–Å]+)\b'
        units_found = re.findall(unit_pattern, text)
        
        for value, unit in units_found:
            if unit not in allowed_units:
                findings.append({
                    "rule": "units",
                    "category": "technical",
                    "severity": self.rules["technical"]["units"]["severity"],
                    "title": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è",
                    "description": self.rules["technical"]["units"]["description"],
                    "location": f"–ï–¥–∏–Ω–∏—Ü–∞: {unit}",
                    "recommendation": f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã: {', '.join(allowed_units)}"
                })
        
        return findings
    
    async def analyze_document(self, document_id: int, auth_token: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = datetime.now()
        logger.info(f"üöÄ [FORCE_START] –ù–∞—á–∞–ª–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üìã [FORCE_START] –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, element_type, element_content, page_number, confidence_score
                    FROM extracted_elements
                    WHERE uploaded_document_id = %s
                    ORDER BY page_number, id
                """, (document_id,))
                elements = cursor.fetchall()
            
            if not elements:
                logger.error(f"‚ùå [FORCE_START] –î–æ–∫—É–º–µ–Ω—Ç ID: {document_id} - —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                raise HTTPException(status_code=404, detail="Document elements not found")
            
            logger.info(f"‚úÖ [FORCE_START] –ü–æ–ª—É—á–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
            all_findings = []
            logger.info(f"üîç [FORCE_START] –ù–∞—á–∞–ª–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            
            for i, element in enumerate(elements):
                text = element["element_content"]
                element_type = element["element_type"]
                
                logger.debug(f"üìÑ [FORCE_START] –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ {i+1}/{len(elements)}: —Ç–∏–ø={element_type}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞={element['page_number']}")
                
                # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
                findings = []
                findings.extend(self.check_formatting_rules(text, element_type))
                findings.extend(self.check_content_rules(text, {}))
                findings.extend(self.check_compliance_rules(text))
                findings.extend(self.check_technical_rules(text))
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏
                for finding in findings:
                    finding["element_id"] = element["id"]
                    finding["page_number"] = element["page_number"]
                    finding["element_type"] = element_type
                
                all_findings.extend(findings)
                logger.debug(f"üìä [FORCE_START] –≠–ª–µ–º–µ–Ω—Ç {i+1}: –Ω–∞–π–¥–µ–Ω–æ {len(findings)} –ø—Ä–æ–±–ª–µ–º")
            
            logger.info(f"‚úÖ [FORCE_START] –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã: {len(all_findings)} –ø—Ä–æ–±–ª–µ–º")
            
            # LLM –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
            logger.info(f"ü§ñ [FORCE_START] –ù–∞—á–∞–ª–æ LLM –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            llm_findings = await self.llm_analysis(elements, auth_token)
            all_findings.extend(llm_findings)
            logger.info(f"‚úÖ [FORCE_START] LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(llm_findings)} –ø—Ä–æ–±–ª–µ–º")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            logger.info(f"üíæ [FORCE_START] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            result_id = self.save_analysis_results(document_id, all_findings, auth_token)
            
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            critical_count = len([f for f in all_findings if f["severity"] >= 4])
            warning_count = len([f for f in all_findings if f["severity"] == 3])
            info_count = len([f for f in all_findings if f["severity"] <= 2])
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"üéâ [FORCE_START] –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"üìà [FORCE_START] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –≤—Å–µ–≥–æ={len(all_findings)}, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö={critical_count}, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π={warning_count}, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö={info_count}")
            logger.info(f"‚è±Ô∏è [FORCE_START] –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            return {
                "analysis_id": result_id,
                "total_findings": len(all_findings),
                "critical_findings": critical_count,
                "warning_findings": warning_count,
                "info_findings": info_count,
                "execution_time": execution_time,
                "findings": all_findings
            }
            
        except HTTPException:
            logger.error(f"‚ùå [FORCE_START] HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            raise
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå [FORCE_START] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID: {document_id}")
            logger.error(f"‚ùå [FORCE_START] –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
            logger.error(f"‚ùå [FORCE_START] –°–æ–æ–±—â–µ–Ω–∏–µ –æ—à–∏–±–∫–∏: {str(e)}")
            logger.error(f"‚ùå [FORCE_START] –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–æ –æ—à–∏–±–∫–∏: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
            raise HTTPException(status_code=500, detail=str(e))
    
    async def llm_analysis(self, elements: List[Dict], auth_token: str) -> List[Dict[str, Any]]:
        """LLM –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫"""
        findings = []
        start_time = datetime.now()
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            full_text = "\n".join([elem["element_content"] for elem in elements])
            text_length = len(full_text)
            
            model_logger.info(f"ü§ñ [LLM_REQUEST] –ù–∞—á–∞–ª–æ LLM –∞–Ω–∞–ª–∏–∑–∞: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {text_length} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = f"""–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:

–î–æ–∫—É–º–µ–Ω—Ç:
{full_text}

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:
1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ì–û–°–¢, –°–ü, –¢–† –¢–°
2. –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π
3. –ü–æ–ª–Ω–æ—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
4. –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
5. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º:
- –¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã (violation/warning/recommendation/info)
- –£—Ä–æ–≤–µ–Ω—å –≤–∞–∂–Ω–æ—Å—Ç–∏ (1-5, –≥–¥–µ 5 - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏)
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è (formatting/content/compliance/technical)
- –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é
- –°—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏: type, severity, category, title, description, recommendation, norm_reference"""

            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
            model_logger.info(f"üì§ [LLM_REQUEST] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ llama3:8b")
            model_logger.debug(f"üì§ [LLM_REQUEST] –ü—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {prompt[:500]}...")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{VLLM_URL}/v1/chat/completions",
                    headers={"Authorization": f"Bearer {auth_token}"},
                    json={
                        "model": "llama3:8b",
                        "messages": [
                            {
                                "role": "system",
                                "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": 0.3,
                        "max_tokens": 2000
                    }
                )
                
                response_time = (datetime.now() - start_time).total_seconds()
                
                if response.status_code == 200:
                    data = response.json()
                    llm_response = data["choices"][0]["message"]["content"]
                    
                    model_logger.info(f"‚úÖ [LLM_RESPONSE] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
                    model_logger.debug(f"üì• [LLM_RESPONSE] –û—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {llm_response[:500]}...")
                    
                    # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                    try:
                        llm_findings = json.loads(llm_response)
                        for finding in llm_findings:
                            finding["rule"] = "llm_analysis"
                            finding["element_id"] = None
                            finding["page_number"] = None
                            finding["element_type"] = "llm"
                        findings.extend(llm_findings)
                        
                        model_logger.info(f"‚úÖ [LLM_RESPONSE] –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Ç–≤–µ—Ç: {len(llm_findings)} findings")
                        
                    except json.JSONDecodeError as e:
                        model_logger.error(f"‚ùå [LLM_RESPONSE] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                        model_logger.error(f"‚ùå [LLM_RESPONSE] –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç: {llm_response}")
                        logger.warning("Failed to parse LLM response as JSON")
                else:
                    model_logger.error(f"‚ùå [LLM_RESPONSE] HTTP –æ—à–∏–±–∫–∞ {response.status_code}: {response.text}")
                
        except Exception as e:
            response_time = (datetime.now() - start_time).total_seconds()
            model_logger.error(f"‚ùå [LLM_ERROR] –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞ –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥: {type(e).__name__}: {str(e)}")
            logger.error(f"LLM analysis error: {e}")
        
        return findings
    
    def save_analysis_results(self, document_id: int, findings: List[Dict], auth_token: str) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                cursor.execute("""
                    INSERT INTO norm_control_results 
                    (uploaded_document_id, analysis_type, model_used, total_findings, 
                     critical_findings, warning_findings, info_findings, analysis_status)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,
                    "full",
                    "rule_engine+llama3:8b",
                    len(findings),
                    len([f for f in findings if f["severity"] >= 4]),
                    len([f for f in findings if f["severity"] == 3]),
                    len([f for f in findings if f["severity"] <= 2]),
                    "completed"
                ))
                
                result_id = cursor.fetchone()["id"]
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ findings
                for finding in findings:
                    cursor.execute("""
                        INSERT INTO findings 
                        (norm_control_result_id, finding_type, severity_level, category, 
                         title, description, recommendation, element_reference, rule_applied)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        result_id,
                        finding.get("type", "violation"),
                        finding.get("severity", 3),
                        finding.get("category", "general"),
                        finding.get("title", ""),
                        finding.get("description", ""),
                        finding.get("recommendation", ""),
                        json.dumps({
                            "element_id": finding.get("element_id"),
                            "page_number": finding.get("page_number"),
                            "element_type": finding.get("element_type")
                        }),
                        finding.get("rule", "unknown")
                    ))
                
                self.db_conn.commit()
                return result_id
                
        except Exception as e:
            self.db_conn.rollback()
            logger.error(f"Save results error: {e}")
            raise

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä rule engine
rule_engine = RuleEngine()

@app.post("/analyze/{document_id}")
async def analyze_document_endpoint(document_id: int, auth_token: str):
    """–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    return await rule_engine.analyze_document(document_id, auth_token)

@app.get("/metrics")
async def get_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        with rule_engine.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_results,
                    COUNT(CASE WHEN analysis_status = 'completed' THEN 1 END) as completed_analyses,
                    COUNT(CASE WHEN analysis_status = 'pending' THEN 1 END) as pending_analyses,
                    COUNT(CASE WHEN analysis_status = 'error' THEN 1 END) as error_analyses,
                    COUNT(CASE WHEN analysis_type = 'full' THEN 1 END) as full_analyses,
                    COUNT(CASE WHEN analysis_type = 'quick' THEN 1 END) as quick_analyses,
                    COUNT(DISTINCT model_used) as unique_models,
                    SUM(total_findings) as total_findings,
                    SUM(critical_findings) as critical_findings,
                    SUM(warning_findings) as warning_findings,
                    SUM(info_findings) as info_findings,
                    AVG(total_findings) as avg_findings_per_analysis,
                    AVG(critical_findings) as avg_critical_per_analysis,
                    AVG(warning_findings) as avg_warning_per_analysis,
                    AVG(info_findings) as avg_info_per_analysis
                FROM norm_control_results
            """)
            analysis_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ findings
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_findings,
                    COUNT(CASE WHEN severity_level >= 4 THEN 1 END) as critical_findings,
                    COUNT(CASE WHEN severity_level = 3 THEN 1 END) as warning_findings,
                    COUNT(CASE WHEN severity_level <= 2 THEN 1 END) as info_findings,
                    COUNT(DISTINCT category) as unique_categories,
                    COUNT(DISTINCT rule_applied) as unique_rules,
                    COUNT(DISTINCT finding_type) as unique_finding_types,
                    AVG(severity_level) as avg_severity_level,
                    AVG(confidence_score) as avg_confidence_score
                FROM findings
            """)
            findings_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º findings
            cursor.execute("""
                SELECT 
                    category,
                    COUNT(*) as count,
                    AVG(severity_level) as avg_severity,
                    COUNT(CASE WHEN severity_level >= 4 THEN 1 END) as critical_count
                FROM findings 
                GROUP BY category 
                ORDER BY count DESC
            """)
            category_stats = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
            cursor.execute("""
                SELECT 
                    rule_applied,
                    COUNT(*) as count,
                    AVG(severity_level) as avg_severity,
                    COUNT(CASE WHEN severity_level >= 4 THEN 1 END) as critical_count
                FROM findings 
                WHERE rule_applied IS NOT NULL
                GROUP BY rule_applied 
                ORDER BY count DESC
            """)
            rule_stats = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞)
            cursor.execute("""
                SELECT 
                    COUNT(*) as analyses_last_24h,
                    COUNT(CASE WHEN analysis_status = 'completed' THEN 1 END) as completed_last_24h,
                    AVG(total_findings) as avg_findings_last_24h
                FROM norm_control_results 
                WHERE created_at >= NOW() - INTERVAL '24 hours'
            """)
            time_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º
            cursor.execute("""
                SELECT 
                    model_used,
                    COUNT(*) as usage_count,
                    AVG(total_findings) as avg_findings,
                    AVG(critical_findings) as avg_critical
                FROM norm_control_results 
                WHERE model_used IS NOT NULL
                GROUP BY model_used 
                ORDER BY usage_count DESC
            """)
            model_stats = cursor.fetchall()
            
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics_lines = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
        metrics_lines.append(f"# HELP rule_engine_analysis_total Total number of analyses")
        metrics_lines.append(f"# TYPE rule_engine_analysis_total counter")
        metrics_lines.append(f"rule_engine_analysis_total {analysis_stats['total_results'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_analysis_completed Completed analyses")
        metrics_lines.append(f"# TYPE rule_engine_analysis_completed counter")
        metrics_lines.append(f"rule_engine_analysis_completed {analysis_stats['completed_analyses'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_analysis_pending Pending analyses")
        metrics_lines.append(f"# TYPE rule_engine_analysis_pending counter")
        metrics_lines.append(f"rule_engine_analysis_pending {analysis_stats['pending_analyses'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_analysis_error Error analyses")
        metrics_lines.append(f"# TYPE rule_engine_analysis_error counter")
        metrics_lines.append(f"rule_engine_analysis_error {analysis_stats['error_analyses'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–∏–∑–∞
        metrics_lines.append(f"# HELP rule_engine_analysis_by_type Analyses by type")
        metrics_lines.append(f"# TYPE rule_engine_analysis_by_type counter")
        metrics_lines.append(f'rule_engine_analysis_by_type{{type="full"}} {analysis_stats["full_analyses"] or 0}')
        metrics_lines.append(f'rule_engine_analysis_by_type{{type="quick"}} {analysis_stats["quick_analyses"] or 0}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ findings
        metrics_lines.append(f"# HELP rule_engine_findings_total Total findings")
        metrics_lines.append(f"# TYPE rule_engine_findings_total counter")
        metrics_lines.append(f"rule_engine_findings_total {findings_stats['total_findings'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_findings_critical Critical findings")
        metrics_lines.append(f"# TYPE rule_engine_findings_critical counter")
        metrics_lines.append(f"rule_engine_findings_critical {findings_stats['critical_findings'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_findings_warning Warning findings")
        metrics_lines.append(f"# TYPE rule_engine_findings_warning counter")
        metrics_lines.append(f"rule_engine_findings_warning {findings_stats['warning_findings'] or 0}")
        
        metrics_lines.append(f"# HELP rule_engine_findings_info Info findings")
        metrics_lines.append(f"# TYPE rule_engine_findings_info counter")
        metrics_lines.append(f"rule_engine_findings_info {findings_stats['info_findings'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for cat in category_stats:
            metrics_lines.append(f'rule_engine_findings_by_category{{category="{cat["category"]}"}} {cat["count"]}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
        for rule in rule_stats:
            metrics_lines.append(f'rule_engine_findings_by_rule{{rule="{rule["rule_applied"]}"}} {rule["count"]}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–æ–¥–µ–ª—è–º
        for model in model_stats:
            metrics_lines.append(f'rule_engine_model_usage{{model="{model["model_used"]}"}} {model["usage_count"]}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        metrics_lines.append(f"# HELP rule_engine_analyses_last_24h Analyses in last 24 hours")
        metrics_lines.append(f"# TYPE rule_engine_analyses_last_24h counter")
        metrics_lines.append(f"rule_engine_analyses_last_24h {time_stats['analyses_last_24h'] or 0}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        from fastapi.responses import Response
        return Response(
            content="\n".join(metrics_lines),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
        
    except Exception as e:
        logger.error(f"Get metrics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ PostgreSQL
        with rule_engine.db_conn.cursor() as cursor:
            cursor.execute("SELECT 1")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        rule_engine.qdrant_client.get_collections()
        
        return {"status": "healthy", "timestamp": datetime.now().isoformat()}
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {"status": "unhealthy", "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
