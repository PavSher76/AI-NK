import logging
import requests
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
chat_logger = logging.getLogger("chat")

class VLLMChatService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —á–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º vLLM –∏ GPT-OSS"""
    
    def __init__(self, vllm_url: str = "http://localhost:8000"):
        self.vllm_url = vllm_url
        self.model_name = "gpt-oss"
        logger.info(f"ü§ñ [VLLM_CHAT] Initialized with {self.model_name} at {vllm_url}")
    
    def generate_response(self, message: str, history: List[Dict[str, str]] = None, 
                         system_prompt: str = None, max_tokens: int = 2048) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º vLLM"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            if system_prompt:
                prompt = f"{system_prompt}\n\n"
            else:
                prompt = "–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.\n\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
            if history:
                for entry in history:
                    if entry.get('role') == 'user':
                        prompt += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {entry.get('content', '')}\n"
                    elif entry.get('role') == 'assistant':
                        prompt += f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {entry.get('content', '')}\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
            prompt += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {message}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ vLLM
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "stop": ["–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "\n\n"],
                "stream": False
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ vLLM
            response = requests.post(
                f"{self.vllm_url}/v1/completions",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("choices", [{}])[0].get("text", "").strip()
                
                if generated_text:
                    chat_logger.info(f"‚úÖ [CHAT] Generated response for message: '{message[:100]}...'")
                    
                    return {
                        "status": "success",
                        "response": generated_text,
                        "model": self.model_name,
                        "tokens_used": result.get("usage", {}).get("total_tokens", 0),
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    raise ValueError("Empty response received from vLLM")
            else:
                raise Exception(f"vLLM API error: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå [CHAT] Error generating response: {e}")
            return {
                "status": "error",
                "response": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}",
                "model": self.model_name,
                "tokens_used": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    def generate_ntd_consultation_response(self, message: str, search_results: List[Dict[str, Any]], 
                                         user_id: str, history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            logger.info(f"üí¨ [NTD_CONSULTATION] Generating consultation response for: '{message[:100]}...'")
            
            if not search_results:
                return {
                    "status": "success",
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "timestamp": datetime.now().isoformat()
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            context = "–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n"
            for i, result in enumerate(search_results[:3], 1):  # –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                context += f"{i}. {result.get('document_title', 'Unknown')}\n"
                context += f"   –†–∞–∑–¥–µ–ª: {result.get('section', 'Unknown')}\n"
                context += f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {result.get('content', '')[:300]}...\n\n"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ
3. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
4. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
5. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–Ω–æ

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message}"""
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            chat_response = self.generate_response(
                message=message,
                history=history,
                system_prompt=system_prompt,
                max_tokens=2048
            )
            
            if chat_response["status"] == "success":
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = []
                for result in search_results[:3]:
                    source = {
                        'document_code': result.get('code', ''),
                        'document_title': result.get('document_title', ''),
                        'section': result.get('section', ''),
                        'page': result.get('page', 1),
                        'content_preview': result.get('content', '')[:200] + "..." if len(result.get('content', '')) > 200 else result.get('content', ''),
                        'relevance_score': result.get('score', 0)
                    }
                    sources.append(source)
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                top_score = search_results[0].get('score', 0) if search_results else 0
                confidence = min(top_score, 1.0) if top_score > 0 else 0.0
                
                return {
                    "status": "success",
                    "response": chat_response["response"],
                    "sources": sources,
                    "confidence": confidence,
                    "documents_used": len(search_results),
                    "model": self.model_name,
                    "tokens_used": chat_response.get("tokens_used", 0),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return chat_response
                
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error generating consultation response: {e}")
            return {
                "status": "error",
                "response": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "documents_used": 0,
                "model": self.model_name,
                "tokens_used": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è vLLM —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM
            response = requests.get(f"{self.vllm_url}/v1/models", timeout=10)
            
            if response.status_code == 200:
                models = response.json().get("data", [])
                gpt_oss_available = any("gpt-oss" in model.get("id", "") for model in models)
                
                return {
                    "status": "healthy" if gpt_oss_available else "degraded",
                    "vllm_url": self.vllm_url,
                    "model_available": gpt_oss_available,
                    "available_models": [model.get("id") for model in models],
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "unhealthy",
                    "vllm_url": self.vllm_url,
                    "error": f"HTTP {response.status_code}",
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå [HEALTH] vLLM health check error: {e}")
            return {
                "status": "unhealthy",
                "vllm_url": self.vllm_url,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ vLLM —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
            response = requests.get(f"{self.vllm_url}/v1/models", timeout=10)
            
            if response.status_code == 200:
                models = response.json().get("data", [])
                
                stats = {
                    "total_models": len(models),
                    "models": [model.get("id") for model in models],
                    "gpt_oss_available": any("gpt-oss" in model.get("id", "") for model in models),
                    "vllm_url": self.vllm_url,
                    "timestamp": datetime.now().isoformat()
                }
                
                return stats
            else:
                return {
                    "error": f"Failed to get models: HTTP {response.status_code}",
                    "vllm_url": self.vllm_url,
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå [STATS] Error getting vLLM stats: {e}")
            return {
                "error": str(e),
                "vllm_url": self.vllm_url,
                "timestamp": datetime.now().isoformat()
            }
