"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import logging
import hashlib
import os
import re
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import fitz  # PyMuPDF
import docx
import pandas as pd
from pathlib import Path

from .models import ArchiveDocument, DocumentSection, DocumentType
from .config import PROJECT_CODE_PATTERNS, CHUNK_SIZE, CHUNK_OVERLAP

logger = logging.getLogger(__name__)

class ArchiveDocumentProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.project_code_patterns = [re.compile(pattern) for pattern in PROJECT_CODE_PATTERNS]
    
    def extract_project_code(self, filename: str, content: str = "") -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –®–ò–§–† –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            for pattern in self.project_code_patterns:
                match = pattern.search(filename)
                if match:
                    logger.info(f"üîç [EXTRACT_PROJECT_CODE] Found project code in filename: {match.group()}")
                    return match.group()
            
            # –ó–∞—Ç–µ–º –∏—â–µ–º –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if content:
                for pattern in self.project_code_patterns:
                    match = pattern.search(content)
                    if match:
                        logger.info(f"üîç [EXTRACT_PROJECT_CODE] Found project code in content: {match.group()}")
                        return match.group()
            
            logger.warning(f"‚ö†Ô∏è [EXTRACT_PROJECT_CODE] No project code found in {filename}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PROJECT_CODE] Error extracting project code: {e}")
            return None
    
    def detect_document_type(self, filename: str, content: str = "") -> DocumentType:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        try:
            filename_lower = filename.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            if any(keyword in filename_lower for keyword in ['–ø–¥', '–ø—Ä–æ–µ–∫—Ç', 'project']):
                return DocumentType.PD
            elif any(keyword in filename_lower for keyword in ['—Ä–¥', '—Ä–∞–±–æ—á', 'working']):
                return DocumentType.RD
            elif any(keyword in filename_lower for keyword in ['—Ç—ç–æ', 'teo', '–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ']):
                return DocumentType.TEO
            elif any(keyword in filename_lower for keyword in ['—á–µ—Ä—Ç–µ–∂', 'drawing', 'dwg']):
                return DocumentType.DRAWING
            elif any(keyword in filename_lower for keyword in ['—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', 'specification', 'spec']):
                return DocumentType.SPECIFICATION
            elif any(keyword in filename_lower for keyword in ['—Ä–∞—Å—á–µ—Ç', 'calculation', 'calc']):
                return DocumentType.CALCULATION
            elif any(keyword in filename_lower for keyword in ['–æ—Ç—á–µ—Ç', 'report']):
                return DocumentType.REPORT
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            if content:
                content_lower = content.lower()
                if any(keyword in content_lower for keyword in ['–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'project documentation']):
                    return DocumentType.PD
                elif any(keyword in content_lower for keyword in ['—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'working documentation']):
                    return DocumentType.RD
                elif any(keyword in content_lower for keyword in ['—Ç–µ—Ö–Ω–∏–∫–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ', 'teo']):
                    return DocumentType.TEO
                elif any(keyword in content_lower for keyword in ['—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', 'specification']):
                    return DocumentType.SPECIFICATION
                elif any(keyword in content_lower for keyword in ['—Ä–∞—Å—á–µ—Ç', 'calculation']):
                    return DocumentType.CALCULATION
                elif any(keyword in content_lower for keyword in ['–æ—Ç—á–µ—Ç', 'report']):
                    return DocumentType.REPORT
            
            logger.info(f"üîç [DETECT_DOCUMENT_TYPE] Using default type for {filename}")
            return DocumentType.OTHER
            
        except Exception as e:
            logger.error(f"‚ùå [DETECT_DOCUMENT_TYPE] Error detecting document type: {e}")
            return DocumentType.OTHER
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        try:
            text_content = ""
            doc = fitz.open(file_path)
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text_content += page.get_text()
            
            doc.close()
            logger.info(f"‚úÖ [EXTRACT_PDF] Extracted text from PDF: {len(text_content)} characters")
            return text_content
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PDF] Error extracting text from PDF: {e}")
            return ""
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        try:
            doc = docx.Document(file_path)
            text_content = ""
            
            for paragraph in doc.paragraphs:
                text_content += paragraph.text + "\n"
            
            logger.info(f"‚úÖ [EXTRACT_DOCX] Extracted text from DOCX: {len(text_content)} characters")
            return text_content
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCX] Error extracting text from DOCX: {e}")
            return ""
    
    def extract_text_from_xlsx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ XLSX —Ñ–∞–π–ª–∞"""
        try:
            text_content = ""
            df = pd.read_excel(file_path, sheet_name=None)
            
            for sheet_name, sheet_df in df.items():
                text_content += f"–õ–∏—Å—Ç: {sheet_name}\n"
                text_content += sheet_df.to_string(index=False) + "\n\n"
            
            logger.info(f"‚úÖ [EXTRACT_XLSX] Extracted text from XLSX: {len(text_content)} characters")
            return text_content
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_XLSX] Error extracting text from XLSX: {e}")
            return ""
    
    def extract_text_from_file(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ —Ç–∏–ø—É"""
        try:
            file_extension = Path(file_path).suffix.lower()
            
            if file_extension == '.pdf':
                return self.extract_text_from_pdf(file_path)
            elif file_extension == '.docx':
                return self.extract_text_from_docx(file_path)
            elif file_extension == '.xlsx':
                return self.extract_text_from_xlsx(file_path)
            elif file_extension == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                logger.warning(f"‚ö†Ô∏è [EXTRACT_TEXT] Unsupported file type: {file_extension}")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_TEXT] Error extracting text from file: {e}")
            return ""
    
    def extract_sections_from_text(self, text: str, document_id: int) -> List[DocumentSection]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            sections = []
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã
            paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            
            current_section = None
            section_number = 1
            page_number = 1
            
            for i, paragraph in enumerate(paragraphs):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∞–±–∑–∞—Ü –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ä–∞–∑–¥–µ–ª–∞
                is_header = self._is_section_header(paragraph)
                
                if is_header:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–¥–µ–ª
                    if current_section:
                        sections.append(current_section)
                    
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
                    section_title = paragraph.strip()
                    section_number_str = self._extract_section_number(section_title)
                    
                    current_section = DocumentSection(
                        archive_document_id=document_id,
                        section_number=section_number_str or str(section_number),
                        section_title=section_title,
                        section_content="",
                        page_number=page_number,
                        section_type="header",
                        importance_level=self._determine_importance_level(section_title)
                    )
                    section_number += 1
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫ —Ç–µ–∫—É—â–µ–º—É —Ä–∞–∑–¥–µ–ª—É
                    if current_section:
                        if current_section.section_content:
                            current_section.section_content += "\n" + paragraph
                        else:
                            current_section.section_content = paragraph
                    else:
                        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–¥–µ–ª –¥–ª—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        current_section = DocumentSection(
                            archive_document_id=document_id,
                            section_number=str(section_number),
                            section_title="–í–≤–µ–¥–µ–Ω–∏–µ",
                            section_content=paragraph,
                            page_number=page_number,
                            section_type="text",
                            importance_level=1
                        )
                        section_number += 1
                
                # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ 50 —Å—Ç—Ä–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
                if i % 50 == 0:
                    page_number += 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
            if current_section:
                sections.append(current_section)
            
            logger.info(f"‚úÖ [EXTRACT_SECTIONS] Extracted {len(sections)} sections from document {document_id}")
            return sections
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_SECTIONS] Error extracting sections: {e}")
            return []
    
    def _is_section_header(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ä–∞–∑–¥–µ–ª–∞"""
        # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        text = text.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞
        if re.match(r'^\d+\.?\s+', text):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        header_keywords = [
            '–≤–≤–µ–¥–µ–Ω–∏–µ', '–≤–≤–µ–¥–µ–Ω–∏–µ', '–æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è',
            '–º–µ—Ç–æ–¥—ã –∏—Å–ø—ã—Ç–∞–Ω–∏–π', '–ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–µ–º–∫–∏', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ö—Ä–∞–Ω–µ–Ω–∏–µ',
            '–≥–∞—Ä–∞–Ω—Ç–∏–∏ –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'
        ]
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in header_keywords)
    
    def _extract_section_number(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        match = re.match(r'^(\d+(?:\.\d+)*)', text.strip())
        return match.group(1) if match else None
    
    def _determine_importance_level(self, text: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–∞"""
        text_lower = text.lower()
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        if any(keyword in text_lower for keyword in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã', '–∞–≤–∞—Ä–∏–π–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏']):
            return 5
        
        # –û—á–µ–Ω—å –≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        if any(keyword in text_lower for keyword in ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–º–µ—Ç–æ–¥—ã –∏—Å–ø—ã—Ç–∞–Ω–∏–π', '–≥–∞—Ä–∞–Ω—Ç–∏–∏']):
            return 4
        
        # –í–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        if any(keyword in text_lower for keyword in ['–æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è', '–ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–µ–º–∫–∏', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ']):
            return 3
        
        # –°—Ä–µ–¥–Ω–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
        if any(keyword in text_lower for keyword in ['–≤–≤–µ–¥–µ–Ω–∏–µ', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞']):
            return 2
        
        # –ù–∏–∑–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
        return 1
    
    def create_document_chunks(self, text: str, document_id: int) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            chunks = []
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = re.split(r'[.!?]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            current_chunk = ""
            chunk_number = 1
            
            for sentence in sentences:
                # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                if len(current_chunk) + len(sentence) > CHUNK_SIZE:
                    if current_chunk:
                        chunks.append({
                            'chunk_id': f"{document_id}_{chunk_number}",
                            'content': current_chunk.strip(),
                            'document_id': document_id,
                            'chunk_number': chunk_number
                        })
                        chunk_number += 1
                    
                    current_chunk = sentence
                else:
                    if current_chunk:
                        current_chunk += ". " + sentence
                    else:
                        current_chunk = sentence
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunks.append({
                    'chunk_id': f"{document_id}_{chunk_number}",
                    'content': current_chunk.strip(),
                    'document_id': document_id,
                    'chunk_number': chunk_number
                })
            
            logger.info(f"‚úÖ [CREATE_CHUNKS] Created {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS] Error creating chunks: {e}")
            return []
    
    def calculate_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.error(f"‚ùå [CALCULATE_HASH] Error calculating file hash: {e}")
            return ""
    
    def process_document(self, file_path: str, project_code: str = None) -> Tuple[ArchiveDocument, List[DocumentSection]]:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üîÑ [PROCESS_DOCUMENT] Processing document: {file_path}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text_content = self.extract_text_from_file(file_path)
            if not text_content:
                raise Exception("Failed to extract text from document")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –®–ò–§–† –ø—Ä–æ–µ–∫—Ç–∞
            if not project_code:
                project_code = self.extract_project_code(os.path.basename(file_path), text_content)
                if not project_code:
                    raise Exception("Project code not found and not provided")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_type = self.detect_document_type(os.path.basename(file_path), text_content)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = ArchiveDocument(
                project_code=project_code,
                document_type=document_type,
                document_name=os.path.basename(file_path),
                original_filename=os.path.basename(file_path),
                file_type=Path(file_path).suffix.lower(),
                file_size=os.path.getsize(file_path),
                file_path=file_path,
                document_hash=self.calculate_file_hash(file_path),
                token_count=len(text_content.split()),
                upload_date=datetime.now(),
                processing_status=ProcessingStatus.PENDING
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª—ã
            sections = self.extract_sections_from_text(text_content, 0)  # ID –±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
            
            logger.info(f"‚úÖ [PROCESS_DOCUMENT] Document processed successfully: {len(sections)} sections")
            return document, sections
            
        except Exception as e:
            logger.error(f"‚ùå [PROCESS_DOCUMENT] Error processing document: {e}")
            raise
