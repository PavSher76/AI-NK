# –°—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º

## üìã –û–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   –ó–∞–≥—Ä—É–∑–∫–∞      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   –ü–∞—Ä—Å–∏–Ω–≥       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å    ‚îÇ
‚îÇ   –¥–æ–∫—É–º–µ–Ω—Ç–∞     ‚îÇ    ‚îÇ   –∫–æ–Ω—Ç–µ–Ω—Ç–∞      ‚îÇ    ‚îÇ   –¥–ª—è –ø–æ–∏—Å–∫–∞    ‚îÇ    ‚îÇ   –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîÑ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏

### 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

```mermaid
graph TD
    A[–§—Ä–æ–Ω—Ç–µ–Ω–¥: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞] --> B[Gateway: /api/upload]
    B --> C[Document Parser: upload_document]
    C --> D{–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞}
    D -->|PDF/DOCX| E[–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
    D -->|–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π| F[–û—à–∏–±–∫–∞: –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç]
    E --> G[–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö]
    G --> H[–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
    H --> I[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ uploaded_documents]
    I --> J[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ extracted_elements]
    J --> K[–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è]
    K --> L[–ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é]
```

### 2. –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

```mermaid
graph TD
    A[Document Parser: upload_document] --> B[–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ö–µ—à—É]
    B --> C{–î–æ–∫—É–º–µ–Ω—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç?}
    C -->|–î–∞| D[–û—à–∏–±–∫–∞: –î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω]
    C -->|–ù–µ—Ç| E[–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞]
    E --> F[PDF: PyPDF2/poppler]
    E --> G[DOCX: python-docx]
    F --> H[–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞]
    G --> H
    H --> I[–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö]
    I --> J[–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞]
    J --> K[–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤]
    K --> L[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î]
```

### 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

```mermaid
graph TD
    A[–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç] --> B[uploaded_documents]
    B --> C[ID, filename, file_type, file_size, upload_date, category]
    A --> D[extracted_elements]
    D --> E[document_id, element_type, content, page_number, section]
    A --> F[–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —á–∞–Ω–∫–∏]
    F --> G[document_id, chunk_id, chunk_type, content, metadata]
    G --> H[–í–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è]
    H --> I[Qdrant: normative_documents]
    I --> J[–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ BGE-M3]
    J --> K[–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É]
```

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

### 1. –¢–∞–±–ª–∏—Ü—ã PostgreSQL

#### `uploaded_documents`
```sql
CREATE TABLE uploaded_documents (
    id SERIAL PRIMARY KEY,
    original_filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(50) NOT NULL,
    file_size BIGINT NOT NULL,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    category VARCHAR(100),
    processing_status VARCHAR(50) DEFAULT 'completed',
    document_hash VARCHAR(64) UNIQUE
);
```

#### `extracted_elements`
```sql
CREATE TABLE extracted_elements (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES uploaded_documents(id),
    element_type VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    page_number INTEGER,
    section VARCHAR(255),
    element_order INTEGER,
    metadata JSONB
);
```

#### `normative_chunks`
```sql
CREATE TABLE normative_chunks (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES uploaded_documents(id),
    chunk_id VARCHAR(255) UNIQUE NOT NULL,
    chunk_type VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ Qdrant

#### –ö–æ–ª–ª–µ–∫—Ü–∏—è `normative_documents`
```python
# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
qdrant_client.create_collection(
    collection_name="normative_documents",
    vectors_config={
        "size": 1024,  # BGE-M3 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        "distance": "Cosine"
    }
)

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–æ—á–∫–∏
{
    "id": "chunk_id",
    "vector": [0.1, 0.2, ...],  # 1024-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    "payload": {
        "document_id": 123,
        "chunk_type": "paragraph",
        "content": "—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞",
        "metadata": {...}
    }
}
```

## üîç –ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

### 1. –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞

```python
# document_parser/services/document_processing.py
class DocumentProcessor:
    def chunk_document(self, content: str, document_id: int) -> List[Dict]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = self.split_into_paragraphs(content)
        
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph.strip()) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                chunk = {
                    "chunk_id": f"doc_{document_id}_chunk_{i}",
                    "chunk_type": "paragraph",
                    "content": paragraph.strip(),
                    "metadata": {
                        "document_id": document_id,
                        "chunk_index": i,
                        "length": len(paragraph)
                    }
                }
                chunks.append(chunk)
        
        return chunks
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

```python
# rag_service/embedding_service.py
class EmbeddingService:
    def __init__(self):
        self.model = SentenceTransformer('BAAI/bge-m3')
    
    def create_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        embeddings = []
        
        for chunk in chunks:
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding = self.model.encode(chunk["content"])
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è Qdrant
            qdrant_point = {
                "id": chunk["chunk_id"],
                "vector": embedding.tolist(),
                "payload": {
                    "document_id": chunk["metadata"]["document_id"],
                    "chunk_type": chunk["chunk_type"],
                    "content": chunk["content"],
                    "metadata": chunk["metadata"]
                }
            }
            embeddings.append(qdrant_point)
        
        return embeddings
```

### 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant

```python
# rag_service/rag_service.py
class NormRAGService:
    def index_document(self, document_id: int, chunks: List[Dict]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embedding_service = EmbeddingService()
            qdrant_points = embedding_service.create_embeddings(chunks)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant
            self.qdrant_client.upsert(
                collection_name="normative_documents",
                points=qdrant_points
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL
            self.save_chunks_metadata(document_id, chunks)
            
            logger.info(f"‚úÖ Document {document_id} indexed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Indexing error: {e}")
            raise
```

## üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–µ

### 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```mermaid
graph TD
    A[–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç] --> B[–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞]
    B --> C[–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞]
    C --> D[–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant]
    D --> E[–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤]
    E --> F[BM25 –ø–æ–∏—Å–∫ –≤ PostgreSQL]
    F --> G[–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤]
    G --> H[–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞]
    H --> I[LLM –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ VLLM]
    I --> J[–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞]
```

### 2. –ü—Ä–æ—Ü–µ—Å—Å –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è

```python
# document_parser/services/norm_control_service.py
class NormControlService:
    async def perform_norm_control_check(self, document_id: int) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            checkable_doc = await self.get_checkable_document(document_id)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_chunks = await self.find_relevant_normative_chunks(
                checkable_doc["content"]
            )
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
            context = self.build_normative_context(relevant_chunks)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            prompt_template = await self.get_normcontrol_prompt_template()
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            formatted_prompt = prompt_template.format(
                document_content=checkable_doc["content"],
                normative_context=context
            )
            
            # LLM –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ VLLM
            llm_response = await self.llm_analysis(formatted_prompt)
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            findings = self.parse_llm_response(llm_response)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result_id = await self.save_norm_control_result(document_id, findings)
            
            return {
                "status": "success",
                "result_id": result_id,
                "findings": findings
            }
            
        except Exception as e:
            logger.error(f"Norm control error: {e}")
            return {"status": "error", "error": str(e)}
    
    async def find_relevant_normative_chunks(self, content: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        embedding_service = EmbeddingService()
        query_embedding = embedding_service.model.encode(content)
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant
        search_results = self.qdrant_client.search(
            collection_name="normative_documents",
            query_vector=query_embedding.tolist(),
            limit=10,
            score_threshold=0.7
        )
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        relevant_chunks = []
        for result in search_results:
            relevant_chunks.append({
                "content": result.payload["content"],
                "document_id": result.payload["document_id"],
                "score": result.score,
                "metadata": result.payload["metadata"]
            })
        
        return relevant_chunks
```

## ü§ñ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è—Ö –ù–¢–î

### 1. –ü—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏

```mermaid
graph TD
    A[–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è] --> B[–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞]
    B --> C[–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant]
    C --> D[–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤]
    D --> E[–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞]
    E --> F[LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ VLLM]
    F --> G[–í–æ–∑–≤—Ä–∞—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é]
```

### 2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏

```python
# rag_service/ntd_consultation.py
class NTDConsultationService:
    async def generate_response(self, question: str, history: List[Dict] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_chunks = await self.find_relevant_chunks(question)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = self.build_context(relevant_chunks)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self.build_prompt(question, context, history)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ VLLM
            response = await self.llm_generate_response(prompt)
            
            return response
            
        except Exception as e:
            logger.error(f"Consultation error: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
    
    async def find_relevant_chunks(self, question: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞"""
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞
        embedding_service = EmbeddingService()
        question_embedding = embedding_service.model.encode(question)
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        search_results = self.qdrant_client.search(
            collection_name="normative_documents",
            query_vector=question_embedding.tolist(),
            limit=5,
            score_threshold=0.6
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        relevant_chunks = []
        for result in search_results:
            relevant_chunks.append({
                "content": result.payload["content"],
                "document_id": result.payload["document_id"],
                "score": result.score
            })
        
        return relevant_chunks
    
    def build_context(self, chunks: List[Dict]) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        context_parts = []
        
        for chunk in chunks:
            context_parts.append(f"–î–æ–∫—É–º–µ–Ω—Ç {chunk['document_id']}: {chunk['content']}")
        
        return "\n\n".join(context_parts)
    
    async def llm_generate_response(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ VLLM"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{GATEWAY_URL}/llm/chat/completions",
                headers={"Authorization": f"Bearer {self.auth_token}"},
                json={
                    "model": "llama3.1:8b",
                    "messages": [
                        {
                            "role": "system",
                            "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.7,
                    "max_tokens": 2000
                },
                timeout=60.0
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                raise Exception(f"LLM request failed: {response.status_code}")
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```python
# –ú–µ—Ç—Ä–∏–∫–∏ Prometheus
document_uploads_total = Counter('document_uploads_total', 'Total document uploads', ['file_type', 'status'])
document_processing_duration = Histogram('document_processing_duration_seconds', 'Document processing time')
chunks_created_total = Counter('chunks_created_total', 'Total chunks created', ['document_id'])
embeddings_created_total = Counter('embeddings_created_total', 'Total embeddings created')
```

### 2. Health Check –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```python
# Health check –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
@app.get("/health/normative-documents")
async def health_normative_documents():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ PostgreSQL
        with db_connection.cursor() as cursor:
            cursor.execute("SELECT COUNT(*) FROM uploaded_documents")
            doc_count = cursor.fetchone()[0]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        collection_info = qdrant_client.get_collection("normative_documents")
        vector_count = collection_info.points_count
        
        return {
            "status": "healthy",
            "documents": {
                "postgresql_count": doc_count,
                "qdrant_count": vector_count,
                "sync_status": "ok" if doc_count > 0 and vector_count > 0 else "warning"
            }
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }
```

## üîÑ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏

### –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞

```mermaid
sequenceDiagram
    participant F as Frontend
    participant G as Gateway
    participant DP as Document Parser
    participant RS as RAG Service
    participant DB as PostgreSQL
    participant Q as Qdrant
    participant V as VLLM

    F->>G: POST /api/upload (—Ñ–∞–π–ª)
    G->>DP: –ü—Ä–æ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
    DP->>DP: –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    DP->>DB: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ uploaded_documents
    DP->>DB: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ extracted_elements
    DP->>RS: –ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    RS->>RS: –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    RS->>RS: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    RS->>Q: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
    RS->>DB: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    RS->>DP: –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    DP->>G: –û—Ç–≤–µ—Ç –æ–± —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
    G->>F: –î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

    Note over F,V: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–µ
    F->>G: POST /api/checkable-documents/{id}/check
    G->>DP: –ó–∞–ø—Ä–æ—Å –Ω–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—å
    DP->>RS: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    RS->>Q: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    RS->>DP: –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    DP->>V: LLM –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ VLLM
    V->>DP: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    DP->>DB: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    DP->>G: –û—Ç—á–µ—Ç –æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–µ
    G->>F: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏

    Note over F,V: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è—Ö
    F->>G: POST /api/rag/consultation
    G->>RS: –ó–∞–ø—Ä–æ—Å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
    RS->>Q: –ü–æ–∏—Å–∫ –ø–æ –≤–æ–ø—Ä–æ—Å—É
    RS->>V: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ VLLM
    V->>RS: –û—Ç–≤–µ—Ç –ò–ò
    RS->>G: –û—Ç–≤–µ—Ç –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é
    G->>F: –û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
```

## üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã

### 1. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```sql
-- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º
SELECT file_type, COUNT(*) as count 
FROM uploaded_documents 
GROUP BY file_type;

-- –†–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
SELECT 
    AVG(file_size) as avg_size,
    MAX(file_size) as max_size,
    MIN(file_size) as min_size
FROM uploaded_documents;

-- –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏
SELECT 
    DATE(upload_date) as date,
    COUNT(*) as uploads
FROM uploaded_documents 
GROUP BY DATE(upload_date)
ORDER BY date DESC;
```

### 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```sql
-- –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–æ–∏—Å–∫–µ
SELECT 
    nd.document_id,
    ud.original_filename,
    COUNT(*) as search_count
FROM normative_chunks nc
JOIN uploaded_documents ud ON nc.document_id = ud.id
JOIN search_logs sl ON nc.chunk_id = sl.chunk_id
GROUP BY nd.document_id, ud.original_filename
ORDER BY search_count DESC;
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–î–∞–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥** - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
2. **–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è** - —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞
3. **–•—Ä–∞–Ω–µ–Ω–∏–µ** - –≥–∏–±—Ä–∏–¥–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PostgreSQL –∏ Qdrant
4. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ** - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–µ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è—Ö
5. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–°–∏—Å—Ç–µ–º–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò.

