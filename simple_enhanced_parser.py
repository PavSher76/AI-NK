"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
–û–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
"""

import logging
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
from ultimate_analyzer_integration import UltimateAnalyzerIntegration

logger = logging.getLogger(__name__)


class SimpleEnhancedDocumentParser:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    
    def __init__(self):
        self.ultimate_integration = UltimateAnalyzerIntegration()
        self.use_ultimate_analyzer = True
        self.fallback_enabled = True
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_metrics = {
            'total_documents': 0,
            'ultimate_analyzer_success': 0,
            'simple_parser_success': 0,
            'total_failures': 0,
            'average_processing_time': 0.0
        }
    
    def parse_document(self, file_path: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∏ fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–∞—Ä—Å–µ—Ä—É
        """
        start_time = time.time()
        self.performance_metrics['total_documents'] += 1
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {file_path}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        if self.use_ultimate_analyzer:
            try:
                ultimate_result = self.ultimate_integration.analyze_document_for_hierarchical_control(file_path)
                
                if ultimate_result['success']:
                    processing_time = time.time() - start_time
                    self.performance_metrics['ultimate_analyzer_success'] += 1
                    self._update_average_processing_time(processing_time)
                    
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º –∑–∞ {processing_time:.3f} —Å–µ–∫")
                    return self._convert_ultimate_to_parser_format(ultimate_result)
                else:
                    logger.warning(f"–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {ultimate_result.get('error')}")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ: {e}")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–∞—Ä—Å–µ—Ä—É
        if self.fallback_enabled:
            try:
                logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä")
                simple_result = self._simple_parse_document(file_path)
                
                processing_time = time.time() - start_time
                self.performance_metrics['simple_parser_success'] += 1
                self._update_average_processing_time(processing_time)
                
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø—Ä–æ—Å—Ç—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∑–∞ {processing_time:.3f} —Å–µ–∫")
                return self._add_enhancement_info(simple_result, False)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º –ø–∞—Ä—Å–µ—Ä–µ: {e}")
        
        # –ï—Å–ª–∏ –æ–±–∞ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        processing_time = time.time() - start_time
        self.performance_metrics['total_failures'] += 1
        self._update_average_processing_time(processing_time)
        
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∏ –æ–¥–Ω–∏–º –∏–∑ –ø–∞—Ä—Å–µ—Ä–æ–≤: {file_path}")
        return self._create_error_result("–í—Å–µ –ø–∞—Ä—Å–µ—Ä—ã –Ω–µ —Å–º–æ–≥–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")
    
    def _simple_parse_document(self, file_path: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ fallback"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
            file_size = file_path.stat().st_size
            file_name = file_path.name
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            except:
                content = f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {file_name} –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç"
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                'success': True,
                'file_name': file_name,
                'file_size': file_size,
                'file_hash': 'simple_hash',
                'analysis_time': 0.001,
                'pages': [{
                    'page_number': 1,
                    'text': content[:1000],  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                    'metadata': {
                        'page_type': 'unknown',
                        'char_count': len(content),
                        'word_count': len(content.split()),
                        'confidence_score': 0.5
                    }
                }],
                'metadata': {
                    'total_pages': 1,
                    'document_type': 'unknown',
                    'project_number': None,
                    'mark': None,
                    'revision': None,
                    'compliance_score': 0.0,
                    'simple_parser_used': True
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º –ø–∞—Ä—Å–µ—Ä–µ: {e}")
            return self._create_error_result(str(e))
    
    def _convert_ultimate_to_parser_format(self, ultimate_result: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç –ø–∞—Ä—Å–µ—Ä–∞"""
        
        # –ë–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ø–∞—Ä—Å–µ—Ä–∞
        parser_result = {
            'success': True,
            'file_name': ultimate_result['file_info']['filename'],
            'file_size': ultimate_result['file_info']['file_size'],
            'file_hash': ultimate_result['file_info']['file_hash'],
            'analysis_time': ultimate_result['file_info']['analysis_time'],
            'pages': [],
            'metadata': {
                'total_pages': ultimate_result['analysis_summary']['total_pages'],
                'document_type': ultimate_result['document_info']['document_type'],
                'project_number': ultimate_result['document_info']['project_number'],
                'mark': ultimate_result['document_info']['mark'],
                'revision': ultimate_result['document_info']['revision'],
                'compliance_score': ultimate_result['analysis_summary']['compliance_score'],
                'ultimate_analyzer_used': True,
                'enhanced_parsing': True
            }
        }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
        for page_info in ultimate_result['pages_info']['pages']:
            page = {
                'page_number': page_info['page_number'],
                'text': '',  # –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
                'metadata': {
                    'page_type': page_info['page_type'],
                    'char_count': page_info['char_count'],
                    'word_count': page_info['word_count'],
                    'confidence_score': page_info['confidence_score'],
                    'has_stamp': page_info['has_stamp'],
                    'has_scale': page_info['has_scale'],
                    'elements_count': page_info['elements_count'],
                    'issues_count': page_info['issues_count']
                }
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —à—Ç–∞–º–ø–∞
            if 'stamp_info' in page_info:
                page['metadata']['stamp_info'] = page_info['stamp_info']
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —á–µ—Ä—Ç–µ–∂–∞
            if 'drawing_elements' in page_info:
                page['metadata']['drawing_elements'] = page_info['drawing_elements']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            if 'compliance_issues' in page_info:
                page['metadata']['compliance_issues'] = page_info['compliance_issues']
            
            parser_result['pages'].append(page)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        parser_result['elements'] = ultimate_result['elements_info']['all_elements']
        parser_result['elements_by_type'] = ultimate_result['elements_info']['elements_by_type']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        parser_result['content_analysis'] = ultimate_result['content_info']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±—ã—Å—Ç—Ä—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        parser_result['quick_metadata'] = ultimate_result['metadata_info']
        
        return parser_result
    
    def _add_enhancement_info(self, original_result: Dict[str, Any], ultimate_used: bool) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É–ª—É—á—à–µ–Ω–∏—è—Ö –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É"""
        if 'metadata' not in original_result:
            original_result['metadata'] = {}
        
        original_result['metadata']['ultimate_analyzer_used'] = ultimate_used
        original_result['metadata']['enhanced_parsing'] = True
        original_result['metadata']['fallback_used'] = not ultimate_used
        
        return original_result
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –æ—à–∏–±–∫–æ–π"""
        return {
            'success': False,
            'error': error_message,
            'metadata': {
                'ultimate_analyzer_used': False,
                'enhanced_parsing': True,
                'fallback_used': False
            }
        }
    
    def _update_average_processing_time(self, processing_time: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        total_documents = self.performance_metrics['total_documents']
        current_avg = self.performance_metrics['average_processing_time']
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        new_avg = ((current_avg * (total_documents - 1)) + processing_time) / total_documents
        self.performance_metrics['average_processing_time'] = new_avg
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return self.performance_metrics.copy()
    
    def reset_metrics(self):
        """–°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.performance_metrics = {
            'total_documents': 0,
            'ultimate_analyzer_success': 0,
            'simple_parser_success': 0,
            'total_failures': 0,
            'average_processing_time': 0.0
        }
    
    def set_ultimate_analyzer_enabled(self, enabled: bool):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.use_ultimate_analyzer = enabled
        logger.info(f"–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä {'–≤–∫–ª—é—á–µ–Ω' if enabled else '–æ—Ç–∫–ª—é—á–µ–Ω'}")
    
    def set_fallback_enabled(self, enabled: bool):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–∞—Ä—Å–µ—Ä—É"""
        self.fallback_enabled = enabled
        logger.info(f"Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–∞—Ä—Å–µ—Ä—É {'–≤–∫–ª—é—á–µ–Ω' if enabled else '–æ—Ç–∫–ª—é—á–µ–Ω'}")


def create_simple_enhanced_parser() -> SimpleEnhancedDocumentParser:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    return SimpleEnhancedDocumentParser()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    parser = create_simple_enhanced_parser()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    test_file = "tests/TestDocs/for_check/3401-21089-–†–î-01-220-221-–ê–†_4_0_RU_IFC (1).pdf"
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 70)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    result = parser.parse_document(test_file)
    
    if result['success']:
        print("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"üìÑ –§–∞–π–ª: {result['file_name']}")
        print(f"üìä –†–∞–∑–º–µ—Ä: {result['file_size'] / (1024 * 1024):.2f} –ú–ë")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['analysis_time']:.3f} —Å–µ–∫")
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {result['metadata']['total_pages']}")
        print(f"üèóÔ∏è –¢–∏–ø: {result['metadata']['document_type']}")
        print(f"üìã –ú–∞—Ä–∫–∞: {result['metadata']['mark']}")
        print(f"üìà –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: {result['metadata']['compliance_score']:.1f}%")
        print(f"üîß –≠–ª–µ–º–µ–Ω—Ç–æ–≤: {len(result.get('elements', []))}")
        print(f"‚ö° –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {'–î–∞' if result['metadata']['ultimate_analyzer_used'] else '–ù–µ—Ç'}")
        print(f"üîÑ Fallback: {'–î–∞' if result['metadata'].get('fallback_used', False) else '–ù–µ—Ç'}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        print(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö:")
        for i, page in enumerate(result['pages'][:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page['page_number']}:")
            print(f"    –¢–∏–ø: {page['metadata']['page_type']}")
            print(f"    –°–∏–º–≤–æ–ª–æ–≤: {page['metadata']['char_count']}")
            print(f"    –°–ª–æ–≤: {page['metadata']['word_count']}")
            print(f"    –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page['metadata']['confidence_score']:.1%}")
            if page['metadata'].get('has_stamp'):
                print(f"    üìã –ï—Å—Ç—å —à—Ç–∞–º–ø")
            if page['metadata'].get('has_scale'):
                print(f"    üìè –ï—Å—Ç—å –º–∞—Å—à—Ç–∞–±")
            if page['metadata'].get('elements_count', 0) > 0:
                print(f"    üîß –≠–ª–µ–º–µ–Ω—Ç–æ–≤: {page['metadata']['elements_count']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('error', 'Unknown error')}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    metrics = parser.get_performance_metrics()
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"  –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {metrics['total_documents']}")
    print(f"  –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {metrics['ultimate_analyzer_success']}")
    print(f"  –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä: {metrics['simple_parser_success']}")
    print(f"  –ù–µ—É–¥–∞—á: {metrics['total_failures']}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {metrics['average_processing_time']:.3f} —Å–µ–∫")
