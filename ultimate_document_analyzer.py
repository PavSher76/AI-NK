"""
–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä—Ç–µ–∂–µ–π –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
"""

import logging
import os
import json
import time
import re
import hashlib
from typing import Dict, Any, List, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DocumentType(Enum):
    """–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    ARCHITECTURAL_DRAWING = "architectural_drawing"
    STRUCTURAL_DRAWING = "structural_drawing"
    METAL_STRUCTURES_DRAWING = "metal_structures_drawing"
    VENTILATION_DRAWING = "ventilation_drawing"
    PLUMBING_DRAWING = "plumbing_drawing"
    ELECTRICAL_DRAWING = "electrical_drawing"
    SPECIFICATION = "specification"
    GENERAL_DATA = "general_data"
    TITLE_PAGE = "title_page"
    UNKNOWN = "unknown"


class PageType(Enum):
    """–¢–∏–ø—ã —Å—Ç—Ä–∞–Ω–∏—Ü"""
    TITLE_PAGE = "title_page"
    GENERAL_DATA_PAGE = "general_data_page"
    DRAWING_PAGE = "drawing_page"
    SPECIFICATION_PAGE = "specification_page"
    TABLE_PAGE = "table_page"
    TEXT_PAGE = "text_page"
    UNKNOWN = "unknown"


class SeverityLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏"""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"
    SUGGESTION = "suggestion"


@dataclass
class DrawingElement:
    """–≠–ª–µ–º–µ–Ω—Ç —á–µ—Ä—Ç–µ–∂–∞"""
    element_type: str
    value: Union[str, float]
    unit: Optional[str] = None
    bbox: Optional[List[float]] = None
    confidence: float = 1.0
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ComplianceIssue:
    """–ü—Ä–æ–±–ª–µ–º–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–º"""
    issue_type: str
    severity: SeverityLevel
    description: str
    page_number: int
    recommendation: Optional[str] = None
    norm_reference: Optional[str] = None
    confidence: float = 1.0


@dataclass
class StampInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —à—Ç–∞–º–ø–∞ —á–µ—Ä—Ç–µ–∂–∞"""
    sheet_number: Optional[int] = None
    revision: Optional[int] = None
    inventory_number: Optional[str] = None
    scale: Optional[str] = None
    project_number: Optional[str] = None
    object_name: Optional[str] = None
    stage: Optional[str] = None
    document_set: Optional[str] = None
    has_stamp: bool = False
    approval_info: Optional[Dict[str, Any]] = None


class UltimateDocumentAnalyzer:
    """–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.drawing_extractor = UltimateDrawingMetadataExtractor()
        self.compliance_rules = self._load_compliance_rules()
        self.document_type_classifiers = self._init_document_classifiers()
        self.analysis_cache = {}
    
    def _load_compliance_rules(self) -> Dict[str, List[Dict[str, Any]]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–º"""
        return {
            'drawing_page': [
                {
                    'rule': 'has_stamp',
                    'description': '–ù–∞–ª–∏—á–∏–µ —à—Ç–∞–º–ø–∞ —á–µ—Ä—Ç–µ–∂–∞',
                    'severity': SeverityLevel.CRITICAL,
                    'norm_reference': '–ì–û–°–¢ 21.501-2018'
                },
                {
                    'rule': 'has_scale',
                    'description': '–£–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞',
                    'severity': SeverityLevel.WARNING,
                    'norm_reference': '–ì–û–°–¢ 21.501-2018'
                },
                {
                    'rule': 'has_dimensions',
                    'description': '–ù–∞–ª–∏—á–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤',
                    'severity': SeverityLevel.INFO,
                    'norm_reference': '–ì–û–°–¢ 21.501-2018'
                }
            ],
            'general_data_page': [
                {
                    'rule': 'has_project_info',
                    'description': '–ù–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ',
                    'severity': SeverityLevel.CRITICAL,
                    'norm_reference': '–ì–û–°–¢ –† 21.101-2020'
                }
            ]
        }
    
    def _init_document_classifiers(self) -> Dict[str, List[str]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return {
            DocumentType.ARCHITECTURAL_DRAWING: ['–ê–†', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π', '–ø–ª–∞–Ω—ã', '—Ñ–∞—Å–∞–¥—ã', '—Ä–∞–∑—Ä–µ–∑—ã'],
            DocumentType.STRUCTURAL_DRAWING: ['–ö–ñ', '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π', '–∂–µ–ª–µ–∑–æ–±–µ—Ç–æ–Ω', '–±–µ—Ç–æ–Ω'],
            DocumentType.METAL_STRUCTURES_DRAWING: ['–ö–ú', '–º–µ—Ç–∞–ª–ª–æ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', '–º–µ—Ç–∞–ª–ª'],
            DocumentType.VENTILATION_DRAWING: ['–û–í', '–≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è', '–æ—Ç–æ–ø–ª–µ–Ω–∏–µ'],
            DocumentType.PLUMBING_DRAWING: ['–í–ö', '–≤–æ–¥–æ–ø—Ä–æ–≤–æ–¥', '–∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è'],
            DocumentType.ELECTRICAL_DRAWING: ['–≠–û', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π', '—ç–ª–µ–∫—Ç—Ä–æ–ø—Ä–æ–≤–æ–¥–∫–∞'],
            DocumentType.SPECIFICATION: ['—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤–µ–¥–æ–º–æ—Å—Ç—å', '–ø–æ–∑–∏—Ü–∏—è'],
            DocumentType.GENERAL_DATA: ['–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ', '–ø—Ä–æ–µ–∫—Ç', '–æ–±—ä–µ–∫—Ç']
        }
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = time.time()
        
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            file_hash = self._calculate_file_hash(file_path)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if file_hash in self.analysis_cache:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {file_path.name}")
                cached_result = self.analysis_cache[file_hash]
                cached_result['analysis_time'] = time.time() - start_time
                return cached_result
            
            # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            filename_analysis = self._analyze_filename(file_path.name)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = self._extract_content(file_path)
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_analysis = self._analyze_content(content, filename_analysis)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_structure = self._create_document_structure(file_path, filename_analysis, content_analysis)
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü
            pages_analysis = self._analyze_pages(content, document_structure)
            
            # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            overall_analysis = self._analyze_document_overall(pages_analysis, filename_analysis)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            quick_metadata = self._generate_quick_metadata(pages_analysis, filename_analysis)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            report = self._generate_report(pages_analysis, overall_analysis, filename_analysis)
            
            result = {
                'success': True,
                'file_path': str(file_path),
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'file_hash': file_hash,
                'analysis_time': time.time() - start_time,
                'filename_analysis': filename_analysis,
                'content_analysis': content_analysis,
                'document_structure': document_structure,
                'pages_analysis': pages_analysis,
                'overall_analysis': overall_analysis,
                'quick_metadata': quick_metadata,
                'report': report,
                'metadata': {
                    'total_pages': len(pages_analysis),
                    'total_chars': sum(page.get('char_count', 0) for page in pages_analysis),
                    'total_words': sum(page.get('word_count', 0) for page in pages_analysis),
                    'analysis_timestamp': time.time()
                }
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            self.analysis_cache[file_hash] = result.copy()
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': str(file_path),
                'analysis_time': time.time() - start_time
            }
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–†–∞—Å—á–µ—Ç —Ö—ç—à–∞ —Ñ–∞–π–ª–∞"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _analyze_filename(self, filename: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        analysis = {
            'project_number': None,
            'document_type': DocumentType.UNKNOWN,
            'revision': None,
            'language': 'RU',
            'format': 'IFC',
            'mark': None,
            'confidence': 0.0
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
        project_match = re.search(r'(\d{4}-\d{5})', filename)
        if project_match:
            analysis['project_number'] = project_match.group(1)
            analysis['confidence'] += 0.3
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        mark_patterns = {
            '–ê–†': 'architectural',
            '–ö–ñ': 'structural',
            '–ö–ú': 'metal_structures',
            '–û–í': 'ventilation',
            '–í–ö': 'plumbing',
            '–≠–û': 'electrical'
        }
        
        for mark, mark_type in mark_patterns.items():
            if mark in filename:
                analysis['mark'] = mark
                analysis['document_type'] = getattr(DocumentType, f"{mark_type.upper()}_DRAWING")
                analysis['confidence'] += 0.4
                break
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Ä–µ–≤–∏–∑–∏–∏
        revision_match = re.search(r'(\d+)_(\d+)_RU', filename)
        if revision_match:
            analysis['revision'] = int(revision_match.group(1))
            analysis['confidence'] += 0.2
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
        if 'RU' in filename:
            analysis['language'] = 'RU'
            analysis['confidence'] += 0.1
        
        return analysis
    
    def _extract_content(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyPDF2 –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            import PyPDF2
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except ImportError:
            logger.warning("PyPDF2 –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return self._create_mock_content_from_filename(file_path.name)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            return self._create_mock_content_from_filename(file_path.name)
    
    def _create_mock_content_from_filename(self, filename: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–∫–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        content_parts = []
        
        # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        filename_analysis = self._analyze_filename(filename)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        if filename_analysis['project_number']:
            content_parts.append(f"–ü–†–û–ï–ö–¢: {filename_analysis['project_number']}")
        
        if filename_analysis['mark']:
            content_parts.append(f"–ö–û–ú–ü–õ–ï–ö–¢: {filename_analysis['mark']}")
        
        if filename_analysis['revision']:
            content_parts.append(f"–ò–ó–ú. {filename_analysis['revision']}")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if filename_analysis['document_type'] == DocumentType.ARCHITECTURAL_DRAWING:
            content_parts.extend([
                "–ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ï –†–ï–®–ï–ù–ò–Ø",
                "–ü–õ–ê–ù–´ –≠–¢–ê–ñ–ï–ô",
                "–§–ê–°–ê–î–´",
                "–†–ê–ó–†–ï–ó–´",
                "–õ–ò–°–¢ 1",
                "–ú 1:100",
                "–ú–ê–¢–ï–†–ò–ê–õ: –ë–µ—Ç–æ–Ω –í25",
                "–ú–ê–†–ö–ê: –ê–†-01"
            ])
        elif filename_analysis['document_type'] == DocumentType.STRUCTURAL_DRAWING:
            content_parts.extend([
                "–ö–û–ù–°–¢–†–£–ö–¢–ò–í–ù–´–ï –†–ï–®–ï–ù–ò–Ø",
                "–ü–õ–ê–ù–´ –ö–û–ù–°–¢–†–£–ö–¶–ò–ô",
                "–°–•–ï–ú–´ –ê–†–ú–ò–†–û–í–ê–ù–ò–Ø",
                "–õ–ò–°–¢ 1",
                "–ú 1:100",
                "–ú–ê–¢–ï–†–ò–ê–õ: –ë–µ—Ç–æ–Ω –í25",
                "–ú–ê–†–ö–ê: –ö–ñ-01"
            ])
        
        return "\n".join(content_parts)
    
    def _analyze_content(self, content: str, filename_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        analysis = {
            'has_stamp': False,
            'has_scale': False,
            'has_dimensions': False,
            'has_materials': False,
            'has_markings': False,
            'project_info': {},
            'technical_elements': [],
            'compliance_indicators': []
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —à—Ç–∞–º–ø–∞
        stamp_indicators = ['–õ–ò–°–¢', '–ò–ó–ú.', '–ò–ù–í.', '–ü–û–î–ü.', '–°–¢. –ò–ù–ñ.']
        analysis['has_stamp'] = any(indicator in content.upper() for indicator in stamp_indicators)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∞
        analysis['has_scale'] = bool(re.search(r'–ú\s*\d+:\d+', content, re.IGNORECASE))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        analysis['has_dimensions'] = bool(re.search(r'\d+\s*–º–º', content))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
        analysis['has_materials'] = bool(re.search(r'–ú–ê–¢–ï–†–ò–ê–õ[:\s]+', content, re.IGNORECASE))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ä–∫–∏—Ä–æ–≤–æ–∫
        analysis['has_markings'] = bool(re.search(r'–ú–ê–†–ö–ê[:\s]+', content, re.IGNORECASE))
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ
        project_match = re.search(r'–ü–†–û–ï–ö–¢[:\s]+([–ê-–Ø–∞-—è\d\-\.]+)', content, re.IGNORECASE)
        if project_match:
            analysis['project_info']['project_number'] = project_match.group(1).strip()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        dimensions = re.findall(r'(\d+(?:\.\d+)?)\s*–º–º', content)
        for dim in dimensions:
            analysis['technical_elements'].append({
                'type': 'dimension',
                'value': float(dim),
                'unit': '–º–º'
            })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if analysis['has_stamp']:
            analysis['compliance_indicators'].append('has_stamp')
        if analysis['has_scale']:
            analysis['compliance_indicators'].append('has_scale')
        if analysis['has_dimensions']:
            analysis['compliance_indicators'].append('has_dimensions')
        
        return analysis
    
    def _create_document_structure(self, file_path: Path, filename_analysis: Dict[str, Any], content_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        file_size = file_path.stat().st_size
        
        # –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        estimated_pages = max(1, file_size // 150000)  # –ü—Ä–∏–º–µ—Ä–Ω–æ 150KB –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        structure = {
            'estimated_pages': estimated_pages,
            'document_type': filename_analysis['document_type'],
            'project_number': filename_analysis['project_number'],
            'mark': filename_analysis['mark'],
            'revision': filename_analysis['revision'],
            'file_size_mb': file_size / (1024 * 1024),
            'content_indicators': content_analysis['compliance_indicators']
        }
        
        return structure
    
    def _analyze_pages(self, content: str, document_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        pages_analysis = []
        estimated_pages = document_structure['estimated_pages']
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ)
        content_lines = content.split('\n')
        lines_per_page = max(1, len(content_lines) // estimated_pages)
        
        for page_num in range(1, estimated_pages + 1):
            start_line = (page_num - 1) * lines_per_page
            end_line = min(page_num * lines_per_page, len(content_lines))
            page_content = '\n'.join(content_lines[start_line:end_line])
            
            page_analysis = self._analyze_single_page(page_num, page_content, document_structure)
            pages_analysis.append(page_analysis)
        
        return pages_analysis
    
    def _analyze_single_page(self, page_num: int, content: str, document_structure: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        analysis = {
            'page_number': page_num,
            'page_type': PageType.UNKNOWN,
            'char_count': len(content),
            'word_count': len(content.split()),
            'stamp_info': StampInfo(),
            'drawing_elements': [],
            'compliance_issues': [],
            'technical_info': {},
            'confidence_score': 0.0
        }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        analysis['page_type'] = self._determine_page_type(content, page_num)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —à—Ç–∞–º–ø–∞
        if analysis['page_type'] == PageType.DRAWING_PAGE:
            analysis['stamp_info'] = self.drawing_extractor.extract_stamp_info(content)
            analysis['drawing_elements'] = self.drawing_extractor.extract_drawing_elements(content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–º
        analysis['compliance_issues'] = self._check_page_compliance(analysis, document_structure)
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        analysis['technical_info'] = self._extract_technical_info(content, document_structure)
        
        # –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        analysis['confidence_score'] = self._calculate_confidence_score(analysis)
        
        return analysis
    
    def _determine_page_type(self, content: str, page_num: int) -> PageType:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        content_upper = content.upper()
        
        if page_num == 1 or '–¢–ò–¢–£–õ–¨–ù–´–ô –õ–ò–°–¢' in content_upper:
            return PageType.TITLE_PAGE
        elif '–û–ë–©–ò–ï –î–ê–ù–ù–´–ï' in content_upper or '–ü–†–û–ï–ö–¢' in content_upper:
            return PageType.GENERAL_DATA_PAGE
        elif '–õ–ò–°–¢' in content_upper and '–ò–ó–ú.' in content_upper:
            return PageType.DRAWING_PAGE
        elif '–°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–Ø' in content_upper or '–í–ï–î–û–ú–û–°–¢–¨' in content_upper:
            return PageType.SPECIFICATION_PAGE
        else:
            return PageType.TEXT_PAGE
    
    def _check_page_compliance(self, page_analysis: Dict[str, Any], document_structure: Dict[str, Any]) -> List[ComplianceIssue]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ—Ä–º–∞–º"""
        issues = []
        page_type = page_analysis['page_type']
        
        if page_type == PageType.DRAWING_PAGE:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —à—Ç–∞–º–ø–∞
            if not page_analysis['stamp_info'].has_stamp:
                issues.append(ComplianceIssue(
                    issue_type='missing_stamp',
                    severity=SeverityLevel.CRITICAL,
                    description='–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —à—Ç–∞–º–ø —á–µ—Ä—Ç–µ–∂–∞',
                    page_number=page_analysis['page_number'],
                    recommendation='–î–æ–±–∞–≤–∏—Ç—å —à—Ç–∞–º–ø —á–µ—Ä—Ç–µ–∂–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –ì–û–°–¢ 21.501-2018',
                    norm_reference='–ì–û–°–¢ 21.501-2018'
                ))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∞
            if not page_analysis['stamp_info'].scale:
                issues.append(ComplianceIssue(
                    issue_type='missing_scale',
                    severity=SeverityLevel.WARNING,
                    description='–ù–µ —É–∫–∞–∑–∞–Ω –º–∞—Å—à—Ç–∞–± —á–µ—Ä—Ç–µ–∂–∞',
                    page_number=page_analysis['page_number'],
                    recommendation='–£–∫–∞–∑–∞—Ç—å –º–∞—Å—à—Ç–∞–± —á–µ—Ä—Ç–µ–∂–∞',
                    norm_reference='–ì–û–°–¢ 21.501-2018'
                ))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
            if not page_analysis['drawing_elements']:
                issues.append(ComplianceIssue(
                    issue_type='missing_dimensions',
                    severity=SeverityLevel.INFO,
                    description='–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ',
                    page_number=page_analysis['page_number'],
                    recommendation='–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂'
                ))
        
        elif page_type == PageType.GENERAL_DATA_PAGE:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ
            if not page_analysis['technical_info'].get('project_number'):
                issues.append(ComplianceIssue(
                    issue_type='missing_project_info',
                    severity=SeverityLevel.CRITICAL,
                    description='–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ',
                    page_number=page_analysis['page_number'],
                    recommendation='–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ',
                    norm_reference='–ì–û–°–¢ –† 21.101-2020'
                ))
        
        return issues
    
    def _extract_technical_info(self, content: str, document_structure: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        tech_info = {
            'project_number': document_structure.get('project_number'),
            'object_name': None,
            'stage': None,
            'document_set': document_structure.get('mark'),
            'materials': [],
            'dimensions': []
        }
        
        # –ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞
        object_match = re.search(r'–û–ë–™–ï–ö–¢[:\s]+([–ê-–Ø–∞-—è\s\d\-\.]+)', content, re.IGNORECASE)
        if object_match:
            tech_info['object_name'] = object_match.group(1).strip()
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞–¥–∏–∏
        stage_match = re.search(r'–°–¢–ê–î–ò–Ø[:\s]+([–ê-–Ø–∞-—è\s\d\-\.]+)', content, re.IGNORECASE)
        if stage_match:
            tech_info['stage'] = stage_match.group(1).strip()
        
        # –ü–æ–∏—Å–∫ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
        materials = re.findall(r'–ú–ê–¢–ï–†–ò–ê–õ[:\s]+([–ê-–Ø–∞-—è\s\d]+)', content, re.IGNORECASE)
        tech_info['materials'] = [material.strip() for material in materials]
        
        # –ü–æ–∏—Å–∫ —Ä–∞–∑–º–µ—Ä–æ–≤
        dimensions = re.findall(r'(\d+(?:\.\d+)?)\s*–º–º', content)
        tech_info['dimensions'] = [float(dim) for dim in dimensions]
        
        return tech_info
    
    def _calculate_confidence_score(self, page_analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–π —Å—á–µ—Ç
        score += 0.3
        
        # –ù–∞–ª–∏—á–∏–µ —à—Ç–∞–º–ø–∞
        if page_analysis['stamp_info'].has_stamp:
            score += 0.3
        
        # –ù–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä—Ç–µ–∂–∞
        if page_analysis['drawing_elements']:
            score += 0.2
        
        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if not page_analysis['compliance_issues']:
            score += 0.2
        
        return min(1.0, score)
    
    def _analyze_document_overall(self, pages_analysis: List[Dict[str, Any]], filename_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        overall = {
            'document_type': filename_analysis['document_type'],
            'total_pages': len(pages_analysis),
            'page_types_distribution': {},
            'compliance_score': 0.0,
            'total_issues': 0,
            'critical_issues': 0,
            'warning_issues': 0,
            'info_issues': 0,
            'average_confidence': 0.0,
            'recommendations': []
        }
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü
        for page in pages_analysis:
            page_type = page['page_type'].value
            overall['page_types_distribution'][page_type] = overall['page_types_distribution'].get(page_type, 0) + 1
        
        # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–ª–µ–º
        for page in pages_analysis:
            for issue in page['compliance_issues']:
                overall['total_issues'] += 1
                if issue.severity == SeverityLevel.CRITICAL:
                    overall['critical_issues'] += 1
                elif issue.severity == SeverityLevel.WARNING:
                    overall['warning_issues'] += 1
                else:
                    overall['info_issues'] += 1
        
        # –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if overall['total_issues'] == 0:
            overall['compliance_score'] = 100.0
        else:
            penalty = overall['critical_issues'] * 20 + overall['warning_issues'] * 10 + overall['info_issues'] * 5
            overall['compliance_score'] = max(0, 100 - penalty)
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if pages_analysis:
            overall['average_confidence'] = sum(page['confidence_score'] for page in pages_analysis) / len(pages_analysis)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if overall['critical_issues'] > 0:
            overall['recommendations'].append('–£—Å—Ç—Ä–∞–Ω–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è')
        if overall['warning_issues'] > 0:
            overall['recommendations'].append('–ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è')
        if overall['compliance_score'] < 80:
            overall['recommendations'].append('–ü–æ–≤—ã—Å–∏—Ç—å –æ–±—â–µ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º')
        
        return overall
    
    def _generate_quick_metadata(self, pages_analysis: List[Dict[str, Any]], filename_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        quick_metadata = {
            'document_id': filename_analysis.get('project_number', 'unknown'),
            'document_type': filename_analysis['document_type'].value,
            'mark': filename_analysis.get('mark'),
            'revision': filename_analysis.get('revision'),
            'total_pages': len(pages_analysis),
            'has_drawings': any(page['page_type'] == PageType.DRAWING_PAGE for page in pages_analysis),
            'has_specifications': any(page['page_type'] == PageType.SPECIFICATION_PAGE for page in pages_analysis),
            'compliance_status': 'compliant',
            'critical_issues_count': 0,
            'last_analyzed': time.time()
        }
        
        # –ü–æ–¥—Å—á–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º
        for page in pages_analysis:
            for issue in page['compliance_issues']:
                if issue.severity == SeverityLevel.CRITICAL:
                    quick_metadata['critical_issues_count'] += 1
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if quick_metadata['critical_issues_count'] > 0:
            quick_metadata['compliance_status'] = 'non_compliant'
        elif any(page['compliance_issues'] for page in pages_analysis):
            quick_metadata['compliance_status'] = 'needs_attention'
        
        return quick_metadata
    
    def _generate_report(self, pages_analysis: List[Dict[str, Any]], overall_analysis: Dict[str, Any], filename_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""
        report = {
            'summary': {
                'document_name': filename_analysis.get('project_number', 'Unknown'),
                'document_type': filename_analysis['document_type'].value,
                'total_pages': len(pages_analysis),
                'compliance_score': overall_analysis['compliance_score'],
                'status': 'compliant' if overall_analysis['compliance_score'] >= 80 else 'needs_attention'
            },
            'findings': {
                'critical_issues': overall_analysis['critical_issues'],
                'warning_issues': overall_analysis['warning_issues'],
                'info_issues': overall_analysis['info_issues'],
                'total_issues': overall_analysis['total_issues']
            },
            'recommendations': overall_analysis['recommendations'],
            'page_analysis': []
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ—Ç—á–µ—Ç–∞
        for page in pages_analysis:
            page_report = {
                'page_number': page['page_number'],
                'page_type': page['page_type'].value,
                'issues_count': len(page['compliance_issues']),
                'confidence_score': page['confidence_score'],
                'has_stamp': page['stamp_info'].has_stamp,
                'has_scale': bool(page['stamp_info'].scale),
                'elements_count': len(page['drawing_elements'])
            }
            report['page_analysis'].append(page_report)
        
        return report


class UltimateDrawingMetadataExtractor:
    """–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —á–µ—Ä—Ç–µ–∂–µ–π"""
    
    def __init__(self):
        self.stamp_patterns = {
            'sheet_number': r'–õ–ò–°–¢\s+(\d+)',
            'revision': r'–ò–ó–ú\.\s*(\d+)',
            'inventory_number': r'–ò–ù–í\.\s*‚Ññ\s*(\d+)',
            'scale': r'–ú\s*(\d+:\d+)',
            'project_number': r'–ü–†–û–ï–ö–¢[:\s]+([–ê-–Ø–∞-—è\d\-\.]+)',
            'object_name': r'–û–ë–™–ï–ö–¢[:\s]+([–ê-–Ø–∞-—è\s\d\-\.]+)',
            'stage': r'–°–¢–ê–î–ò–Ø[:\s]+([–ê-–Ø–∞-—è\s\d\-\.]+)',
            'document_set': r'–ö–û–ú–ü–õ–ï–ö–¢[:\s]+([–ê-–Ø–∞-—è\s\d\-\.]+)'
        }
        
        self.drawing_element_patterns = {
            'dimension': r'(\d+(?:\.\d+)?)\s*–º–º',
            'material': r'–ú–ê–¢–ï–†–ò–ê–õ[:\s]+([–ê-–Ø–∞-—è\s\d]+)',
            'marking': r'–ú–ê–†–ö–ê[:\s]+([–ê-–Ø–∞-—è\d\-\.]+)',
            'coordinate': r'([XYZ])\s*=\s*(\d+(?:\.\d+)?)',
            'angle': r'(\d+(?:\.\d+)?)\s*¬∞',
            'radius': r'R\s*(\d+(?:\.\d+)?)',
            'diameter': r'√ò\s*(\d+(?:\.\d+)?)'
        }
    
    def extract_stamp_info(self, text: str) -> StampInfo:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —à—Ç–∞–º–ø–∞ —á–µ—Ä—Ç–µ–∂–∞"""
        stamp_info = StampInfo()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–ª–µ–π —à—Ç–∞–º–ø–∞
        for field, pattern in self.stamp_patterns.items():
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                
                if field == 'sheet_number':
                    stamp_info.sheet_number = int(value)
                elif field == 'revision':
                    stamp_info.revision = int(value)
                elif field == 'inventory_number':
                    stamp_info.inventory_number = value
                elif field == 'scale':
                    stamp_info.scale = value
                elif field == 'project_number':
                    stamp_info.project_number = value
                elif field == 'object_name':
                    stamp_info.object_name = value
                elif field == 'stage':
                    stamp_info.stage = value
                elif field == 'document_set':
                    stamp_info.document_set = value
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —à—Ç–∞–º–ø–∞
        stamp_indicators = ['–õ–ò–°–¢', '–ò–ó–ú.', '–ò–ù–í.', '–ü–û–î–ü.', '–°–¢. –ò–ù–ñ.']
        stamp_info.has_stamp = any(indicator in text.upper() for indicator in stamp_indicators)
        
        return stamp_info
    
    def extract_drawing_elements(self, text: str) -> List[DrawingElement]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä—Ç–µ–∂–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        elements = []
        
        for element_type, pattern in self.drawing_element_patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                if element_type == 'dimension':
                    elements.append(DrawingElement(
                        element_type=element_type,
                        value=float(match.group(1)),
                        unit='–º–º',
                        confidence=0.9
                    ))
                elif element_type == 'coordinate':
                    elements.append(DrawingElement(
                        element_type=element_type,
                        value=f"{match.group(1)}={match.group(2)}",
                        unit='–º–º',
                        confidence=0.9
                    ))
                elif element_type in ['angle', 'radius', 'diameter']:
                    elements.append(DrawingElement(
                        element_type=element_type,
                        value=float(match.group(1)),
                        unit='¬∞' if element_type == 'angle' else '–º–º',
                        confidence=0.9
                    ))
                else:
                    elements.append(DrawingElement(
                        element_type=element_type,
                        value=match.group(1).strip(),
                        confidence=0.8
                    ))
        
        return elements


def analyze_document_ultimate(file_path: str) -> Dict[str, Any]:
    """–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    analyzer = UltimateDocumentAnalyzer()
    return analyzer.analyze_document(file_path)


def print_ultimate_analysis_results(result: Dict[str, Any]):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    if not result['success']:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {result.get('error', 'Unknown error')}")
        return
    
    print("=" * 80)
    print("üöÄ –£–õ–¨–¢–ò–ú–ê–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–û–ï–ö–¢–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê")
    print("=" * 80)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"üìÑ –§–∞–π–ª: {result['file_name']}")
    print(f"üìä –†–∞–∑–º–µ—Ä: {result['file_size'] / 1024 / 1024:.2f} –ú–ë")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {result['analysis_time']:.2f} —Å–µ–∫")
    print(f"üîë –•—ç—à —Ñ–∞–π–ª–∞: {result['file_hash'][:16]}...")
    
    # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    filename_analysis = result['filename_analysis']
    print(f"\nüìã –ê–ù–ê–õ–ò–ó –ò–ú–ï–ù–ò –§–ê–ô–õ–ê:")
    print(f"  –ù–æ–º–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞: {filename_analysis.get('project_number', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}")
    print(f"  –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename_analysis['document_type'].value}")
    print(f"  –ú–∞—Ä–∫–∞: {filename_analysis.get('mark', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}")
    print(f"  –†–µ–≤–∏–∑–∏—è: {filename_analysis.get('revision', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}")
    print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {filename_analysis['confidence']:.1%}")
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    content_analysis = result['content_analysis']
    print(f"\nüìÑ –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ò–ú–û–ì–û:")
    print(f"  –ï—Å—Ç—å —à—Ç–∞–º–ø: {'–î–∞' if content_analysis['has_stamp'] else '–ù–µ—Ç'}")
    print(f"  –ï—Å—Ç—å –º–∞—Å—à—Ç–∞–±: {'–î–∞' if content_analysis['has_scale'] else '–ù–µ—Ç'}")
    print(f"  –ï—Å—Ç—å —Ä–∞–∑–º–µ—Ä—ã: {'–î–∞' if content_analysis['has_dimensions'] else '–ù–µ—Ç'}")
    print(f"  –ï—Å—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã: {'–î–∞' if content_analysis['has_materials'] else '–ù–µ—Ç'}")
    print(f"  –ï—Å—Ç—å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏: {'–î–∞' if content_analysis['has_markings'] else '–ù–µ—Ç'}")
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    doc_structure = result['document_structure']
    print(f"\nüèóÔ∏è –°–¢–†–£–ö–¢–£–†–ê –î–û–ö–£–ú–ï–ù–¢–ê:")
    print(f"  –û—Ü–µ–Ω–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {doc_structure['estimated_pages']}")
    print(f"  –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_structure['document_type'].value}")
    print(f"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {doc_structure['file_size_mb']:.2f} –ú–ë")
    print(f"  –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {', '.join(doc_structure['content_indicators'])}")
    
    # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑
    overall = result['overall_analysis']
    print(f"\nüìä –û–ë–©–ò–ô –ê–ù–ê–õ–ò–ó:")
    print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {overall['total_pages']}")
    print(f"  –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {overall['compliance_score']:.1f}%")
    print(f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {overall['average_confidence']:.1%}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü
    print(f"\nüìÑ –¢–ò–ü–´ –°–¢–†–ê–ù–ò–¶:")
    for page_type, count in overall['page_types_distribution'].items():
        print(f"  - {page_type}: {count}")
    
    # –ü—Ä–æ–±–ª–µ–º—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
    print(f"\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´ –°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø:")
    print(f"  –í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {overall['total_issues']}")
    print(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {overall['critical_issues']}")
    print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {overall['warning_issues']}")
    print(f"  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {overall['info_issues']}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if overall['recommendations']:
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for i, rec in enumerate(overall['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    # –ë—ã—Å—Ç—Ä—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    quick_metadata = result['quick_metadata']
    print(f"\n‚ö° –ë–´–°–¢–†–´–ï –ú–ï–¢–ê–î–ê–ù–ù–´–ï:")
    print(f"  ID –¥–æ–∫—É–º–µ–Ω—Ç–∞: {quick_metadata['document_id']}")
    print(f"  –°—Ç–∞—Ç—É—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {quick_metadata['compliance_status']}")
    print(f"  –ï—Å—Ç—å —á–µ—Ä—Ç–µ–∂–∏: {'–î–∞' if quick_metadata['has_drawings'] else '–ù–µ—Ç'}")
    print(f"  –ï—Å—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏: {'–î–∞' if quick_metadata['has_specifications'] else '–ù–µ—Ç'}")
    print(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {quick_metadata['critical_issues_count']}")
    
    # –û—Ç—á–µ—Ç
    report = result['report']
    print(f"\nüìã –û–¢–ß–ï–¢:")
    print(f"  –°—Ç–∞—Ç—É—Å: {report['summary']['status']}")
    print(f"  –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {report['summary']['compliance_score']:.1f}%")
    print(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {report['findings']['critical_issues']}")
    print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {report['findings']['warning_issues']}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    pages = result['pages_analysis']
    print(f"\nüîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ï–†–í–´–• 3 –°–¢–†–ê–ù–ò–¶:")
    for i, page in enumerate(pages[:3]):
        print(f"\n  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page['page_number']}:")
        print(f"    –¢–∏–ø: {page['page_type'].value}")
        print(f"    –°–∏–º–≤–æ–ª–æ–≤: {page['char_count']}")
        print(f"    –°–ª–æ–≤: {page['word_count']}")
        print(f"    –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page['confidence_score']:.1%}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —à—Ç–∞–º–ø–∞
        stamp_info = page['stamp_info']
        if stamp_info.has_stamp:
            print(f"    üìã –®—Ç–∞–º–ø:")
            if stamp_info.sheet_number:
                print(f"      –ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞: {stamp_info.sheet_number}")
            if stamp_info.revision:
                print(f"      –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {stamp_info.revision}")
            if stamp_info.scale:
                print(f"      –ú–∞—Å—à—Ç–∞–±: {stamp_info.scale}")
            if stamp_info.inventory_number:
                print(f"      –ò–Ω–≤. –Ω–æ–º–µ—Ä: {stamp_info.inventory_number}")
        
        # –≠–ª–µ–º–µ–Ω—Ç—ã —á–µ—Ä—Ç–µ–∂–∞
        elements = page['drawing_elements']
        if elements:
            print(f"    üîß –≠–ª–µ–º–µ–Ω—Ç—ã —á–µ—Ä—Ç–µ–∂–∞: {len(elements)}")
            for element in elements[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"      - {element.element_type}: {element.value} {element.unit or ''}")
        
        # –ü—Ä–æ–±–ª–µ–º—ã
        issues = page['compliance_issues']
        if issues:
            print(f"    ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã: {len(issues)}")
            for issue in issues:
                print(f"      - {issue.description} ({issue.severity.value})")


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
    file_path = "tests/TestDocs/for_check/3401-21089-–†–î-01-220-221-–ê–†_4_0_RU_IFC (1).pdf"
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
    
    # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
    result = analyze_document_ultimate(file_path)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print_ultimate_analysis_results(result)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if result['success']:
        output_file = f"ultimate_analysis_result_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Enum –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
            def convert_enums(obj):
                if isinstance(obj, dict):
                    return {k: convert_enums(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_enums(item) for item in obj]
                elif isinstance(obj, Enum):
                    return obj.value
                elif hasattr(obj, '__dict__'):
                    return convert_enums(obj.__dict__)
                else:
                    return obj
            
            json.dump(convert_enums(result), f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
