"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
"""

import logging
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
from ultimate_document_analyzer import UltimateDocumentAnalyzer, DocumentType, PageType, SeverityLevel

logger = logging.getLogger(__name__)


class UltimateAnalyzerIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.ultimate_analyzer = UltimateDocumentAnalyzer()
        self.fallback_enabled = True
        self.performance_metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_processing_time': 0.0
        }
    
    def analyze_document_for_hierarchical_control(self, file_path: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        """
        start_time = time.time()
        self.performance_metrics['total_requests'] += 1
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º: {file_path}")
            
            # –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            ultimate_result = self.ultimate_analyzer.analyze_document(file_path)
            
            if ultimate_result['success']:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
                hierarchical_result = self._convert_to_hierarchical_format(ultimate_result)
                
                processing_time = time.time() - start_time
                self.performance_metrics['successful_requests'] += 1
                self._update_average_processing_time(processing_time)
                
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞ {processing_time:.3f} —Å–µ–∫")
                return hierarchical_result
            else:
                logger.warning(f"–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {ultimate_result.get('error')}")
                if self.fallback_enabled:
                    return self._fallback_analysis(file_path, start_time)
                else:
                    return self._create_error_result(ultimate_result.get('error', 'Unknown error'))
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ: {e}")
            if self.fallback_enabled:
                return self._fallback_analysis(file_path, start_time)
            else:
                return self._create_error_result(str(e))
    
    def _convert_to_hierarchical_format(self, ultimate_result: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        file_info = {
            'filename': ultimate_result['file_name'],
            'file_size': ultimate_result['file_size'],
            'file_size_mb': ultimate_result['file_size'] / (1024 * 1024),
            'file_hash': ultimate_result['file_hash'],
            'analysis_time': ultimate_result['analysis_time']
        }
        
        # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        filename_analysis = ultimate_result['filename_analysis']
        document_info = {
            'project_number': filename_analysis.get('project_number'),
            'document_type': filename_analysis['document_type'].value,
            'mark': filename_analysis.get('mark'),
            'revision': filename_analysis.get('revision'),
            'language': filename_analysis.get('language'),
            'confidence': filename_analysis.get('confidence', 0.0)
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        content_analysis = ultimate_result['content_analysis']
        content_info = {
            'has_stamp': content_analysis['has_stamp'],
            'has_scale': content_analysis['has_scale'],
            'has_dimensions': content_analysis['has_dimensions'],
            'has_materials': content_analysis['has_materials'],
            'has_markings': content_analysis['has_markings'],
            'compliance_indicators': content_analysis['compliance_indicators']
        }
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_structure = ultimate_result['document_structure']
        structure_info = {
            'estimated_pages': doc_structure['estimated_pages'],
            'document_type': doc_structure['document_type'].value,
            'file_size_mb': doc_structure['file_size_mb'],
            'content_indicators': doc_structure['content_indicators']
        }
        
        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑
        overall_analysis = ultimate_result['overall_analysis']
        analysis_summary = {
            'total_pages': overall_analysis['total_pages'],
            'compliance_score': overall_analysis['compliance_score'],
            'average_confidence': overall_analysis['average_confidence'],
            'total_issues': overall_analysis['total_issues'],
            'critical_issues': overall_analysis['critical_issues'],
            'warning_issues': overall_analysis['warning_issues'],
            'info_issues': overall_analysis['info_issues'],
            'page_types_distribution': overall_analysis['page_types_distribution']
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü
        pages_analysis = ultimate_result['pages_analysis']
        pages_info = self._extract_pages_info(pages_analysis)
        
        # –≠–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        elements_info = self._extract_elements_info(pages_analysis)
        
        # –ë—ã—Å—Ç—Ä—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        quick_metadata = ultimate_result['quick_metadata']
        metadata_info = {
            'document_id': quick_metadata['document_id'],
            'compliance_status': quick_metadata['compliance_status'],
            'has_drawings': quick_metadata['has_drawings'],
            'has_specifications': quick_metadata['has_specifications'],
            'critical_issues_count': quick_metadata['critical_issues_count']
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
        hierarchical_result = {
            'success': True,
            'file_info': file_info,
            'document_info': document_info,
            'content_info': content_info,
            'structure_info': structure_info,
            'analysis_summary': analysis_summary,
            'pages_info': pages_info,
            'elements_info': elements_info,
            'metadata_info': metadata_info,
            'ultimate_analyzer_used': True,
            'processing_timestamp': time.time()
        }
        
        return hierarchical_result
    
    def _extract_pages_info(self, pages_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö"""
        pages_info = {
            'total_pages': len(pages_analysis),
            'pages': []
        }
        
        for page in pages_analysis:
            page_info = {
                'page_number': page['page_number'],
                'page_type': page['page_type'].value,
                'char_count': page['char_count'],
                'word_count': page['word_count'],
                'confidence_score': page['confidence_score'],
                'has_stamp': page['stamp_info'].has_stamp,
                'has_scale': bool(page['stamp_info'].scale),
                'elements_count': len(page['drawing_elements']),
                'issues_count': len(page['compliance_issues'])
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —à—Ç–∞–º–ø–∞
            stamp_info = page['stamp_info']
            if stamp_info.has_stamp:
                page_info['stamp_info'] = {
                    'sheet_number': stamp_info.sheet_number,
                    'revision': stamp_info.revision,
                    'inventory_number': stamp_info.inventory_number,
                    'scale': stamp_info.scale,
                    'project_number': stamp_info.project_number,
                    'object_name': stamp_info.object_name,
                    'stage': stamp_info.stage,
                    'document_set': stamp_info.document_set
                }
            
            # –≠–ª–µ–º–µ–Ω—Ç—ã —á–µ—Ä—Ç–µ–∂–∞
            if page['drawing_elements']:
                page_info['drawing_elements'] = [
                    {
                        'element_type': element.element_type,
                        'value': element.value,
                        'unit': element.unit,
                        'confidence': element.confidence
                    }
                    for element in page['drawing_elements']
                ]
            
            # –ü—Ä–æ–±–ª–µ–º—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            if page['compliance_issues']:
                page_info['compliance_issues'] = [
                    {
                        'issue_type': issue.issue_type,
                        'severity': issue.severity.value,
                        'description': issue.description,
                        'recommendation': issue.recommendation,
                        'norm_reference': issue.norm_reference
                    }
                    for issue in page['compliance_issues']
                ]
            
            pages_info['pages'].append(page_info)
        
        return pages_info
    
    def _extract_elements_info(self, pages_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        all_elements = []
        total_elements = 0
        
        for page in pages_analysis:
            elements = page['drawing_elements']
            total_elements += len(elements)
            
            for element in elements:
                element_info = {
                    'page_number': page['page_number'],
                    'element_type': element.element_type,
                    'value': element.value,
                    'unit': element.unit,
                    'confidence': element.confidence,
                    'bbox': element.bbox,
                    'metadata': element.metadata
                }
                all_elements.append(element_info)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º
        elements_by_type = {}
        for element in all_elements:
            element_type = element['element_type']
            if element_type not in elements_by_type:
                elements_by_type[element_type] = []
            elements_by_type[element_type].append(element)
        
        return {
            'total_elements': total_elements,
            'elements_by_type': elements_by_type,
            'all_elements': all_elements
        }
    
    def _fallback_analysis(self, file_path: str, start_time: float) -> Dict[str, Any]:
        """Fallback –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º fallback –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {file_path}")
        
        try:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤—ã–∑–æ–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            fallback_result = {
                'success': True,
                'file_info': {
                    'filename': Path(file_path).name,
                    'file_size': Path(file_path).stat().st_size,
                    'file_hash': 'fallback_hash',
                    'analysis_time': time.time() - start_time
                },
                'document_info': {
                    'project_number': None,
                    'document_type': 'unknown',
                    'mark': None,
                    'revision': None,
                    'language': 'RU',
                    'confidence': 0.0
                },
                'content_info': {
                    'has_stamp': False,
                    'has_scale': False,
                    'has_dimensions': False,
                    'has_materials': False,
                    'has_markings': False,
                    'compliance_indicators': []
                },
                'structure_info': {
                    'estimated_pages': 1,
                    'document_type': 'unknown',
                    'file_size_mb': Path(file_path).stat().st_size / (1024 * 1024),
                    'content_indicators': []
                },
                'analysis_summary': {
                    'total_pages': 1,
                    'compliance_score': 0.0,
                    'average_confidence': 0.0,
                    'total_issues': 1,
                    'critical_issues': 1,
                    'warning_issues': 0,
                    'info_issues': 0,
                    'page_types_distribution': {'unknown': 1}
                },
                'pages_info': {
                    'total_pages': 1,
                    'pages': [{
                        'page_number': 1,
                        'page_type': 'unknown',
                        'char_count': 0,
                        'word_count': 0,
                        'confidence_score': 0.0,
                        'has_stamp': False,
                        'has_scale': False,
                        'elements_count': 0,
                        'issues_count': 1
                    }]
                },
                'elements_info': {
                    'total_elements': 0,
                    'elements_by_type': {},
                    'all_elements': []
                },
                'metadata_info': {
                    'document_id': 'unknown',
                    'compliance_status': 'error',
                    'has_drawings': False,
                    'has_specifications': False,
                    'critical_issues_count': 1
                },
                'ultimate_analyzer_used': False,
                'fallback_used': True,
                'processing_timestamp': time.time()
            }
            
            processing_time = time.time() - start_time
            self.performance_metrics['failed_requests'] += 1
            self._update_average_processing_time(processing_time)
            
            logger.warning(f"Fallback –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time:.3f} —Å–µ–∫")
            return fallback_result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ fallback –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return self._create_error_result(str(e))
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –æ—à–∏–±–∫–æ–π"""
        return {
            'success': False,
            'error': error_message,
            'ultimate_analyzer_used': False,
            'fallback_used': False,
            'processing_timestamp': time.time()
        }
    
    def _update_average_processing_time(self, processing_time: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        total_requests = self.performance_metrics['total_requests']
        current_avg = self.performance_metrics['average_processing_time']
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        new_avg = ((current_avg * (total_requests - 1)) + processing_time) / total_requests
        self.performance_metrics['average_processing_time'] = new_avg
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return self.performance_metrics.copy()
    
    def reset_metrics(self):
        """–°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.performance_metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_processing_time': 0.0
        }
    
    def set_fallback_enabled(self, enabled: bool):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ fallback –º–µ—Ö–∞–Ω–∏–∑–º–∞"""
        self.fallback_enabled = enabled
        logger.info(f"Fallback –º–µ—Ö–∞–Ω–∏–∑–º {'–≤–∫–ª—é—á–µ–Ω' if enabled else '–æ—Ç–∫–ª—é—á–µ–Ω'}")


def create_ultimate_analyzer_integration() -> UltimateAnalyzerIntegration:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è"""
    return UltimateAnalyzerIntegration()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    integration = create_ultimate_analyzer_integration()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    test_file = "tests/TestDocs/for_check/3401-21089-–†–î-01-220-221-–ê–†_4_0_RU_IFC (1).pdf"
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
    print("=" * 60)
    
    # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    result = integration.analyze_document_for_hierarchical_control(test_file)
    
    if result['success']:
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"üìÑ –§–∞–π–ª: {result['file_info']['filename']}")
        print(f"üìä –†–∞–∑–º–µ—Ä: {result['file_info']['file_size_mb']:.2f} –ú–ë")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['file_info']['analysis_time']:.3f} —Å–µ–∫")
        print(f"üèóÔ∏è –¢–∏–ø: {result['document_info']['document_type']}")
        print(f"üìã –ú–∞—Ä–∫–∞: {result['document_info']['mark']}")
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {result['analysis_summary']['total_pages']}")
        print(f"üìà –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: {result['analysis_summary']['compliance_score']:.1f}%")
        print(f"üîß –≠–ª–µ–º–µ–Ω—Ç–æ–≤: {result['elements_info']['total_elements']}")
        print(f"‚ö° –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {'–î–∞' if result['ultimate_analyzer_used'] else '–ù–µ—Ç'}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {result.get('error', 'Unknown error')}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    metrics = integration.get_performance_metrics()
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['total_requests']}")
    print(f"  –£—Å–ø–µ—à–Ω—ã—Ö: {metrics['successful_requests']}")
    print(f"  –ù–µ—É–¥–∞—á–Ω—ã—Ö: {metrics['failed_requests']}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {metrics['average_processing_time']:.3f} —Å–µ–∫")
