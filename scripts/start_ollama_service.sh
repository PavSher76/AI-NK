#!/bin/bash

# Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ollama Integration Service
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ: ./start_vllm_ollama.sh [port]

set -e

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
PORT=${1:-8005}
OLLAMA_SERVICE_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)/ollama_service"

echo "ðŸš€ [OLLAMA_INTEGRATION] Starting Ollama Integration Service..."
echo "ðŸ“ [OLLAMA_INTEGRATION] Port: $PORT"
echo "ðŸ“ [OLLAMA_INTEGRATION] Service directory: $OLLAMA_SERVICE_DIR"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
if [ ! -f "$OLLAMA_SERVICE_DIR/main.py" ]; then
    echo "âŒ [OLLAMA_INTEGRATION] main.py not found in $OLLAMA_SERVICE_DIR"
    exit 1
fi

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ollama
echo "ðŸ” [OLLAMA_INTEGRATION] Checking Ollama availability..."
if ! curl -s "http://localhost:11434/api/tags" > /dev/null; then
    echo "âŒ [OLLAMA_INTEGRATION] Ollama is not running. Please start Ollama first."
    echo "ðŸ’¡ [OLLAMA_INTEGRATION] Run: ollama serve"
    exit 1
fi

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
echo "ðŸ” [VLLM_OLLAMA] Checking available models..."
BGE_M3_AVAILABLE=$(curl -s "http://localhost:11434/api/tags" | jq -r '.models[] | select(.name | contains("bge-m3")) | .name' 2>/dev/null || echo "")
GPT_OSS_AVAILABLE=$(curl -s "http://localhost:11434/api/tags" | jq -r '.models[] | select(.name | contains("gpt-oss")) | .name' 2>/dev/null || echo "")

if [ -z "$BGE_M3_AVAILABLE" ]; then
    echo "âš ï¸ [VLLM_OLLAMA] BGE-M3 model not found in Ollama."
    echo "ðŸ’¡ [VLLM_OLLAMA] Please pull the model: ollama pull bge-m3"
fi

if [ -z "$GPT_OSS_AVAILABLE" ]; then
    echo "âš ï¸ [VLLM_OLLAMA] GPT-OSS model not found in Ollama."
    echo "ðŸ’¡ [VLLM_OLLAMA] Please pull the model: ollama pull gpt-oss:20b"
fi

if [ -n "$BGE_M3_AVAILABLE" ]; then
    echo "âœ… [VLLM_OLLAMA] BGE-M3 model available: $BGE_M3_AVAILABLE"
fi

if [ -n "$GPT_OSS_AVAILABLE" ]; then
    echo "âœ… [VLLM_OLLAMA] GPT-OSS model available: $GPT_OSS_AVAILABLE"
fi

# ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼ Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ ÑÐµÑ€Ð²Ð¸ÑÐ°
cd "$VLLM_SERVICE_DIR"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð»Ð¸ ÑƒÐ¶Ðµ ÑÐµÑ€Ð²Ð¸Ñ
if pgrep -f "main.py" > /dev/null; then
    echo "âš ï¸ [VLLM_OLLAMA] VLLM + Ollama Service is already running. Stopping existing process..."
    pkill -f "main.py"
    sleep 2
fi

# Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
export PYTHONPATH="$VLLM_SERVICE_DIR:$PYTHONPATH"

# Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐµÑ€Ð²Ð¸Ñ
echo "ðŸ¤– [VLLM_OLLAMA] Starting VLLM + Ollama Integration Service..."
python main.py &

SERVICE_PID=$!
echo "âœ… [VLLM_OLLAMA] Service started with PID: $SERVICE_PID"

# Ð–Ð´ÐµÐ¼ Ð·Ð°Ð¿ÑƒÑÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ°
echo "â³ [VLLM_OLLAMA] Waiting for service to start..."
sleep 5

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ
if curl -s "http://localhost:$PORT/health" > /dev/null; then
    echo "âœ… [VLLM_OLLAMA] Service is running successfully on port $PORT"
    echo "ðŸ”— [VLLM_OLLAMA] Service endpoint: http://localhost:$PORT"
    echo "ðŸ’¬ [CHAT] Chat endpoint: http://localhost:$PORT/chat"
    echo "ðŸ¥ [HEALTH] Health check: http://localhost:$PORT/health"
    echo "ðŸ“Š [STATS] Statistics: http://localhost:$PORT/stats"
    echo "ðŸ¤– [MODELS] Models list: http://localhost:$PORT/models"
    
    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ
    echo "ðŸ¥ [HEALTH] Service health status:"
    curl -s "http://localhost:$PORT/health" | jq '.' 2>/dev/null || curl -s "http://localhost:$PORT/health"
    
    echo ""
    echo "ðŸ’¡ [VLLM_OLLAMA] To stop the service, run: pkill -f 'main.py'"
    echo "ðŸ’¡ [VLLM_OLLAMA] Or use Ctrl+C to stop this script"
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ PID Ð² Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
    echo $SERVICE_PID > /tmp/vllm_ollama.pid
    
    # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
    wait $SERVICE_PID
else
    echo "âŒ [VLLM_OLLAMA] Failed to start service"
    kill $SERVICE_PID 2>/dev/null || true
    exit 1
fi
