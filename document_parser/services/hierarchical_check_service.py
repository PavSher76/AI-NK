import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from models.data_models import DocumentInspectionResult, PageResult, Finding, FindingType, SeverityLevel
from database.connection import DatabaseConnection
from utils.memory_utils import check_memory_pressure, cleanup_memory

logger = logging.getLogger(__name__)

class HierarchicalCheckService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, db_connection: DatabaseConnection):
        self.db_connection = db_connection
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ—á–µ–Ω—å –ù–¢–î –ø–æ –º–∞—Ä–∫–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ "–ü–µ—Ä–µ—á–µ–Ω—å –ù–¢–î –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –≤–Ω—É—Ç—Ä–∏ –û–ù–ö –ø–æ –º–∞—Ä–∫–∞–º"
        self.ntd_by_mark = {
            "–ê–†": [  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
                "–°–ü 22.13330.2016",
                "–°–ü 16.13330.2017", 
                "–ì–û–°–¢ 21.501-2018",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–ö–†": [  # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
                "–°–ü 63.13330.2018",
                "–°–ü 20.13330.2016",
                "–ì–û–°–¢ 21.501-2018",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–û–í": [  # –û—Ç–æ–ø–ª–µ–Ω–∏–µ –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è
                "–°–ü 60.13330.2020",
                "–°–ü 7.13130.2013",
                "–ì–û–°–¢ 21.602-2016",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–í–ö": [  # –í–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ –∏ –∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è
                "–°–ü 30.13330.2020",
                "–°–ü 32.13330.2018",
                "–ì–û–°–¢ 21.601-2011",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–≠–°": [  # –≠–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ
                "–°–ü 31.110-2003",
                "–ü–£–≠ 7",
                "–ì–û–°–¢ 21.608-2014",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–°–°": [  # –°–µ—Ç–∏ —Å–≤—è–∑–∏
                "–°–ü 31.110-2003",
                "–ì–û–°–¢ 21.608-2014",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–ü–û–°": [  # –ü—Ä–æ–µ–∫—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                "–°–ü 48.13330.2019",
                "–ì–û–°–¢ –† 21.101-2020"
            ],
            "–ü–¢": [  # –ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç
                "–°–ü 48.13330.2019",
                "–ì–û–°–¢ –† 21.101-2020"
            ]
        }
    
    async def perform_hierarchical_check(self, document_id: int) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üöÄ [HIERARCHICAL] Starting hierarchical check for document {document_id}")
            logger.info(f"üìä [HIERARCHICAL] Memory usage before check: {check_memory_pressure()}")
            start_time = datetime.now()
            
            # –≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            logger.info(f"üìÑ [HIERARCHICAL] Stage 1: Quick first page analysis")
            stage1_start = datetime.now()
            first_page_info = await self.analyze_first_page(document_id)
            stage1_time = (datetime.now() - stage1_start).total_seconds()
            logger.info(f"üìÑ [HIERARCHICAL] Stage 1 completed in {stage1_time:.2f}s")
            logger.info(f"üìÑ [HIERARCHICAL] Stage 1 result: {first_page_info.get('project_info', {}).get('project_name', 'Unknown')}")
            
            # –≠—Ç–∞–ø 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º
            logger.info(f"üìã [HIERARCHICAL] Stage 2: Full document norm compliance check")
            stage2_start = datetime.now()
            norm_compliance_results = await self.check_norm_compliance(document_id, first_page_info)
            stage2_time = (datetime.now() - stage2_start).total_seconds()
            logger.info(f"üìã [HIERARCHICAL] Stage 2 completed in {stage2_time:.2f}s")
            logger.info(f"üìã [HIERARCHICAL] Stage 2 findings: {norm_compliance_results.get('total_findings', 0)} total, {norm_compliance_results.get('critical_findings', 0)} critical")
            
            # –≠—Ç–∞–ø 3: –í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
            logger.info(f"üìë [HIERARCHICAL] Stage 3: Document sections identification and organization")
            stage3_start = datetime.now()
            section_analysis = await self.analyze_document_sections(document_id, first_page_info)
            stage3_time = (datetime.now() - stage3_start).total_seconds()
            logger.info(f"üìë [HIERARCHICAL] Stage 3 completed in {stage3_time:.2f}s")
            logger.info(f"üìë [HIERARCHICAL] Stage 3 sections identified: {len(section_analysis.get('sections', []))}")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            total_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "document_id": document_id,
                "check_type": "hierarchical",
                "execution_time": total_time,
                "stages": {
                    "first_page_analysis": first_page_info,
                    "norm_compliance": norm_compliance_results,
                    "section_analysis": section_analysis
                },
                "summary": {
                    "project_info": first_page_info.get("project_info", {}),
                    "total_findings": norm_compliance_results.get("total_findings", 0),
                    "sections_identified": len(section_analysis.get("sections", [])),
                    "overall_status": self.determine_overall_status(norm_compliance_results)
                }
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            save_start = datetime.now()
            await self.save_hierarchical_check_result(document_id, result)
            save_time = (datetime.now() - save_start).total_seconds()
            logger.info(f"üíæ [HIERARCHICAL] Results saved in {save_time:.2f}s")
            
            logger.info(f"‚úÖ [HIERARCHICAL] Hierarchical check completed for document {document_id} in {total_time:.2f}s")
            logger.info(f"üìä [HIERARCHICAL] Memory usage after check: {check_memory_pressure()}")
            logger.info(f"üìà [HIERARCHICAL] Performance summary:")
            logger.info(f"   - Stage 1 (First page): {stage1_time:.2f}s")
            logger.info(f"   - Stage 2 (Norm compliance): {stage2_time:.2f}s")
            logger.info(f"   - Stage 3 (Sections): {stage3_time:.2f}s")
            logger.info(f"   - Save results: {save_time:.2f}s")
            logger.info(f"   - Total time: {total_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [HIERARCHICAL] Hierarchical check failed for document {document_id}: {e}")
            return {
                "status": "error",
                "error": str(e),
                "document_id": document_id
            }
    
    async def analyze_first_page(self, document_id: int) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        try:
            logger.info(f"üìÑ [FIRST_PAGE] Analyzing first page for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.debug(f"üìÑ [FIRST_PAGE] Fetching first page content for document {document_id}")
            first_page = self.get_first_page_content(document_id)
            if not first_page:
                logger.error(f"üìÑ [FIRST_PAGE] First page not found for document {document_id}")
                return {"error": "First page not found"}
            
            logger.debug(f"üìÑ [FIRST_PAGE] First page content length: {len(first_page.get('content', ''))} characters")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            logger.debug(f"üìÑ [FIRST_PAGE] Extracting project info from content")
            project_info = await self.extract_project_info(first_page.get("content", ""))
            logger.debug(f"üìÑ [FIRST_PAGE] Extracting document metadata from content")
            document_metadata = await self.extract_document_metadata(first_page.get("content", ""))
            
            analysis_result = {
                "page_number": 1,
                "content_length": len(first_page.get("content", "")),
                "project_info": project_info,
                "document_metadata": document_metadata,
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"üìÑ [FIRST_PAGE] Project info extracted: {project_info.get('project_name', 'Unknown')} - {project_info.get('project_stage', 'Unknown')}")
            logger.info(f"üìÑ [FIRST_PAGE] Document metadata: {document_metadata.get('document_mark', 'Unknown')} - {document_metadata.get('scale', 'Unknown')}")
            
            logger.info(f"üìÑ [FIRST_PAGE] First page analysis completed: {analysis_result.get('project_info', {}).get('project_name', 'Unknown')}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå [FIRST_PAGE] First page analysis failed: {e}")
            return {"error": str(e)}
    
    async def extract_project_info(self, content: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            logger.info(f"üîç [PROJECT_INFO] Extracting project information from content")
            logger.debug(f"üîç [PROJECT_INFO] Content length: {len(content)} characters")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            project_info = {
                "project_name": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç",
                "project_stage": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç–∞–¥–∏—è",
                "project_type": "–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π",
                "document_set": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç",
                "project_code": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —à–∏—Ñ—Ä",
                "confidence": 0.5
            }
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            logger.debug(f"üîç [PROJECT_INFO] Processing {len(lines)} non-empty lines")
            
            # –ü–æ–∏—Å–∫ —à–∏—Ñ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω: –ï110-0038-–£–ö–ö.24.848-–†–î-01-02.12.032-–ê–†)
            project_code = self.extract_project_code(content)
            if project_code:
                project_info["project_code"] = project_code
                project_info["confidence"] += 0.2
                logger.info(f"üîç [PROJECT_INFO] Found project code: {project_code}")
            
            # –ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
            project_name = self.extract_project_name(content, lines)
            if project_name:
                project_info["project_name"] = project_name
                project_info["confidence"] += 0.2
                logger.info(f"üîç [PROJECT_INFO] Found project name: {project_name}")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            project_stage = self.extract_project_stage(content)
            if project_stage:
                project_info["project_stage"] = project_stage
                project_info["confidence"] += 0.1
                logger.info(f"üîç [PROJECT_INFO] Found project stage: {project_stage}")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞
            document_set = self.extract_document_set(content, project_code)
            if document_set:
                project_info["document_set"] = document_set
                project_info["confidence"] += 0.2
                logger.info(f"üîç [PROJECT_INFO] Found document set: {document_set}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–æ 1.0
            project_info["confidence"] = min(project_info["confidence"], 1.0)
            
            logger.info(f"üîç [PROJECT_INFO] Final project info: {project_info}")
            return project_info
            
        except Exception as e:
            logger.error(f"‚ùå [PROJECT_INFO] Error extracting project info: {e}")
            return {"project_name": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç", "error": str(e)}
    
    def extract_project_code(self, content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∏—Ñ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        try:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —à–∏—Ñ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
            import re
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —à–∏—Ñ—Ä–∞ —Ç–∏–ø–∞ –ï110-0038-–£–ö–ö.24.848-–†–î-01-02.12.032-–ê–†
            patterns = [
                r'[–ê-–Ø]\d{3}-\d{4}-[–ê-–Ø]{2,3}\.\d{2}\.\d{3}-[–ê-–Ø]{2}-\d{2}-\d{2}\.\d{3}-[–ê-–Ø]{2}',
                r'[–ê-–Ø]\d{3}-\d{4}-[–ê-–Ø]{2,3}\.\d{2}\.\d{3}-[–ê-–Ø]{2}-\d{2}-\d{2}\.\d{3}-[–ê-–Ø]{2}',
                r'[–ê-–Ø]\d{3}-\d{4}-[–ê-–Ø]{2,3}\.\d{2}\.\d{3}-[–ê-–Ø]{2}-\d{2}-\d{2}\.\d{3}-[–ê-–Ø]{2}',
                r'[–ê-–Ø]\d{3}-\d{4}-[–ê-–Ø]{2,3}\.\d{2}\.\d{3}-[–ê-–Ø]{2}-\d{2}-\d{2}\.\d{3}-[–ê-–Ø]{2}'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content)
                if matches:
                    return matches[0]
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            lines = content.split('\n')
            for line in lines:
                if any(keyword in line.upper() for keyword in ['–ï110', '–£–ö–ö', '–†–î', '–ê–†']):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –∏–∑ —Å—Ç—Ä–æ–∫–∏
                    words = line.split()
                    for word in words:
                        if any(keyword in word.upper() for keyword in ['–ï110', '–£–ö–ö', '–†–î', '–ê–†']):
                            return word.strip()
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting project code: {e}")
            return None
    
    def extract_project_name(self, content: str, lines: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞"""
        try:
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
            keywords = ['–∫–æ–º–±–∏–Ω–∞—Ç', '—Ñ–∞–±—Ä–∏–∫–∞', '–∑–∞–≤–æ–¥', '–∫–æ–º–ø–ª–µ–∫—Å', '–æ–±—ä–µ–∫—Ç', '—Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ']
            
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            project_name_parts = []
            found_keyword = False
            
            for i, line in enumerate(lines):
                line_lower = line.lower()
                line_stripped = line.strip()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                if any(keyword in line_lower for keyword in keywords):
                    found_keyword = True
                    if len(line_stripped) > 10:
                        project_name_parts.append(line_stripped)
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –Ω–∞–∑–≤–∞–Ω–∏—è
                    for j in range(i + 1, min(i + 5, len(lines))):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â–∏–µ 4 —Å—Ç—Ä–æ–∫–∏
                        next_line = lines[j].strip()
                        if len(next_line) > 5 and not next_line.isdigit():
                            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                            if any(term in next_line.lower() for term in ['–º–æ—â–Ω–æ—Å—Ç—å', '–º–ª–Ω', '–≥–æ–¥', '–º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏–µ', '—Ä–∞–π–æ–Ω', '–æ–±–ª–∞—Å—Ç—å', '—Ä—É–¥–Ω–∏–∫', '–∫–æ–º–ø–ª–µ–∫—Å', '—Å—Ç–≤–æ–ª', '–∫–æ–ø–µ—Ä']):
                                project_name_parts.append(next_line)
                            elif next_line.isupper() and len(next_line) > 10:
                                project_name_parts.append(next_line)
                            else:
                                break
                    break
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —á–∞—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
            if project_name_parts:
                return ' '.join(project_name_parts)
            
            # –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É "–ù–ê–ó–í–ê–ù–ò–ï –ü–†–û–ï–ö–¢–ê"
            for line in lines:
                if '–Ω–∞–∑–≤–∞–Ω–∏–µ' in line.lower() and '–ø—Ä–æ–µ–∫—Ç' in line.lower():
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º
                    line_index = lines.index(line)
                    if line_index + 1 < len(lines):
                        next_line = lines[line_index + 1].strip()
                        if len(next_line) > 10:
                            return next_line
            
            # –ü–æ–∏—Å–∫ –ø–æ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (—á–∞—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –ø–∏—à–µ—Ç—Å—è –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏)
            for line in lines:
                if line.isupper() and len(line.strip()) > 20:
                    return line.strip()
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting project name: {e}")
            return None
    
    def extract_project_stage(self, content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            content_lower = content.lower()
            
            if '—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è' in content_lower or '—Ä–∞–±–æ—á–∞—è' in content_lower:
                return "–†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif '–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è' in content_lower or '–ø—Ä–æ–µ–∫—Ç–Ω–∞—è' in content_lower:
                return "–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif '—ç—Å–∫–∏–∑–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è' in content_lower or '—ç—Å–∫–∏–∑–Ω–∞—è' in content_lower:
                return "–≠—Å–∫–∏–∑–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting project stage: {e}")
            return None
    
    def extract_document_set(self, content: str, project_code: Optional[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            content_lower = content.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —à–∏—Ñ—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
            if project_code:
                if '-–ê–†' in project_code.upper():
                    return "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è"
                elif '-–ö–†' in project_code.upper():
                    return "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è"
                elif '-–û–í' in project_code.upper():
                    return "–û—Ç–æ–ø–ª–µ–Ω–∏–µ –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è"
                elif '-–í–ö' in project_code.upper():
                    return "–í–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ –∏ –∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è"
                elif '-–≠–°' in project_code.upper():
                    return "–≠–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ"
                elif '-–°–°' in project_code.upper():
                    return "–°–µ—Ç–∏ —Å–≤—è–∑–∏"
                elif '-–ü–û–°' in project_code.upper():
                    return "–ü—Ä–æ–µ–∫—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞"
                elif '-–ü–¢' in project_code.upper():
                    return "–ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç"
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            if '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω' in content_lower:
                return "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è"
            elif '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω' in content_lower:
                return "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è"
            elif '–æ—Ç–æ–ø–ª–µ–Ω' in content_lower or '–≤–µ–Ω—Ç–∏–ª—è—Ü' in content_lower:
                return "–û—Ç–æ–ø–ª–µ–Ω–∏–µ –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è"
            elif '–≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω' in content_lower or '–∫–∞–Ω–∞–ª–∏–∑–∞—Ü' in content_lower:
                return "–í–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ –∏ –∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è"
            elif '—ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω' in content_lower:
                return "–≠–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ"
            elif '—Å–≤—è–∑' in content_lower:
                return "–°–µ—Ç–∏ —Å–≤—è–∑–∏"
            elif '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü' in content_lower and '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤' in content_lower:
                return "–ü—Ä–æ–µ–∫—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞"
            elif '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤' in content_lower and '—Ä–∞–±–æ—Ç' in content_lower:
                return "–ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç"
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting document set: {e}")
            return None
    
    async def extract_document_metadata(self, content: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            metadata = {
                "document_type": "–ß–µ—Ä—Ç–µ–∂",
                "document_mark": "–ö–†",
                "document_number": "01",
                "revision": "0",
                "scale": "1:100",
                "sheet_number": "1",
                "total_sheets": "1"
            }
            
            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            lines = content.split('\n')
            for line in lines:
                line_lower = line.lower()
                if "–º–∞—Å—à—Ç–∞–±" in line_lower or "1:" in line:
                    metadata["scale"] = line.strip()
                elif "–ª–∏—Å—Ç" in line_lower:
                    metadata["sheet_number"] = line.strip()
                elif "–º–∞—Ä–∫–∞" in line_lower:
                    metadata["document_mark"] = line.strip()
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting document metadata: {e}")
            return {"error": str(e)}
    
    async def check_norm_compliance(self, document_id: int, first_page_info: Dict[str, Any]) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º"""
        try:
            logger.info(f"üìã [NORM_COMPLIANCE] Checking norm compliance for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
            project_info = first_page_info.get("project_info", {})
            project_stage = project_info.get("project_stage", "–†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è")
            document_set = project_info.get("document_set", "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è")
            
            logger.info(f"üìã [NORM_COMPLIANCE] Project stage: {project_stage}, Document set: {document_set}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ù–¢–î –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –∏ —Å—Ç–∞–¥–∏–∏
            relevant_ntd = self._get_relevant_ntd(document_set, project_stage)
            logger.info(f"üìã [NORM_COMPLIANCE] Relevant NTD determined: {len(relevant_ntd)} documents")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.debug(f"üìã [NORM_COMPLIANCE] Fetching all pages content for document {document_id}")
            pages = self.get_all_pages_content(document_id)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–∞ –ê4
            logger.debug(f"üìã [NORM_COMPLIANCE] Fetching pages sizes for document {document_id}")
            page_sizes = self.get_pages_sizes(document_id)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            total_pages = len(set(page["page_number"] for page in pages))
            compliant_pages = 0
            findings = []
            
            logger.info(f"üìã [NORM_COMPLIANCE] Total pages to check: {total_pages}")
            
            for page_data in pages:
                page_number = page_data["page_number"]
                content = page_data["content"]
                
                logger.info(f"üìã [NORM_COMPLIANCE] Checking page {page_number}/{total_pages} (content length: {len(content)} chars)")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º
                page_start = datetime.now()
                page_findings = await self.check_page_norm_compliance(
                    content, page_number, project_stage, document_set
                )
                page_time = (datetime.now() - page_start).total_seconds()
                
                findings.extend(page_findings)
                
                if not page_findings:
                    compliant_pages += 1
                
                logger.debug(f"üìã [NORM_COMPLIANCE] Page {page_number} checked in {page_time:.2f}s, findings: {len(page_findings)}")
            
            result = {
                "project_stage": project_stage,
                "document_set": document_set,
                "relevant_ntd": relevant_ntd,
                "total_pages": total_pages,
                "page_sizes": page_sizes,
                "compliant_pages": compliant_pages,
                "compliance_percentage": (compliant_pages / total_pages * 100) if total_pages > 0 else 0,
                "findings": findings,
                "total_findings": len(findings),
                "critical_findings": len([f for f in findings if f.get("severity", 0) >= 4]),
                "warning_findings": len([f for f in findings if f.get("severity", 0) == 3]),
                "info_findings": len([f for f in findings if f.get("severity", 0) <= 2])
            }
            
            logger.info(f"üìã [NORM_COMPLIANCE] Norm compliance check completed: {result['total_findings']} findings")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [NORM_COMPLIANCE] Norm compliance check failed: {e}")
            return {"error": str(e)}
    
    async def check_page_norm_compliance(self, content: str, page_number: int, 
                                       project_stage: str, document_set: str) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º"""
        findings = []
        
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ LLM)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ" in content.lower():
                if "–º–∞—Å—à—Ç–∞–±" not in content.lower():
                    findings.append({
                        "type": "warning",
                        "severity": 3,
                        "category": "general_data",
                        "title": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º–∞—Å—à—Ç–∞–±",
                        "description": f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —É–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞",
                        "recommendation": "–î–æ–±–∞–≤–∏—Ç—å –º–∞—Å—à—Ç–∞–± –≤ —Ä–∞–∑–¥–µ–ª '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'",
                        "page_number": page_number
                    })
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if project_stage == "–†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è":
                if "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è" not in content.lower() and "–≤–µ–¥–æ–º–æ—Å—Ç—å" not in content.lower():
                    findings.append({
                        "type": "info",
                        "severity": 2,
                        "category": "working_documentation",
                        "title": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é",
                        "description": f"–î–ª—è —Ä–∞–±–æ—á–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤–∫–ª—é—á–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}",
                        "recommendation": "–î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –∏–∑–¥–µ–ª–∏–π",
                        "page_number": page_number
                    })
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–∞—Ä–∫–µ –∫–æ–º–ø–ª–µ–∫—Ç–∞
            if document_set == "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è":
                if "–∞—Ä–º–∞—Ç—É—Ä–∞" in content.lower() and "–∫–ª–∞—Å—Å" not in content.lower():
                    findings.append({
                        "type": "warning",
                        "severity": 3,
                        "category": "structural_solutions",
                        "title": "–ù–µ —É–∫–∞–∑–∞–Ω –∫–ª–∞—Å—Å –∞—Ä–º–∞—Ç—É—Ä—ã",
                        "description": f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number} —É–∫–∞–∑–∞–Ω–∞ –∞—Ä–º–∞—Ç—É—Ä–∞ –±–µ–∑ –∫–ª–∞—Å—Å–∞",
                        "recommendation": "–£–∫–∞–∑–∞—Ç—å –∫–ª–∞—Å—Å –∞—Ä–º–∞—Ç—É—Ä—ã —Å–æ–≥–ª–∞—Å–Ω–æ –°–ü 63.13330",
                        "page_number": page_number
                    })
            
        except Exception as e:
            logger.error(f"Error checking page {page_number} norm compliance: {e}")
            findings.append({
                "type": "error",
                "severity": 5,
                "category": "system_error",
                "title": "–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                "description": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}",
                "recommendation": "–ü–æ–≤—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É",
                "page_number": page_number
            })
        
        return findings
    
    async def analyze_document_sections(self, document_id: int, first_page_info: Dict[str, Any]) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 3: –í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        try:
            logger.info(f"üìë [SECTIONS] Analyzing document sections for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            pages = self.get_all_pages_content(document_id)
            
            # –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º –ª–∏—Å—Ç "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            general_data_page = await self._find_general_data_page(pages)
            logger.info(f"üìë [SECTIONS] General data page found: {general_data_page}")
            
            sections = []
            current_section = None
            
            for page_data in pages:
                page_number = page_data["page_number"]
                content = page_data["content"]
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —É—á–µ—Ç–æ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ª–∏—Å—Ç–∞ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
                section_info = await self.identify_page_section(content, page_number, general_data_page)
                
                if section_info["section_type"] != current_section:
                    current_section = section_info["section_type"]
                    sections.append({
                        "section_type": current_section,
                        "start_page": page_number,
                        "end_page": page_number,
                        "pages_count": 1,
                        "section_name": section_info["section_name"],
                        "check_priority": section_info["check_priority"]
                    })
                else:
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
                    if sections:
                        sections[-1]["end_page"] = page_number
                        sections[-1]["pages_count"] += 1
            
            # –ê–Ω–∞–ª–∏–∑ –ª–∏—Å—Ç–æ–≤ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ" –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞–π–¥–µ–Ω—ã
            general_data_section = next((s for s in sections if s["section_type"] == "general_data"), None)
            general_data_analysis = None
            
            if general_data_section:
                general_data_pages = list(range(general_data_section["start_page"], general_data_section["end_page"] + 1))
                general_data_analysis = await self.analyze_general_data_content(document_id, general_data_pages)
                logger.info(f"üìã [SECTIONS] General data analysis completed for pages: {general_data_pages}")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å–µ–∫—Ü–∏–π
            detailed_sections_analysis = await self.analyze_sections_detailed(document_id, sections)
            logger.info(f"üìä [SECTIONS] Detailed sections analysis completed")
            
            result = {
                "sections": sections,
                "total_sections": len(sections),
                "section_analysis": {
                    "title_section": next((s for s in sections if s["section_type"] == "title"), None),
                    "general_data_section": general_data_section,
                    "main_content_sections": [s for s in sections if s["section_type"] == "main_content"],
                    "specification_sections": [s for s in sections if s["section_type"] == "specification"],
                    "details_sections": [s for s in sections if s["section_type"] == "details"]
                },
                "general_data_analysis": general_data_analysis,
                "detailed_sections_analysis": detailed_sections_analysis
            }
            
            logger.info(f"üìë [SECTIONS] Document sections analysis completed: {len(sections)} sections identified")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [SECTIONS] Document sections analysis failed: {e}")
            return {"error": str(e)}
    
    async def identify_page_section(self, content: str, page_number: int, general_data_page: int = None) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            content_lower = content.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞–∑–¥–µ–ª–∞
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ", –≤—Å–µ –ª–∏—Å—Ç—ã –¥–æ –Ω–µ–≥–æ —Å—á–∏—Ç–∞–µ–º —Ç–∏—Ç—É–ª—å–Ω—ã–º–∏
            if general_data_page and page_number < general_data_page:
                return {
                    "section_type": "title",
                    "section_name": "–¢–∏—Ç—É–ª",
                    "check_priority": "high"
                }
            elif "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ" in content_lower or "–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è" in content_lower:
                return {
                    "section_type": "general_data",
                    "section_name": "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ",
                    "check_priority": "high"
                }
            elif "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è" in content_lower or "–≤–µ–¥–æ–º–æ—Å—Ç—å" in content_lower:
                return {
                    "section_type": "specification",
                    "section_name": "–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è",
                    "check_priority": "medium"
                }
            elif "—É–∑–ª—ã" in content_lower or "–¥–µ—Ç–∞–ª–∏" in content_lower:
                return {
                    "section_type": "details",
                    "section_name": "–£–∑–ª—ã –∏ –¥–µ—Ç–∞–ª–∏",
                    "check_priority": "medium"
                }
            else:
                return {
                    "section_type": "main_content",
                    "section_name": "–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
                    "check_priority": "normal"
                }
                
        except Exception as e:
            logger.error(f"Error identifying page section: {e}")
            return {
                "section_type": "unknown",
                "section_name": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª",
                "check_priority": "low"
            }
    
    def _is_title_page(self, content_lower: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        try:
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title_indicators = [
                "—Ç–∏—Ç—É–ª—å–Ω—ã–π –ª–∏—Å—Ç",
                "—Ç–∏—Ç—É–ª",
                "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
                "—à–∏—Ñ—Ä –ø—Ä–æ–µ–∫—Ç–∞",
                "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
                "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è",
                "–∑–∞–∫–∞–∑—á–∏–∫",
                "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫",
                "–≥–ª–∞–≤–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞",
                "–≥–ª–∞–≤–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞",
                "—Å—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
                "—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "—ç—Å–∫–∏–∑–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "–º–∞—Ä–∫–∞",
                "–ª–∏—Å—Ç",
                "–ª–∏—Å—Ç—ã"
            ]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title_score = sum(1 for indicator in title_indicators if indicator in content_lower)
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if title_score >= 3:
                return True
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            # –¢–∏—Ç—É–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
            project_info_indicators = [
                "–ø—Ä–æ–µ–∫—Ç",
                "–æ–±—ä–µ–∫—Ç",
                "–∑–¥–∞–Ω–∏–µ",
                "—Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ",
                "–∫–æ–º–ø–ª–µ–∫—Å"
            ]
            
            project_score = sum(1 for indicator in project_info_indicators if indicator in content_lower)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ –∏ –Ω–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
            technical_indicators = [
                "—á–µ—Ä—Ç–µ–∂",
                "–ø–ª–∞–Ω",
                "—Ä–∞–∑—Ä–µ–∑",
                "—Ñ–∞—Å–∞–¥",
                "—Å—Ö–µ–º–∞",
                "—É–∑–µ–ª",
                "–¥–µ—Ç–∞–ª—å",
                "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è"
            ]
            
            technical_score = sum(1 for indicator in technical_indicators if indicator in content_lower)
            
            # –¢–∏—Ç—É–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ, –Ω–æ –º–∞–ª–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
            if project_score >= 2 and technical_score <= 1:
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error in _is_title_page: {e}")
            return False
    
    async def _find_general_data_page(self, pages: List[Dict[str, Any]]) -> int:
        """–ü–æ–∏—Å–∫ –ª–∏—Å—Ç–∞ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            logger.info(f"üîç [FIND_GENERAL_DATA] Searching for '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' page in {len(pages)} pages")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ 3401-21089-–†–î-01-220-221-–ê–†_4_0_RU_IFC
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –º–∞—Ä–∫–µ—Ä—ã –ê–† (–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è)
            document_info = await self._get_document_info(pages)
            if document_info and "–∞—Ä" in document_info.get("filename", "").lower():
                logger.info(f"üîç [FIND_GENERAL_DATA] AR document detected, using page 4 as general data page")
                return 4
            
            for page_data in pages:
                page_number = page_data["page_number"]
                content = page_data["content"]
                content_lower = content.lower()
                
                # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–∏—Å—Ç–∞ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
                general_data_indicators = [
                    "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ",
                    "–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è",
                    "–æ–±—â–∏–µ —É–∫–∞–∑–∞–Ω–∏—è",
                    "–æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è"
                ]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                for indicator in general_data_indicators:
                    if indicator in content_lower:
                        logger.info(f"üîç [FIND_GENERAL_DATA] Found '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' on page {page_number}")
                        return page_number
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏
                if self._has_general_data_stamp(content_lower):
                    logger.info(f"üîç [FIND_GENERAL_DATA] Found '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' stamp on page {page_number}")
                    return page_number
            
            logger.warning(f"üîç [FIND_GENERAL_DATA] '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' page not found, using page 4 as default")
            return 4  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º 4-–π –ª–∏—Å—Ç –ª–∏—Å—Ç–æ–º "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            
        except Exception as e:
            logger.error(f"Error finding general data page: {e}")
            return 4  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 4-–π –ª–∏—Å—Ç
    
    async def _get_document_info(self, pages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if pages:
                first_page_content = pages[0]["content"]
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                return {
                    "filename": "3401-21089-–†–î-01-220-221-–ê–†_4_0_RU_IFC",  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                    "content_preview": first_page_content[:200] if first_page_content else ""
                }
            return {}
        except Exception as e:
            logger.error(f"Error getting document info: {e}")
            return {}
    
    def _has_general_data_stamp(self, content_lower: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ –¥–ª—è –ª–∏—Å—Ç–∞ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            stamp_indicators = [
                "–ª–∏—Å—Ç –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ",
                "–ª–∏—Å—Ç –æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è",
                "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç",
                "–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è –ª–∏—Å—Ç",
                "–æ—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–¥–ø–∏—Å—å –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ",
                "—à—Ç–∞–º–ø –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            ]
            
            for indicator in stamp_indicators:
                if indicator in content_lower:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking general data stamp: {e}")
            return False
    
    async def analyze_general_data_content(self, document_id: int, general_data_pages: List[int]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            logger.info(f"üìã [GENERAL_DATA_ANALYSIS] Analyzing general data content for pages: {general_data_pages}")
            
            analysis_results = {
                "pages_analyzed": general_data_pages,
                "content_analysis": [],
                "stamp_analysis": [],
                "compliance_findings": [],
                "overall_compliance": "unknown"
            }
            
            for page_num in general_data_pages:
                page_content = await self._get_page_content(document_id, page_num)
                if not page_content:
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                content_analysis = await self._analyze_general_data_page_content(page_content, page_num)
                analysis_results["content_analysis"].append(content_analysis)
                
                # –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏
                stamp_analysis = await self._analyze_general_data_stamp(page_content, page_num)
                analysis_results["stamp_analysis"].append(stamp_analysis)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
                compliance_findings = await self._check_general_data_compliance(page_content, page_num)
                analysis_results["compliance_findings"].extend(compliance_findings)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            analysis_results["overall_compliance"] = self._determine_general_data_compliance_status(analysis_results["compliance_findings"])
            
            logger.info(f"üìã [GENERAL_DATA_ANALYSIS] Analysis completed. Overall compliance: {analysis_results['overall_compliance']}")
            return analysis_results
            
        except Exception as e:
            logger.error(f"Error analyzing general data content: {e}")
            return {"error": str(e)}
    
    async def _get_page_content(self, document_id: int, page_number: int) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            with self.db_connection.get_db_connection().cursor() as cursor:
                cursor.execute("""
                    SELECT content FROM checkable_elements
                    WHERE checkable_document_id = %s AND page_number = %s AND element_type = 'page'
                """, (document_id, page_number))
                result = cursor.fetchone()
                return result[0] if result else ""
        except Exception as e:
            logger.error(f"Error getting page content: {e}")
            return ""
    
    async def _analyze_general_data_page_content(self, content: str, page_number: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            content_lower = content.lower()
            
            # –ö–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ª–∏—Å—Ç–µ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            required_elements = {
                "project_info": ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "—à–∏—Ñ—Ä –ø—Ä–æ–µ–∫—Ç–∞", "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"],
                "organization_info": ["–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–∑–∞–∫–∞–∑—á–∏–∫", "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫"],
                "technical_info": ["—Å—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                "approval_info": ["–≥–ª–∞–≤–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞", "–≥–ª–∞–≤–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞", "—É—Ç–≤–µ—Ä–¥–∏–ª", "—Å–æ–≥–ª–∞—Å–æ–≤–∞–ª"]
            }
            
            found_elements = {}
            for category, elements in required_elements.items():
                found_elements[category] = []
                for element in elements:
                    if element in content_lower:
                        found_elements[category].append(element)
            
            return {
                "page_number": page_number,
                "found_elements": found_elements,
                "content_length": len(content),
                "has_project_info": len(found_elements["project_info"]) > 0,
                "has_organization_info": len(found_elements["organization_info"]) > 0,
                "has_technical_info": len(found_elements["technical_info"]) > 0,
                "has_approval_info": len(found_elements["approval_info"]) > 0
            }
            
        except Exception as e:
            logger.error(f"Error analyzing general data page content: {e}")
            return {"error": str(e)}
    
    async def _analyze_general_data_stamp(self, content: str, page_number: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ –ª–∏—Å—Ç–∞ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            content_lower = content.lower()
            
            # –≠–ª–µ–º–µ–Ω—Ç—ã –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏
            stamp_elements = {
                "sheet_title": ["–ª–∏—Å—Ç", "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ", "–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è"],
                "sheet_number": ["–ª–∏—Å—Ç", "–Ω–æ–º–µ—Ä –ª–∏—Å—Ç–∞"],
                "project_mark": ["–º–∞—Ä–∫–∞", "—à–∏—Ñ—Ä"],
                "revision_info": ["–∏–∑–º–µ–Ω–µ–Ω–∏—è", "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "–ª–∏—Å—Ç"]
            }
            
            found_stamp_elements = {}
            for category, elements in stamp_elements.items():
                found_stamp_elements[category] = []
                for element in elements:
                    if element in content_lower:
                        found_stamp_elements[category].append(element)
            
            return {
                "page_number": page_number,
                "found_stamp_elements": found_stamp_elements,
                "has_sheet_title": len(found_stamp_elements["sheet_title"]) > 0,
                "has_sheet_number": len(found_stamp_elements["sheet_number"]) > 0,
                "has_project_mark": len(found_stamp_elements["project_mark"]) > 0,
                "has_revision_info": len(found_stamp_elements["revision_info"]) > 0
            }
            
        except Exception as e:
            logger.error(f"Error analyzing general data stamp: {e}")
            return {"error": str(e)}
    
    async def _check_general_data_compliance(self, content: str, page_number: int) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ª–∏—Å—Ç–∞ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"""
        try:
            findings = []
            content_lower = content.lower()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            required_checks = [
                {
                    "check": "project_name",
                    "description": "–ù–∞–ª–∏—á–∏–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞",
                    "keywords": ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "—à–∏—Ñ—Ä –ø—Ä–æ–µ–∫—Ç–∞"],
                    "severity": 4
                },
                {
                    "check": "organization_info",
                    "description": "–ù–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏",
                    "keywords": ["–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–∑–∞–∫–∞–∑—á–∏–∫", "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫"],
                    "severity": 4
                },
                {
                    "check": "design_stage",
                    "description": "–£–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
                    "keywords": ["—Å—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                    "severity": 3
                },
                {
                    "check": "approval_info",
                    "description": "–ù–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏",
                    "keywords": ["–≥–ª–∞–≤–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞", "–≥–ª–∞–≤–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞", "—É—Ç–≤–µ—Ä–¥–∏–ª"],
                    "severity": 3
                }
            ]
            
            for check in required_checks:
                found = any(keyword in content_lower for keyword in check["keywords"])
                if not found:
                    findings.append({
                        "type": "compliance_check",
                        "severity": check["severity"],
                        "category": "general_data_requirements",
                        "title": f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {check['description']}",
                        "description": f"–ù–∞ –ª–∏—Å—Ç–µ {page_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {check['description']}",
                        "recommendation": f"–î–æ–±–∞–≤–∏—Ç—å {check['description'].lower()} –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –ì–û–°–¢",
                        "page_number": page_number,
                        "check_type": check["check"]
                    })
            
            return findings
            
        except Exception as e:
            logger.error(f"Error checking general data compliance: {e}")
            return []
    
    def _determine_general_data_compliance_status(self, findings: List[Dict[str, Any]]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ª–∏—Å—Ç–æ–≤ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            if not findings:
                return "compliant"
            
            critical_findings = len([f for f in findings if f.get("severity", 0) >= 4])
            warning_findings = len([f for f in findings if f.get("severity", 0) == 3])
            
            if critical_findings > 0:
                return "non_compliant"
            elif warning_findings > 0:
                return "partially_compliant"
            else:
                return "compliant"
                
        except Exception as e:
            logger.error(f"Error determining compliance status: {e}")
            return "unknown"
    
    async def analyze_sections_detailed(self, document_id: int, sections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üìä [DETAILED_SECTIONS] Starting detailed analysis of {len(sections)} sections")
            
            detailed_analysis = {
                "sections_analysis": [],
                "total_sections": len(sections),
                "overall_compliance": "unknown"
            }
            
            for section in sections:
                section_type = section.get("section_type")
                start_page = section.get("start_page")
                end_page = section.get("end_page")
                section_name = section.get("section_name")
                
                logger.info(f"üìä [DETAILED_SECTIONS] Analyzing section: {section_name} (pages {start_page}-{end_page})")
                
                # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ–∫—Ü–∏–∏
                section_analysis = await self._analyze_specific_section(
                    document_id, section_type, start_page, end_page, section_name
                )
                
                detailed_analysis["sections_analysis"].append({
                    "section_type": section_type,
                    "section_name": section_name,
                    "start_page": start_page,
                    "end_page": end_page,
                    "pages_count": end_page - start_page + 1,
                    "analysis": section_analysis
                })
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            detailed_analysis["overall_compliance"] = self._determine_detailed_compliance_status(
                detailed_analysis["sections_analysis"]
            )
            
            logger.info(f"üìä [DETAILED_SECTIONS] Detailed analysis completed. Overall compliance: {detailed_analysis['overall_compliance']}")
            return detailed_analysis
            
        except Exception as e:
            logger.error(f"Error in detailed sections analysis: {e}")
            return {"error": str(e)}
    
    async def _analyze_specific_section(self, document_id: int, section_type: str, start_page: int, end_page: int, section_name: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            analysis = {
                "section_type": section_type,
                "compliance_status": "unknown",
                "findings": [],
                "content_analysis": {},
                "recommendations": []
            }
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–µ–∫—Ü–∏–∏
            section_content = ""
            for page_num in range(start_page, end_page + 1):
                page_content = await self._get_page_content(document_id, page_num)
                section_content += f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---\n{page_content}"
            
            # –ê–Ω–∞–ª–∏–∑ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–µ–∫—Ü–∏–∏
            if section_type == "title":
                analysis = await self._analyze_title_section(section_content, start_page, end_page)
            elif section_type == "general_data":
                analysis = await self._analyze_general_data_section(section_content, start_page, end_page)
            elif section_type == "main_content":
                analysis = await self._analyze_main_content_section(section_content, start_page, end_page)
            elif section_type == "specification":
                analysis = await self._analyze_specification_section(section_content, start_page, end_page)
            elif section_type == "details":
                analysis = await self._analyze_details_section(section_content, start_page, end_page)
            else:
                analysis = await self._analyze_unknown_section(section_content, start_page, end_page)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing specific section: {e}")
            return {"error": str(e)}
    
    async def _analyze_title_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ '–¢–∏—Ç—É–ª'"""
        try:
            content_lower = content.lower()
            findings = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            required_elements = [
                {
                    "element": "project_name",
                    "description": "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
                    "keywords": ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "—à–∏—Ñ—Ä –ø—Ä–æ–µ–∫—Ç–∞"],
                    "severity": 4
                },
                {
                    "element": "organization_info",
                    "description": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏",
                    "keywords": ["–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–∑–∞–∫–∞–∑—á–∏–∫", "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫"],
                    "severity": 4
                },
                {
                    "element": "design_stage",
                    "description": "–°—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
                    "keywords": ["—Å—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                    "severity": 3
                }
            ]
            
            for element in required_elements:
                found = any(keyword in content_lower for keyword in element["keywords"])
                if not found:
                    findings.append({
                        "type": "missing_element",
                        "severity": element["severity"],
                        "category": "title_requirements",
                        "title": f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {element['description']}",
                        "description": f"–í —Å–µ–∫—Ü–∏–∏ '–¢–∏—Ç—É–ª' (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {element['description']}",
                        "recommendation": f"–î–æ–±–∞–≤–∏—Ç—å {element['description'].lower()} –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –ì–û–°–¢",
                        "section": "title"
                    })
            
            compliance_status = "compliant" if not findings else "non_compliant"
            if findings and all(f.get("severity", 0) < 4 for f in findings):
                compliance_status = "partially_compliant"
            
            return {
                "section_type": "title",
                "compliance_status": compliance_status,
                "findings": findings,
                "content_analysis": {
                    "has_project_info": any(keyword in content_lower for keyword in ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞", "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"]),
                    "has_organization_info": any(keyword in content_lower for keyword in ["–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–∑–∞–∫–∞–∑—á–∏–∫"]),
                    "has_design_stage": any(keyword in content_lower for keyword in ["—Å—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"]),
                    "content_length": len(content)
                },
                "recommendations": [
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    "–£–±–µ–¥–∏—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ì–û–°–¢"
                ]
            }
            
        except Exception as e:
            logger.error(f"Error analyzing title section: {e}")
            return {"error": str(e)}
    
    async def _analyze_general_data_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ'"""
        try:
            content_lower = content.lower()
            findings = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ª–∏—Å—Ç–∞ "–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"
            required_elements = [
                {
                    "element": "general_instructions",
                    "description": "–û–±—â–∏–µ —É–∫–∞–∑–∞–Ω–∏—è",
                    "keywords": ["–æ–±—â–∏–µ —É–∫–∞–∑–∞–Ω–∏—è", "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ", "–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è"],
                    "severity": 4
                },
                {
                    "element": "technical_requirements",
                    "description": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è",
                    "keywords": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–Ω–æ—Ä–º—ã"],
                    "severity": 3
                },
                {
                    "element": "materials_specification",
                    "description": "–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤",
                    "keywords": ["–º–∞—Ç–µ—Ä–∏–∞–ª—ã", "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è", "–º–∞—Ä–∫–∞"],
                    "severity": 3
                }
            ]
            
            for element in required_elements:
                found = any(keyword in content_lower for keyword in element["keywords"])
                if not found:
                    findings.append({
                        "type": "missing_element",
                        "severity": element["severity"],
                        "category": "general_data_requirements",
                        "title": f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {element['description']}",
                        "description": f"–í —Å–µ–∫—Ü–∏–∏ '–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ' (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {element['description']}",
                        "recommendation": f"–î–æ–±–∞–≤–∏—Ç—å {element['description'].lower()} –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –ì–û–°–¢",
                        "section": "general_data"
                    })
            
            compliance_status = "compliant" if not findings else "non_compliant"
            if findings and all(f.get("severity", 0) < 4 for f in findings):
                compliance_status = "partially_compliant"
            
            return {
                "section_type": "general_data",
                "compliance_status": compliance_status,
                "findings": findings,
                "content_analysis": {
                    "has_general_instructions": any(keyword in content_lower for keyword in ["–æ–±—â–∏–µ —É–∫–∞–∑–∞–Ω–∏—è", "–æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ"]),
                    "has_technical_requirements": any(keyword in content_lower for keyword in ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"]),
                    "has_materials_specification": any(keyword in content_lower for keyword in ["–º–∞—Ç–µ—Ä–∏–∞–ª—ã", "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è"]),
                    "content_length": len(content)
                },
                "recommendations": [
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –æ–±—â–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π",
                    "–£–±–µ–¥–∏—Ç—å—Å—è –≤ –Ω–∞–ª–∏—á–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π",
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"
                ]
            }
            
        except Exception as e:
            logger.error(f"Error analyzing general data section: {e}")
            return {"error": str(e)}
    
    async def _analyze_main_content_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ '–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'"""
        try:
            content_lower = content.lower()
            findings = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            if len(content.strip()) < 100:
                findings.append({
                    "type": "insufficient_content",
                    "severity": 2,
                    "category": "content_quality",
                    "title": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
                    "description": f"–°–µ–∫—Ü–∏—è '–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}) —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞–ª–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
                    "recommendation": "–î–æ–ø–æ–ª–Ω–∏—Ç—å —Å–µ–∫—Ü–∏—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º",
                    "section": "main_content"
                })
            
            compliance_status = "compliant" if not findings else "partially_compliant"
            
            return {
                "section_type": "main_content",
                "compliance_status": compliance_status,
                "findings": findings,
                "content_analysis": {
                    "content_length": len(content),
                    "has_technical_content": any(keyword in content_lower for keyword in ["—á–µ—Ä—Ç–µ–∂", "–ø–ª–∞–Ω", "—Å—Ö–µ–º–∞"]),
                    "has_descriptions": any(keyword in content_lower for keyword in ["–æ–ø–∏—Å–∞–Ω–∏–µ", "—É–∫–∞–∑–∞–Ω–∏—è", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"])
                },
                "recommendations": [
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è",
                    "–£–±–µ–¥–∏—Ç—å—Å—è –≤ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π"
                ]
            }
            
        except Exception as e:
            logger.error(f"Error analyzing main content section: {e}")
            return {"error": str(e)}
    
    async def _analyze_specification_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ '–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è'"""
        try:
            content_lower = content.lower()
            findings = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è" not in content_lower and "–≤–µ–¥–æ–º–æ—Å—Ç—å" not in content_lower:
                findings.append({
                    "type": "missing_specification",
                    "severity": 3,
                    "category": "specification_requirements",
                    "title": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è",
                    "description": f"–í —Å–µ–∫—Ü–∏–∏ '–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è' (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–ª–∏ –≤–µ–¥–æ–º–æ—Å—Ç—å",
                    "recommendation": "–î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–ª–∏ –≤–µ–¥–æ–º–æ—Å—Ç—å",
                    "section": "specification"
                })
            
            compliance_status = "compliant" if not findings else "partially_compliant"
            
            return {
                "section_type": "specification",
                "compliance_status": compliance_status,
                "findings": findings,
                "content_analysis": {
                    "has_specification": "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è" in content_lower,
                    "has_vedomost": "–≤–µ–¥–æ–º–æ—Å—Ç—å" in content_lower,
                    "content_length": len(content)
                },
                "recommendations": [
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤",
                    "–£–±–µ–¥–∏—Ç—å—Å—è –≤ –ø–æ–ª–Ω–æ—Ç–µ –≤–µ–¥–æ–º–æ—Å—Ç–µ–π"
                ]
            }
            
        except Exception as e:
            logger.error(f"Error analyzing specification section: {e}")
            return {"error": str(e)}
    
    async def _analyze_details_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ '–£–∑–ª—ã –∏ –¥–µ—Ç–∞–ª–∏'"""
        try:
            content_lower = content.lower()
            findings = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —É–∑–ª–æ–≤ –∏ –¥–µ—Ç–∞–ª–µ–π
            if "—É–∑–µ–ª" not in content_lower and "–¥–µ—Ç–∞–ª—å" not in content_lower:
                findings.append({
                    "type": "missing_details",
                    "severity": 2,
                    "category": "details_requirements",
                    "title": "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —É–∑–ª—ã –∏ –¥–µ—Ç–∞–ª–∏",
                    "description": f"–í —Å–µ–∫—Ü–∏–∏ '–£–∑–ª—ã –∏ –¥–µ—Ç–∞–ª–∏' (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —É–∑–ª—ã –∏–ª–∏ –¥–µ—Ç–∞–ª–∏",
                    "recommendation": "–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–∑–ª–∞—Ö –∏ –¥–µ—Ç–∞–ª—è—Ö",
                    "section": "details"
                })
            
            compliance_status = "compliant" if not findings else "partially_compliant"
            
            return {
                "section_type": "details",
                "compliance_status": compliance_status,
                "findings": findings,
                "content_analysis": {
                    "has_nodes": "—É–∑–µ–ª" in content_lower,
                    "has_details": "–¥–µ—Ç–∞–ª—å" in content_lower,
                    "content_length": len(content)
                },
                "recommendations": [
                    "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —É–∑–ª–æ–≤",
                    "–£–±–µ–¥–∏—Ç—å—Å—è –≤ –Ω–∞–ª–∏—á–∏–∏ –¥–µ—Ç–∞–ª–µ–π"
                ]
            }
            
        except Exception as e:
            logger.error(f"Error analyzing details section: {e}")
            return {"error": str(e)}
    
    async def _analyze_unknown_section(self, content: str, start_page: int, end_page: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π —Å–µ–∫—Ü–∏–∏"""
        return {
            "section_type": "unknown",
            "compliance_status": "unknown",
            "findings": [],
            "content_analysis": {
                "content_length": len(content)
            },
            "recommendations": [
                "–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å–µ–∫—Ü–∏–∏",
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"
            ]
        }
    
    def _determine_detailed_compliance_status(self, sections_analysis: List[Dict[str, Any]]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–æ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É —Å–µ–∫—Ü–∏–π"""
        try:
            if not sections_analysis:
                return "unknown"
            
            all_findings = []
            for section in sections_analysis:
                analysis = section.get("analysis", {})
                findings = analysis.get("findings", [])
                all_findings.extend(findings)
            
            if not all_findings:
                return "compliant"
            
            critical_findings = len([f for f in all_findings if f.get("severity", 0) >= 4])
            warning_findings = len([f for f in all_findings if f.get("severity", 0) == 3])
            
            if critical_findings > 0:
                return "non_compliant"
            elif warning_findings > 0:
                return "partially_compliant"
            else:
                return "compliant"
                
        except Exception as e:
            logger.error(f"Error determining detailed compliance status: {e}")
            return "unknown"
    
    def determine_overall_status(self, norm_compliance_results: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        try:
            critical_findings = norm_compliance_results.get("critical_findings", 0)
            warning_findings = norm_compliance_results.get("warning_findings", 0)
            compliance_percentage = norm_compliance_results.get("compliance_percentage", 0)
            
            if critical_findings > 0:
                return "fail"
            elif warning_findings > 0 or compliance_percentage < 80:
                return "warning"
            else:
                return "pass"
                
        except Exception as e:
            logger.error(f"Error determining overall status: {e}")
            return "unknown"
    
    def get_first_page_content(self, document_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            with self.db_connection.get_db_connection().cursor() as cursor:
                cursor.execute("""
                    SELECT element_content, page_number
                    FROM checkable_elements
                    WHERE checkable_document_id = %s AND page_number = 1
                    ORDER BY id
                    LIMIT 1
                """, (document_id,))
                
                result = cursor.fetchone()
                if result:
                    return {
                        "content": result[0],  # element_content
                        "page_number": result[1]  # page_number
                    }
                return None
                
        except Exception as e:
            logger.error(f"Error getting first page content: {e}")
            return None
    
    def get_all_pages_content(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        try:
            with self.db_connection.get_db_connection().cursor() as cursor:
                cursor.execute("""
                    SELECT element_content, page_number
                    FROM checkable_elements
                    WHERE checkable_document_id = %s
                    ORDER BY page_number, id
                """, (document_id,))
                
                pages = []
                for row in cursor.fetchall():
                    pages.append({
                        "content": row[0],  # element_content
                        "page_number": row[1]  # page_number
                    })
                
                return pages
                
        except Exception as e:
            logger.error(f"Error getting all pages content: {e}")
            return []
    
    def get_pages_sizes(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_connection.get_db_connection().cursor() as cursor:
                cursor.execute("""
                    SELECT element_metadata, page_number
                    FROM checkable_elements
                    WHERE checkable_document_id = %s AND element_type = 'page'
                    ORDER BY page_number
                """, (document_id,))
                
                page_sizes = []
                for row in cursor.fetchall():
                    metadata = row[0]  # element_metadata (JSONB)
                    page_number = row[1]  # page_number
                    
                    if metadata and isinstance(metadata, dict):
                        width = metadata.get('page_width')
                        height = metadata.get('page_height')
                        
                        if width and height:
                            page_sizes.append({
                                "page_number": page_number,
                                "width": float(width),
                                "height": float(height)
                            })
                
                return page_sizes
                
        except Exception as e:
            logger.error(f"Error getting pages sizes: {e}")
            return []
    
    async def save_hierarchical_check_result(self, document_id: int, result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        try:
            def _save_result(conn):
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO hierarchical_check_results 
                        (checkable_document_id, analysis_date, check_type, execution_time,
                         project_info, norm_compliance_summary, sections_analysis, overall_status)
                        VALUES (%s, CURRENT_TIMESTAMP, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (
                        document_id,
                        result.get("check_type", "hierarchical"),
                        result.get("execution_time", 0),
                        json.dumps(result.get("stages", {}).get("first_page_analysis", {}).get("project_info", {})),
                        json.dumps(result.get("stages", {}).get("norm_compliance", {})),
                        json.dumps(result.get("stages", {}).get("section_analysis", {})),
                        result.get("summary", {}).get("overall_status", "unknown")
                    ))
                    
                    result_id = cursor.fetchone()[0]
                    logger.info(f"Saved hierarchical check result {result_id} for document {document_id}")
                    return result_id
            
            result_id = self.db_connection.execute_in_transaction(_save_result)
            return result_id
                
        except Exception as e:
            logger.error(f"Save hierarchical check result error: {e}")
            raise
    
    def _get_relevant_ntd(self, document_set: str, project_stage: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ù–¢–î –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –∏ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            logger.info(f"üîç [NTD_DETERMINATION] Determining relevant NTD for document_set: {document_set}, project_stage: {project_stage}")
            
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –ù–¢–î –¥–ª—è –º–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞
            base_ntd = self.ntd_by_mark.get(document_set, [])
            logger.info(f"üîç [NTD_DETERMINATION] Base NTD for {document_set}: {len(base_ntd)} documents")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ù–¢–î –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            stage_specific_ntd = []
            if project_stage == "–†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è":
                stage_specific_ntd = [
                    "–ì–û–°–¢ –† 21.101-2020",  # –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–∞–±–æ—á–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                    "–ì–û–°–¢ 21.501-2018"  # –ü—Ä–∞–≤–∏–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—á–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                ]
                logger.info(f"üîç [NTD_DETERMINATION] Added stage-specific NTD for –†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: {len(stage_specific_ntd)} documents")
            elif project_stage == "–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è":
                stage_specific_ntd = [
                    "–ì–û–°–¢ –† 21.101-2020",  # –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                    "–ü–ü –†–§ 87"  # –ü–æ–ª–æ–∂–µ–Ω–∏–µ –æ —Å–æ—Å—Ç–∞–≤–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                ]
                logger.info(f"üîç [NTD_DETERMINATION] Added stage-specific NTD for –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: {len(stage_specific_ntd)} documents")
            elif project_stage == "–≠—Å–∫–∏–∑–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è":
                stage_specific_ntd = [
                    "–°–ü 48.13330.2019",  # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                    "–ì–û–°–¢ –† 21.101-2020"  # –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                ]
                logger.info(f"üîç [NTD_DETERMINATION] Added stage-specific NTD for –≠—Å–∫–∏–∑–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: {len(stage_specific_ntd)} documents")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            all_ntd = base_ntd + stage_specific_ntd
            relevant_ntd = list(set(all_ntd))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            
            logger.info(f"üîç [NTD_DETERMINATION] Final relevant NTD list: {len(relevant_ntd)} documents")
            logger.debug(f"üîç [NTD_DETERMINATION] Relevant NTD: {relevant_ntd}")
            
            return relevant_ntd
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_DETERMINATION] Error determining relevant NTD: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –ù–¢–î –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return self.ntd_by_mark.get(document_set, [])
