import os
import hashlib
import json
import logging
import io
import psutil
import gc
import time
import signal
import sys
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams
import magic
import PyPDF2
from docx import Document
import re
import httpx
import asyncio
import tiktoken

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
import sys
sys.path.append('/app')
try:
    from config import MAX_CHECKABLE_DOCUMENT_SIZE, MAX_NORMATIVE_DOCUMENT_SIZE, LLM_REQUEST_TIMEOUT
except ImportError:
    # Fallback –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    MAX_CHECKABLE_DOCUMENT_SIZE = int(os.getenv("MAX_CHECKABLE_DOCUMENT_SIZE", "104857600"))  # 100 –ú–ë
    MAX_NORMATIVE_DOCUMENT_SIZE = int(os.getenv("MAX_NORMATIVE_DOCUMENT_SIZE", "209715200"))  # 200 –ú–ë
    LLM_REQUEST_TIMEOUT = int(os.getenv("LLM_REQUEST_TIMEOUT", "120"))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è graceful shutdown
shutdown_event = asyncio.Event()
is_shutting_down = False
startup_time = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞ datetime

def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown"""
    global is_shutting_down
    signal_name = {
        signal.SIGTERM: "SIGTERM",
        signal.SIGINT: "SIGINT",
        signal.SIGHUP: "SIGHUP",
        signal.SIGUSR1: "SIGUSR1",
        signal.SIGUSR2: "SIGUSR2"
    }.get(signum, f"Signal {signum}")
    
    logger.info(f"üîç [SHUTDOWN] Received {signal_name} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}, initiating graceful shutdown...")
    logger.info(f"üîç [SHUTDOWN] Process ID: {os.getpid()}, Parent PID: {os.getppid()}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ shutdown
    try:
        memory_info = get_memory_usage()
        if "error" not in memory_info:
            logger.info(f"üîç [SHUTDOWN] Memory usage before shutdown: RSS: {memory_info['rss_mb']:.1f}MB, VMS: {memory_info['vms_mb']:.1f}MB, Percent: {memory_info['percent']:.1f}%")
    except Exception as e:
        logger.warning(f"üîç [SHUTDOWN] Could not get memory info: {e}")
    
    is_shutting_down = True
    shutdown_event.set()

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

def get_memory_usage() -> Dict[str, float]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_percent = process.memory_percent()
        
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # RSS –≤ –ú–ë
            "vms_mb": memory_info.vms / 1024 / 1024,  # VMS –≤ –ú–ë
            "percent": memory_percent,  # –ü—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            "available_mb": psutil.virtual_memory().available / 1024 / 1024  # –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å –≤ –ú–ë
        }
    except Exception as e:
        logger.error(f"Error getting memory usage: {e}")
        return {"error": str(e)}

def get_available_memory() -> float:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –ú–ë"""
    try:
        return psutil.virtual_memory().available / 1024 / 1024
    except Exception as e:
        logger.error(f"Error getting available memory: {e}")
        return 0.0

def log_memory_usage(context: str = ""):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    try:
        memory_info = get_memory_usage()
        if "error" not in memory_info:
            logger.info(f"üîç [DEBUG] DocumentParser: Memory usage {context}: "
                       f"RSS: {memory_info['rss_mb']:.1f}MB, "
                       f"VMS: {memory_info['vms_mb']:.1f}MB, "
                       f"Percent: {memory_info['percent']:.1f}%, "
                       f"Available: {memory_info['available_mb']:.1f}MB")
        else:
            logger.warning(f"üîç [DEBUG] DocumentParser: Could not get memory usage {context}: {memory_info['error']}")
    except Exception as e:
        logger.error(f"Error in log_memory_usage: {e}")

def cleanup_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    try:
        gc.collect()
        logger.info(f"üîç [DEBUG] DocumentParser: Memory cleanup completed")
    except Exception as e:
        logger.error(f"Error in cleanup_memory: {e}")

def check_memory_pressure() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –ø–∞–º—è—Ç—å"""
    try:
        memory_info = get_memory_usage()
        if "error" in memory_info:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª—å—à–µ 80% –ø–∞–º—è—Ç–∏ –∏–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –º–µ–Ω—å—à–µ 500MB
        if memory_info['percent'] > 80 or memory_info['available_mb'] < 500:
            logger.warning(f"üîç [MEMORY] High memory pressure detected: "
                          f"Usage: {memory_info['percent']:.1f}%, "
                          f"Available: {memory_info['available_mb']:.1f}MB")
            return True
        return False
    except Exception as e:
        logger.error(f"Error checking memory pressure: {e}")
        return False

# OCR imports
import pytesseract
from PIL import Image
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import tempfile
import math

from datetime import datetime, timedelta

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º startup_time –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞ datetime
startup_time = datetime.now()

# PDF generation imports
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_RIGHT, TA_JUSTIFY
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont


# —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
class DocumentInspectionResult:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_id = None # ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_name = None # –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ "A9.5.MTH.04"
        self.document_type = None # pdf, docx, dwg, txt
        self.document_engineering_stage = None # –ü–î, –†–î, –¢–≠–û
        self.document_mark = None # –ú–∞—Ä–∫–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: –ö–ñ, –ö–ú, –ê–°, –¢–•, –ö–°, –ö–ü, –ö–†, –ö–†–°, –ö–†–°-1, –ö–†–°-2, –ö–†–°-3, –ö–†–°-4, –ö–†–°-5, –ö–†–°-6, –ö–†–°-7, –ö–†–°-8, –ö–†–°-9, –ö–†–°-10
        self.document_number = None # –ù–æ–º–µ—Ä –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_date = None # –î–∞—Ç–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_author = None # –†–ê–ó–†–ê–ë–û–¢–ê–õ –∫–æ–º–ø–ª–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_reviewer = None # –ü–†–û–í–ï–†–ò–õ –∫–æ–º–ø–ª–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_approver = None # –ì–ò–ü –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_status = None # –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: IFR, IFD, IFC, IFS, IFT, IFT-1, IFT-2, IFT-3, IFT-4, IFT-5, IFT-6, IFT-7, IFT-8, IFT-9, IFT-10
        self.document_size = None # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        self.document_pages = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_vector = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_scanned = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_unknown = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_total = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_total_a4_sheets_equivalent = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4 —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        self.document_pages_with_violations = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏
        self.document_pages_clean = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        
        # –û–±—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        self.document_total_violations = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        self.document_critical_violations = None # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_major_violations = None # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_minor_violations = None # –ú–µ–ª–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_info_violations = None # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        self.document_compliance_percentage = None # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (0-100)
        self.document_violations_per_page_avg = None # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        # –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–æ–ª—è (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.document_total_errors_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_warnings_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_info_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_suggestions_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_corrections_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_pages_results = [] # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞

class DocumentPageInspectionResult:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_number = None # –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.page_type = None # vector, scanned, unknown
        self.page_format = None # A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, B0, B1, B2, B3, B4, B5, B6, B7, B8, B9, B10
        self.page_width_mm = None # –®–∏—Ä–∏–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º
        self.page_height_mm = None # –í—ã—Å–æ—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º
        self.page_orientation = None # portrait, landscape
        self.page_a4_sheets_equivalent = None # –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –ª–∏—Å—Ç–æ–≤ –ê4
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.page_text = None # –¢–µ–∫—Å—Ç –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_method = None # –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_length = None # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR (–¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
        self.page_ocr_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR
        self.page_ocr_method = None # –ú–µ—Ç–æ–¥ OCR
        self.page_ocr_all_results = [] # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR
        
        # –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_violations_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_critical_violations = None # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_major_violations = None # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_minor_violations = None # –ú–µ–ª–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_info_violations = None # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_compliance_percentage = None # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        # –î–µ—Ç–∞–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_violations_details = [] # –î–µ—Ç–∞–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ


class DocumentViolationDetail:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª–µ–π –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"""
    
    def __init__(self):
        self.violation_type = None # critical, major, minor, info
        self.violation_category = None # general_requirements, text_part, graphical_part, specifications, assembly_drawings, detail_drawings, schemes
        self.violation_description = None # –û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.violation_clause = None # –ü—É–Ω–∫—Ç –Ω–æ—Ä–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ì–û–°–¢ –† 21.1101-2013 –ø.5.2)
        self.violation_severity = None # –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (1-5)
        self.violation_recommendation = None # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
        self.violation_location = None # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.violation_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–∏ (0-1)




# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º —Ç—Ä–µ–π—Å–ª–æ–≥–æ–º
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TraceLogger:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–π—Å–ª–æ–≥–∞ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    
    @staticmethod
    def log_step(step: str, details: str = "", document_id: int = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        doc_info = f"[DOC:{document_id}]" if document_id else ""
        logger.info(f"üîç [TRACE] {timestamp} {doc_info} STEP: {step} - {details}")
    
    @staticmethod
    def log_upload_start(filename: str, file_size: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        size_mb = file_size / (1024 * 1024)
        logger.info(f"üöÄ [UPLOAD] {timestamp} START: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{filename}' ({size_mb:.2f}MB)")
    
    @staticmethod
    def log_upload_success(filename: str, document_id: int, processing_time: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"‚úÖ [UPLOAD] {timestamp} [DOC:{document_id}] SUCCESS: –î–æ–∫—É–º–µ–Ω—Ç '{filename}' –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {processing_time:.2f}—Å")
    
    @staticmethod
    def log_indexing_start(document_id: int, pages_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"üìö [INDEX] {timestamp} [DOC:{document_id}] START: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {pages_count} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    @staticmethod
    def log_indexing_success(document_id: int, processing_time: float, tokens_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"‚úÖ [INDEX] {timestamp} [DOC:{document_id}] SUCCESS: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å, {tokens_count} —Ç–æ–∫–µ–Ω–æ–≤")
    
    @staticmethod
    def log_normcontrol_start(document_id: int, pages_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"üîç [NORMCONTROL] {timestamp} [DOC:{document_id}] START: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è {pages_count} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    @staticmethod
    def log_llm_request(document_id: int, page_number: int, prompt_length: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"ü§ñ [LLM] {timestamp} [DOC:{document_id}] REQUEST: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}, –ø—Ä–æ–º–ø—Ç {prompt_length} —Å–∏–º–≤–æ–ª–æ–≤")
    
    @staticmethod
    def log_llm_response(document_id: int, page_number: int, response_time: float, response_length: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"ü§ñ [LLM] {timestamp} [DOC:{document_id}] RESPONSE: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}, –≤—Ä–µ–º—è {response_time:.2f}—Å, –æ—Ç–≤–µ—Ç {response_length} —Å–∏–º–≤–æ–ª–æ–≤")
    
    @staticmethod
    def log_report_generation(document_id: int, findings_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        logger.info(f"üìÑ [REPORT] {timestamp} [DOC:{document_id}] GENERATION: –û—Ç—á–µ—Ç —Å {findings_count} –Ω–∞—Ö–æ–¥–∫–∞–º–∏")
    
    @staticmethod
    def log_error(step: str, error: str, document_id: int = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        doc_info = f"[DOC:{document_id}]" if document_id else ""
        logger.error(f"‚ùå [ERROR] {timestamp} {doc_info} {step}: {error}")

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç—Ä–µ–π—Å–ª–æ–≥–≥–µ—Ä–∞
trace_logger = TraceLogger()

app = FastAPI(title="Document Parser Service", version="1.0.0")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤ –¥–æ 100MB
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class LargeFileMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if request.url.path == "/upload":
            request.scope["max_content_size"] = MAX_NORMATIVE_DOCUMENT_SIZE
        elif request.url.path == "/upload/checkable":
            request.scope["max_content_size"] = MAX_CHECKABLE_DOCUMENT_SIZE
        return await call_next(request)

class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        request_id = f"req_{int(start_time * 1000)}"
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(f"üîç [REQUEST] {request_id}: {request.method} {request.url.path} started")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ shutdown
            if is_shutting_down and request.url.path not in ["/health", "/metrics"]:
                logger.warning(f"üîç [REQUEST] {request_id}: Service is shutting down, rejecting request")
                return JSONResponse(
                    status_code=503,
                    content={"error": "Service is shutting down", "request_id": request_id}
                )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞–º—è—Ç—å
            if check_memory_pressure():
                logger.warning(f"üîç [REQUEST] {request_id}: High memory pressure detected")
                cleanup_memory()
            
            response = await call_next(request)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            duration = time.time() - start_time
            logger.info(f"üîç [REQUEST] {request_id}: {request.method} {request.url.path} completed in {duration:.3f}s (status: {response.status_code})")
            
            return response
            
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            duration = time.time() - start_time
            logger.error(f"üîç [REQUEST] {request_id}: {request.method} {request.url.path} failed after {duration:.3f}s: {e}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É 500
            return JSONResponse(
                status_code=500,
                content={
                    "error": "Internal server error",
                    "request_id": request_id,
                    "timestamp": datetime.now().isoformat()
                }
            )

app.add_middleware(LargeFileMiddleware)
app.add_middleware(ErrorHandlingMiddleware)

# CORS middleware —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤—ã—à–µ

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "norms-db")
POSTGRES_DB = os.getenv("POSTGRES_DB", "norms_db")
POSTGRES_USER = os.getenv("POSTGRES_USER", "norms_user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "norms_password")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "http://rag-service:8003")

class TransactionContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏ PostgreSQL"""
    
    def __init__(self, connection):
        self.connection = connection
        self.cursor = None
        self.transaction_id = f"tx_{int(time.time() * 1000)}"
    
    def __enter__(self):
        """–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
        try:
            self.cursor = self.connection.cursor()
            logger.debug(f"üîç [TRANSACTION] {self.transaction_id}: Transaction started")
            return self.connection
        except Exception as e:
            logger.error(f"üîç [TRANSACTION] {self.transaction_id}: Error starting transaction: {e}")
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º commit/rollback"""
        try:
            if exc_type is None:
                # –ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π - –∫–æ–º–º–∏—Ç–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                self.connection.commit()
                logger.debug(f"üîç [TRANSACTION] {self.transaction_id}: Transaction committed successfully")
            else:
                # –ï—Å—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è - –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                self.connection.rollback()
                logger.error(f"üîç [TRANSACTION] {self.transaction_id}: Transaction rolled back due to error: {exc_type.__name__}: {exc_val}")
        except Exception as e:
            logger.error(f"üîç [TRANSACTION] {self.transaction_id}: Error during transaction cleanup: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–∫–∞—Ç–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ—á–∏—Å—Ç–∫–∏
            try:
                if not self.connection.closed:
                    self.connection.rollback()
                    logger.debug(f"üîç [TRANSACTION] {self.transaction_id}: Emergency rollback completed")
            except Exception as rollback_error:
                logger.error(f"üîç [TRANSACTION] {self.transaction_id}: Emergency rollback failed: {rollback_error}")
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫—É—Ä—Å–æ—Ä
            if self.cursor:
                try:
                    self.cursor.close()
                    logger.debug(f"üîç [TRANSACTION] {self.transaction_id}: Cursor closed")
                except Exception as cursor_error:
                    logger.error(f"üîç [TRANSACTION] {self.transaction_id}: Error closing cursor: {cursor_error}")


class ReadOnlyTransactionContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è PostgreSQL"""
    
    def __init__(self, connection):
        self.connection = connection
        self.cursor = None
        self.transaction_id = f"read_tx_{int(time.time() * 1000)}"
    
    def __enter__(self):
        """–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è"""
        try:
            self.cursor = self.connection.cursor()
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è
            self.cursor.execute("SET TRANSACTION READ ONLY")
            logger.debug(f"üîç [READ_TRANSACTION] {self.transaction_id}: Read-only transaction started")
            return self.connection
        except Exception as e:
            logger.error(f"üîç [READ_TRANSACTION] {self.transaction_id}: Error starting read-only transaction: {e}")
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è"""
        try:
            if exc_type is None:
                # –ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π - –∫–æ–º–º–∏—Ç–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é (–¥–ª—è —á—Ç–µ–Ω–∏—è —ç—Ç–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ)
                self.connection.commit()
                logger.debug(f"üîç [READ_TRANSACTION] {self.transaction_id}: Read-only transaction committed successfully")
            else:
                # –ï—Å—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è - –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                self.connection.rollback()
                logger.error(f"üîç [READ_TRANSACTION] {self.transaction_id}: Read-only transaction rolled back due to error: {exc_type.__name__}: {exc_val}")
        except Exception as e:
            logger.error(f"üîç [READ_TRANSACTION] {self.transaction_id}: Error during read-only transaction cleanup: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–∫–∞—Ç–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ—á–∏—Å—Ç–∫–∏
            try:
                if not self.connection.closed:
                    self.connection.rollback()
                    logger.debug(f"üîç [READ_TRANSACTION] {self.transaction_id}: Emergency rollback completed")
            except Exception as rollback_error:
                logger.error(f"üîç [READ_TRANSACTION] {self.transaction_id}: Emergency rollback failed: {rollback_error}")
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫—É—Ä—Å–æ—Ä
            if self.cursor:
                try:
                    self.cursor.close()
                    logger.debug(f"üîç [READ_TRANSACTION] {self.transaction_id}: Cursor closed")
                except Exception as cursor_error:
                    logger.error(f"üîç [READ_TRANSACTION] {self.transaction_id}: Error closing cursor: {cursor_error}")


class DocumentParser:
    def __init__(self):
        self.db_conn = None
        self.qdrant_client = None
        self.connection_retry_count = 0
        self.max_retries = 3
        self.retry_delay = 5  # —Å–µ–∫—É–Ω–¥—ã
        self.connect_databases()
    
    def connect_databases(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        connection_start_time = datetime.now()
        logger.info(f"üîç [CONNECTION] Starting database connections at {connection_start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"üîç [CONNECTION] Attempt {attempt + 1}/{self.max_retries} to connect to databases")
                
                # PostgreSQL
                postgres_start_time = datetime.now()
                self.db_conn = psycopg2.connect(
                    host=POSTGRES_HOST,
                    database=POSTGRES_DB,
                    user=POSTGRES_USER,
                    password=POSTGRES_PASSWORD,
                    connect_timeout=10,
                    application_name="document_parser"
                )
                postgres_end_time = datetime.now()
                postgres_duration = (postgres_end_time - postgres_start_time).total_seconds()
                logger.info(f"üîç [CONNECTION] Connected to PostgreSQL in {postgres_duration:.3f}s")
                
                # Qdrant
                qdrant_start_time = datetime.now()
                self.qdrant_client = qdrant_client.QdrantClient(
                    host=QDRANT_HOST,
                    port=QDRANT_PORT,
                    timeout=10
                )
                qdrant_end_time = datetime.now()
                qdrant_duration = (qdrant_end_time - qdrant_start_time).total_seconds()
                logger.info(f"üîç [CONNECTION] Connected to Qdrant in {qdrant_duration:.3f}s")
                
                # –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏
                self.connection_retry_count = 0
                connection_end_time = datetime.now()
                total_duration = (connection_end_time - connection_start_time).total_seconds()
                logger.info(f"üîç [CONNECTION] All database connections established successfully in {total_duration:.3f}s")
                return
                
            except Exception as e:
                self.connection_retry_count += 1
                logger.error(f"üîç [CONNECTION] Database connection error (attempt {attempt + 1}/{self.max_retries}): {e}")
                logger.error(f"üîç [CONNECTION] Error type: {type(e).__name__}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"üîç [CONNECTION] Retrying in {self.retry_delay} seconds...")
                    time.sleep(self.retry_delay)
                else:
                    connection_end_time = datetime.now()
                    total_duration = (connection_end_time - connection_start_time).total_seconds()
                    logger.error(f"üîç [CONNECTION] Failed to connect to databases after {self.max_retries} attempts in {total_duration:.3f}s")
                    raise
    
    def get_db_connection(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∞–∫—Ç–∏–≤–Ω–æ
            if self.db_conn is None or self.db_conn.closed:
                logger.info("üîç [CONNECTION] Reconnecting to PostgreSQL...")
                self.db_conn = psycopg2.connect(
                    host=POSTGRES_HOST,
                    database=POSTGRES_DB,
                    user=POSTGRES_USER,
                    password=POSTGRES_PASSWORD,
                    connect_timeout=10,
                    application_name="document_parser"
                )
                logger.info("üîç [CONNECTION] Reconnected to PostgreSQL")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            try:
                with self.db_conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    cursor.fetchone()
            except Exception as test_error:
                logger.warning(f"üîç [CONNECTION] Database connection test failed: {test_error}")
                # –ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –ø—Ä–∏ –æ—à–∏–±–∫–µ —Ç–µ—Å—Ç–∞
                if self.db_conn and not self.db_conn.closed:
                    try:
                        self.db_conn.close()
                    except:
                        pass
                
                self.db_conn = psycopg2.connect(
                    host=POSTGRES_HOST,
                    database=POSTGRES_DB,
                    user=POSTGRES_USER,
                    password=POSTGRES_PASSWORD,
                    connect_timeout=10,
                    application_name="document_parser"
                )
                logger.info("üîç [CONNECTION] Reconnected to PostgreSQL after test failure")
            
            return self.db_conn
            
        except Exception as e:
            logger.error(f"üîç [CONNECTION] Database connection check failed: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
            try:
                if self.db_conn and not self.db_conn.closed:
                    self.db_conn.close()
            except Exception as close_error:
                logger.error(f"üîç [CONNECTION] Error closing connection: {close_error}")
            
            try:
                self.db_conn = psycopg2.connect(
                    host=POSTGRES_HOST,
                    database=POSTGRES_DB,
                    user=POSTGRES_USER,
                    password=POSTGRES_PASSWORD,
                    connect_timeout=10,
                    application_name="document_parser"
                )
                logger.info("üîç [CONNECTION] Reconnected to PostgreSQL after error")
                return self.db_conn
            except Exception as reconnect_error:
                logger.error(f"üîç [CONNECTION] Failed to reconnect: {reconnect_error}")
                raise
    
    def transaction_context(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏"""
        return TransactionContext(self.get_db_connection())
    
    def read_only_transaction_context(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è"""
        return ReadOnlyTransactionContext(self.get_db_connection())
    
    def execute_in_transaction(self, operation, *args, **kwargs):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º"""
        with self.transaction_context() as conn:
            return operation(conn, *args, **kwargs)
    
    def execute_in_read_only_transaction(self, operation, *args, **kwargs):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è"""
        with self.read_only_transaction_context() as conn:
            return operation(conn, *args, **kwargs)
    def calculate_file_hash(self, file_content: bytes) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHA-256 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        return hashlib.sha256(file_content).hexdigest()

    def extract_text_from_image(self, image: Image.Image, page_number: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OCR"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL Image –≤ OpenCV —Ñ–æ—Ä–º–∞—Ç
            opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è OCR
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è
            # 1. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ—Ä–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            
            # 2. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞
            kernel = np.ones((1, 1), np.uint8)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
            
            # 3. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OCR –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            custom_config = r'--oem 3 --psm 6 -l rus+eng'
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
            results = []
            
            # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            try:
                text1 = pytesseract.image_to_string(image, config=custom_config)
                if text1.strip():
                    results.append(("original", text1.strip(), 0.7))
            except Exception as e:
                logger.warning(f"OCR failed on original image for page {page_number}: {e}")
            
            # 2. –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ—Ä–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            try:
                text2 = pytesseract.image_to_string(thresh, config=custom_config)
                if text2.strip():
                    results.append(("threshold", text2.strip(), 0.8))
            except Exception as e:
                logger.warning(f"OCR failed on threshold image for page {page_number}: {e}")
            
            # 3. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (CLAHE)
            try:
                text3 = pytesseract.image_to_string(enhanced, config=custom_config)
                if text3.strip():
                    results.append(("enhanced", text3.strip(), 0.9))
            except Exception as e:
                logger.warning(f"OCR failed on enhanced image for page {page_number}: {e}")
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if results:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                results.sort(key=lambda x: (len(x[1]), x[2]), reverse=True)
                best_method, best_text, confidence = results[0]
                
                logger.info(f"OCR completed for page {page_number} using {best_method} method, confidence: {confidence}")
                
                return {
                    "text": best_text,
                    "confidence": confidence,
                    "method": best_method,
                    "all_results": results
                }
            else:
                logger.warning(f"No OCR results for page {page_number}")
                return {
                    "text": "",
                    "confidence": 0.0,
                    "method": "none",
                    "all_results": []
                }
                
        except Exception as e:
            logger.error(f"OCR processing error for page {page_number}: {e}")
            return {
                "text": "",
                "confidence": 0.0,
                "method": "error",
                "all_results": []
            }

    def get_page_format_info(self, page) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ç–æ—á–∫–∞—Ö (1/72 –¥—é–π–º–∞)
            media_box = page.mediabox
            width_pt = float(media_box.width)
            height_pt = float(media_box.height)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–∏–ª–ª–∏–º–µ—Ç—Ä—ã (1 –¥—é–π–º = 25.4 –º–º, 1 —Ç–æ—á–∫–∞ = 1/72 –¥—é–π–º–∞)
            width_mm = width_pt * 25.4 / 72
            height_mm = height_pt * 25.4 / 72
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            format_name = self.determine_page_format(width_mm, height_mm)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
            a4_sheets = self.calculate_a4_sheets(width_mm, height_mm)
            
            return {
                "width_pt": width_pt,
                "height_pt": height_pt,
                "width_mm": round(width_mm, 2),
                "height_mm": round(height_mm, 2),
                "format_name": format_name,
                "a4_sheets": a4_sheets,
                "orientation": "portrait" if height_mm > width_mm else "landscape"
            }
            
        except Exception as e:
            logger.error(f"Error getting page format info: {e}")
            return {
                "width_pt": 0,
                "height_pt": 0,
                "width_mm": 0,
                "height_mm": 0,
                "format_name": "unknown",
                "a4_sheets": 0,
                "orientation": "unknown"
            }

    def determine_page_format(self, width_mm: float, height_mm: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤ –º–º (—à–∏—Ä–∏–Ω–∞ x –≤—ã—Å–æ—Ç–∞)
        formats = {
            "A0": (841, 1189),
            "A1": (594, 841),
            "A2": (420, 594),
            "A3": (297, 420),
            "A4": (210, 297),
            "A5": (148, 210),
            "A6": (105, 148),
            "A7": (74, 105),
            "A8": (52, 74),
            "A9": (37, 52),
            "A10": (26, 37),
            "B0": (1000, 1414),
            "B1": (707, 1000),
            "B2": (500, 707),
            "B3": (353, 500),
            "B4": (250, 353),
            "B5": (176, 250),
            "B6": (125, 176),
            "B7": (88, 125),
            "B8": (62, 88),
            "B9": (44, 62),
            "B10": (31, 44),
        }
        
        # –î–æ–ø—É—Å—Ç–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –º–º
        tolerance = 5
        
        for format_name, (std_width, std_height) in formats.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ–±–µ–∏—Ö –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è—Ö
            if (abs(width_mm - std_width) <= tolerance and abs(height_mm - std_height) <= tolerance) or \
               (abs(width_mm - std_height) <= tolerance and abs(height_mm - std_width) <= tolerance):
                return format_name
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π
        return f"custom_{int(width_mm)}x{int(height_mm)}"

    def calculate_a4_sheets(self, width_mm: float, height_mm: float) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Å—Ç–æ–≤ –ê4 (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º)"""
        try:
            # –ü–ª–æ—â–∞–¥—å –ê4 –≤ –º–º¬≤
            a4_area = 210 * 297  # 62,370 –º–º¬≤
            
            # –ü–ª–æ—â–∞–¥—å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º¬≤
            page_area = width_mm * height_mm
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
            a4_sheets = page_area / a4_area
            
            # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 2 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            return round(a4_sheets, 2)
            
        except Exception as e:
            logger.error(f"Error calculating A4 sheets: {e}")
            return 0.0

    def detect_page_type(self, page) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = page.extract_text()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –ª–µ–≥–∫–æ –∏ –µ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π
            if text and len(text.strip()) > 50:
                return "vector"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if '/XObject' in page['/Resources']:
                xObject = page['/Resources']['/XObject'].get_object()
                for obj in xObject:
                    if xObject[obj]['/Subtype'] == '/Image':
                        return "scanned"
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å—á–∏—Ç–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π
            if not text or len(text.strip()) < 10:
                return "scanned"
            
            return "vector"
            
        except Exception as e:
            logger.warning(f"Error detecting page type: {e}")
            return "unknown"

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º cl100k_base –∫–æ–¥–∏—Ä–æ–≤–∫—É (GPT-4, GPT-3.5-turbo)
            encoding = tiktoken.get_encoding("cl100k_base")
            tokens = encoding.encode(text)
            return len(tokens)
        except Exception as e:
            logger.error(f"Token counting error: {e}")
            # Fallback: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –ø–æ —Å–ª–æ–≤–∞–º
            return len(text.split())

    def calculate_document_tokens(self, elements: List[Dict[str, Any]]) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        total_tokens = 0
        for element in elements:
            content = element.get("element_content", "")
            if content:
                total_tokens += self.count_tokens(content)
        return total_tokens
    
    def detect_file_type(self, file_content: bytes) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        print(f"üîç [DEBUG] DocumentParser: detect_file_type called with content length: {len(file_content)}")
        print(f"üîç [DEBUG] DocumentParser: Content preview: {file_content[:100]}...")
        
        mime_type = magic.from_buffer(file_content, mime=True)
        print(f"üîç [DEBUG] DocumentParser: Magic detected MIME type: {mime_type}")
        
        if mime_type == "application/pdf":
            print(f"üîç [DEBUG] DocumentParser: Detected as PDF")
            return "pdf"
        else:
            print(f"üîç [DEBUG] DocumentParser: Unsupported MIME type: {mime_type}")
            raise ValueError(f"Only PDF files are supported. Detected MIME type: {mime_type}")

    def parse_file(self, file_content: bytes, file_type: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
        if file_type == "pdf":
            return self.parse_pdf(file_content)
        elif file_type == "docx":
            return self.parse_docx(file_content)
        elif file_type == "dwg":
            return self.parse_dwg(file_content)
        elif file_type == "txt":
            return self.parse_text(file_content)
        else:
            raise ValueError(f"Unsupported file type for parsing: {file_type}")

    def parse_text(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        elements = []
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            text_content = file_content.decode('utf-8')
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            lines = text_content.split('\n')
            
            for i, line in enumerate(lines, 1):
                line = line.strip()
                if line:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    elements.append({
                        "element_type": "text",
                        "element_content": line,
                        "page_number": 1,
                        "confidence_score": 1.0
                    })
            
            return elements
            
        except Exception as e:
            logger.error(f"Text parsing error: {e}")
            raise
    
    def parse_pdf(self, file_content: bytes) -> DocumentInspectionResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        logger.info(f"üîç [DEBUG] DocumentParser: Starting PDF parsing for {len(file_content)} bytes")
        log_memory_usage("before PDF parsing")
        result = DocumentInspectionResult()
        document_format_stats = {
            "total_pages": 0,
            "total_a4_sheets": 0.0,
            "formats": {},
            "orientations": {"portrait": 0, "landscape": 0},
            "page_types": {"vector": 0, "scanned": 0, "unknown": 0}
        }
        
        try:
            logger.info(f"üîç [DEBUG] DocumentParser: Creating PDF reader from {len(file_content)} bytes")
            # –°–æ–∑–¥–∞–µ–º file-like –æ–±—ä–µ–∫—Ç –∏–∑ bytes
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            logger.info(f"üîç [DEBUG] DocumentParser: PDF reader created successfully")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            document_info = self.get_document_info(pdf_reader)
            result.document_name = document_info["name"]
            result.document_type = document_info["type"]
            result.document_engineering_stage = document_info["engineering_stage"]
            result.document_mark = document_info["mark"]
            result.document_number = document_info["number"]
            result.document_date = document_info["date"]
            result.document_author = document_info["author"]
            result.document_reviewer = document_info["reviewer"]
            result.document_approver = document_info["approver"]
            result.document_status = document_info["status"]
            result.document_size = document_info["size"]
            # –ø–æ–∫–∞–∂–µ–º –≤ –ª–æ–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            logger.info(f"Document info: {document_info}")
            
            
            document_format_stats["total_pages"] = len(pdf_reader.pages)
            logger.info(f"Processing PDF with {len(pdf_reader.pages)} pages")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR
            try:
                images = convert_from_bytes(file_content, dpi=300)
                logger.info(f"Converted PDF to {len(images)} images")
            except Exception as e:
                logger.warning(f"Failed to convert PDF to images: {e}")
                images = []
            
            logger.info(f"üîç [DEBUG] DocumentParser: Starting to process {len(pdf_reader.pages)} pages")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            logger.info(f"üîç [DEBUG] DocumentParser: PDF document parameters:")
            logger.info(f"üîç [DEBUG] DocumentParser: - Total pages: {len(pdf_reader.pages)}")
            logger.info(f"üîç [DEBUG] DocumentParser: - File size: {len(file_content)} bytes")
            logger.info(f"üîç [DEBUG] DocumentParser: - Average page size: {len(file_content) // len(pdf_reader.pages)} bytes")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            BATCH_SIZE = 5  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 5 —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑
            total_batches = (len(pdf_reader.pages) + BATCH_SIZE - 1) // BATCH_SIZE
            logger.info(f"üîç [DEBUG] DocumentParser: Batch processing settings:")
            logger.info(f"üîç [DEBUG] DocumentParser: - Batch size: {BATCH_SIZE} pages")
            logger.info(f"üîç [DEBUG] DocumentParser: - Total batches: {total_batches}")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –±–∞—Ç—á–∞–º–∏
            for batch_num in range(total_batches):
                start_page = batch_num * BATCH_SIZE
                end_page = min((batch_num + 1) * BATCH_SIZE, len(pdf_reader.pages))
                
                logger.info(f"üîç [DEBUG] DocumentParser: Starting batch {batch_num + 1}/{total_batches} (pages {start_page + 1}-{end_page})")
                log_memory_usage(f"before batch {batch_num + 1}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ
                for page_num in range(start_page, end_page):
                    page = pdf_reader.pages[page_num]
                    page_number = page_num + 1
                    
                    logger.info(f"üîç [DEBUG] DocumentParser: Processing page {page_number}/{len(pdf_reader.pages)} in batch {batch_num + 1}")
                    
                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result = DocumentPageInspectionResult()
                    page_result.page_number = page_number
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    try:
                        page_text = page.extract_text()
                        page_size = len(page_text) if page_text else 0
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number} parameters:")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Text size: {page_size} characters")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Has text: {bool(page_text)}")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Text preview: {page_text[:100] if page_text else 'No text'}...")
                    except Exception as e:
                        logger.warning(f"üîç [DEBUG] DocumentParser: Failed to extract text from page {page_number}: {e}")
                        page_size = 0
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    format_info = self.get_page_format_info(page)
                    document_format_stats["total_a4_sheets"] += format_info["a4_sheets"]
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result.page_format = format_info["format_name"]
                    page_result.page_width_mm = format_info["width_mm"]
                    page_result.page_height_mm = format_info["height_mm"]
                    page_result.page_orientation = format_info["orientation"]
                    page_result.page_a4_sheets_equivalent = format_info["a4_sheets"]
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–æ—Ä–º–∞—Ç–æ–≤
                    format_name = format_info["format_name"]
                    if format_name not in document_format_stats["formats"]:
                        document_format_stats["formats"][format_name] = 0
                    document_format_stats["formats"][format_name] += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–π
                    orientation = format_info["orientation"]
                    document_format_stats["orientations"][orientation] += 1
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    logger.info(f"Page {page_number} format: {format_name} "
                              f"({format_info['width_mm']}x{format_info['height_mm']} mm, "
                              f"{orientation}, {format_info['a4_sheets']} A4 sheets)")
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_type = self.detect_page_type(page)
                    page_result.page_type = page_type
                    document_format_stats["page_types"][page_type] += 1
                    logger.info(f"Page {page_number} type: {page_type}")
                    
                    if page_type == "vector":
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        text = page.extract_text()
                        if text.strip():
                            page_result.page_text = text
                            page_result.page_text_confidence = 0.9
                            page_result.page_text_method = "direct_extraction"
                            page_result.page_text_length = len(text)
                            logger.info(f"Page {page_number}: Extracted {len(text)} characters (vector)")
                        else:
                            logger.warning(f"Page {page_number}: No text extracted from vector page")
                            page_result.page_text = ""
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "failed"
                            page_result.page_text_length = 0
                    
                    elif page_type == "scanned" and page_num < len(images):
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å OCR
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: Processing as scanned page with OCR")
                    
                        try:
                            logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: Starting OCR processing")
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR
                            ocr_result = self.extract_text_from_image(images[page_num], page_number)
                            logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: OCR processing completed")
                            
                            if ocr_result["text"]:
                                page_result.page_text = ocr_result["text"]
                                page_result.page_text_confidence = ocr_result["confidence"]
                                page_result.page_text_method = f"ocr_{ocr_result['method']}"
                                page_result.page_text_length = len(ocr_result["text"])
                                page_result.page_ocr_confidence = ocr_result["confidence"]
                                page_result.page_ocr_method = ocr_result["method"]
                                page_result.page_ocr_all_results = ocr_result.get("all_results", [])
                                logger.info(f"Page {page_number}: OCR extracted {len(ocr_result['text'])} characters "
                                          f"(confidence: {ocr_result['confidence']})")
                            else:
                                logger.warning(f"Page {page_number}: OCR failed to extract text")
                                page_result.page_text = f"[OCR –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}]"
                                page_result.page_text_confidence = 0.1
                                page_result.page_text_method = "ocr_failed"
                                page_result.page_text_length = 0
                        
                        except Exception as e:
                            logger.error(f"OCR processing failed for page {page_number}: {e}")
                            page_result.page_text = f"[–û—à–∏–±–∫–∞ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {str(e)}]"
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "ocr_error"
                            page_result.page_text_length = 0
                    
                    else:
                        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        logger.warning(f"Page {page_number}: Unknown type or no image available")
                        text = page.extract_text()
                        if text.strip():
                            page_result.page_text = text
                            page_result.page_text_confidence = 0.5
                            page_result.page_text_method = "fallback_extraction"
                            page_result.page_text_length = len(text)
                        else:
                            page_result.page_text = f"[–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}]"
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "failed"
                            page_result.page_text_length = 0
                
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    logger.info(f"üîç [DEBUG] DocumentParser: Adding page {page_result.page_number} to results. Total results before: {len(result.document_pages_results)}")
                    result.document_pages_results.append(page_result)
                    logger.info(f"üîç [DEBUG] DocumentParser: Added page {page_result.page_number} to results. Total results after: {len(result.document_pages_results)}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                    if page_result.page_text and len(page_result.page_text.strip()) > 0:
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_result.page_number} has content ({len(page_result.page_text)} chars)")
                    else:
                        logger.warning(f"üîç [DEBUG] DocumentParser: Page {page_result.page_number} has no content!")
                    
                    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —à—Ç–∞–º–ø–æ–≤
                    # —Å –ø–æ–º–æ—â—å—é OpenCV –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    
                    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    log_memory_usage(f"processing page {page_number}")
                    
                    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –±–∞—Ç—á–µ
                    if (page_num - start_page) % 3 == 0:
                        cleanup_memory()
                
                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞
                logger.info(f"üîç [DEBUG] DocumentParser: Completed batch {batch_num + 1}/{total_batches}")
                log_memory_usage(f"after batch {batch_num + 1}")
                cleanup_memory()
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            result.document_pages = document_format_stats['total_pages']
            result.document_pages_total = document_format_stats['total_pages']
            result.document_pages_total_a4_sheets_equivalent = document_format_stats['total_a4_sheets']
            result.document_pages_vector = document_format_stats['page_types'].get('vector', 0)
            result.document_pages_scanned = document_format_stats['page_types'].get('scanned', 0)
            result.document_pages_unknown = document_format_stats['page_types'].get('unknown', 0)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info("=== PDF DOCUMENT FORMAT STATISTICS ===")
            logger.info(f"Total pages: {document_format_stats['total_pages']}")
            logger.info(f"Total A4 sheets equivalent: {document_format_stats['total_a4_sheets']:.2f}")
            logger.info(f"Page formats: {document_format_stats['formats']}")
            logger.info(f"Orientations: {document_format_stats['orientations']}")
            logger.info(f"Page types: {document_format_stats['page_types']}")
            logger.info("=====================================")
                
        except Exception as e:
            logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing error: {e}")
            import traceback
            logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing traceback: {traceback.format_exc()}")
            raise
        
        log_memory_usage("after PDF parsing")
        cleanup_memory()
        logger.info(f"üîç [DEBUG] DocumentParser: PDF parsing completed successfully. Total pages: {len(result.document_pages_results)}")
        return result
    
    def parse_docx(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ DOCX –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        elements = []
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_path = "/app/temp/temp.docx"
            with open(temp_path, "wb") as f:
                f.write(file_content)
            
            doc = Document(temp_path)
            
            for para in doc.paragraphs:
                if para.text.strip():
                    elements.append({
                        "element_type": "text",
                        "element_content": para.text,
                        "page_number": 1,  # DOCX –Ω–µ –∏–º–µ–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü
                        "confidence_score": 0.95
                    })
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                table_data = []
                for row in table.rows:
                    row_data = [cell.text for cell in row.cells]
                    table_data.append(row_data)
                
                if table_data:
                    elements.append({
                        "element_type": "table",
                        "element_content": json.dumps(table_data),
                        "page_number": 1,
                        "confidence_score": 0.9
                    })
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_path)
            
        except Exception as e:
            logger.error(f"DOCX parsing error: {e}")
            raise
        
        return elements
    
    def parse_dwg(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ DWG –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        elements = []
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DWG —Ñ–∞–π–ª–∞
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç ezdxf
            elements.append({
                "element_type": "text",
                "element_content": "DWG —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω (–ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)",
                "page_number": 1,
                "confidence_score": 0.5
            })
            
        except Exception as e:
            logger.error(f"DWG parsing error: {e}")
            raise
        
        return elements
    
    def parse_ifc(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ IFC –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        elements = []
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            text_content = file_content.decode('utf-8', errors='ignore')
            
            # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            entity_counts = {}
            project_info = None
            buildings = []
            storeys = []
            walls = []
            windows = []
            doors = []
            materials = []
            property_sets = []
            
            # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ IFC —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            entity_pattern = r'#(\d+)\s*=\s*([A-Z_]+)\s*\('
            name_pattern = r'Name\s*=\s*[\'"]([^\'"]*)[\'"]'
            description_pattern = r'Description\s*=\s*[\'"]([^\'"]*)[\'"]'
            object_type_pattern = r'ObjectType\s*=\s*[\'"]([^\'"]*)[\'"]'
            elevation_pattern = r'Elevation\s*=\s*([-\d.]+)'
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ IFC —Å—É—â–Ω–æ—Å—Ç–µ–π
            entity_matches = re.finditer(entity_pattern, text_content)
            
            for entity_match in entity_matches:
                entity_id = entity_match.group(1)
                entity_type = entity_match.group(2)
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –∑–∞–ø–∏—Å–∏ (–∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É)
                start_pos = entity_match.end()
                bracket_count = 0
                end_pos = start_pos
                
                for i, char in enumerate(text_content[start_pos:], start_pos):
                    if char == '(':
                        bracket_count += 1
                    elif char == ')':
                        bracket_count -= 1
                        if bracket_count < 0:
                            end_pos = i + 1
                            break
                
                entity_params = text_content[start_pos:end_pos]
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
                if entity_type == 'IFCPROJECT':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    project_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    project_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    project_info = f"–ü—Ä–æ–µ–∫—Ç: {project_name} - {project_desc}"
                    
                elif entity_type == 'IFCBUILDING':
                    name_match = re.search(name_pattern, entity_params)
                    building_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    buildings.append(f"–ó–¥–∞–Ω–∏–µ: {building_name}")
                    
                elif entity_type == 'IFCBUILDINGSTOREY':
                    name_match = re.search(name_pattern, entity_params)
                    elevation_match = re.search(elevation_pattern, entity_params)
                    storey_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    elevation = elevation_match.group(1) if elevation_match else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'
                    storeys.append(f"–≠—Ç–∞–∂: {storey_name} - –í—ã—Å–æ—Ç–∞: {elevation}")
                    
                elif entity_type == 'IFCWALL':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    wall_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    wall_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    walls.append(f"–°—Ç–µ–Ω–∞: {wall_name} - –¢–∏–ø: {wall_type}")
                    
                elif entity_type == 'IFCWINDOW':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    window_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    window_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    windows.append(f"–û–∫–Ω–æ: {window_name} - –¢–∏–ø: {window_type}")
                    
                elif entity_type == 'IFCDOOR':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    door_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    door_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    doors.append(f"–î–≤–µ—Ä—å: {door_name} - –¢–∏–ø: {door_type}")
                    
                elif entity_type == 'IFCMATERIAL':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    material_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    material_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    materials.append(f"–ú–∞—Ç–µ—Ä–∏–∞–ª: {material_name} - {material_desc}")
                        
                elif entity_type == 'IFCPROPERTYSET':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    pset_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    pset_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    property_sets.append(f"–ù–∞–±–æ—Ä —Å–≤–æ–π—Å—Ç–≤: {pset_name} - {pset_desc}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —ç–ª–µ–º–µ–Ω—Ç—ã
            if project_info:
                elements.append({
                    "element_type": "project_info",
                    "element_content": project_info,
                    "page_number": 1,
                    "confidence_score": 0.95
                })
            
            for building in buildings[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                elements.append({
                    "element_type": "building",
                    "element_content": building,
                    "page_number": 1,
                    "confidence_score": 0.9
                })
            
            for storey in storeys[:10]:
                elements.append({
                    "element_type": "storey",
                    "element_content": storey,
                    "page_number": 1,
                    "confidence_score": 0.9
                })
            
            for wall in walls[:20]:
                elements.append({
                    "element_type": "wall",
                    "element_content": wall,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for window in windows[:20]:
                elements.append({
                    "element_type": "window",
                    "element_content": window,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for door in doors[:20]:
                elements.append({
                    "element_type": "door",
                    "element_content": door,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for material in materials[:10]:
                elements.append({
                    "element_type": "material",
                    "element_content": material,
                    "page_number": 1,
                    "confidence_score": 0.8
                })
            
            for pset in property_sets[:10]:
                elements.append({
                    "element_type": "property_set",
                    "element_content": pset,
                    "page_number": 1,
                    "confidence_score": 0.8
                })
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
            total_entities = sum(entity_counts.values())
            top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            stats_text = f"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {total_entities}. –¢–æ–ø —Ç–∏–ø–æ–≤: " + ", ".join([f"{entity}({count})" for entity, count in top_entities])
            
            elements.append({
                "element_type": "statistics",
                "element_content": stats_text,
                "page_number": 1,
                "confidence_score": 0.95
            })
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if len(elements.document_pages_results) <= 1:  # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                elements.append({
                    "element_type": "text",
                    "element_content": f"IFC —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {total_entities} —Å—É—â–Ω–æ—Å—Ç–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤",
                    "page_number": 1,
                    "confidence_score": 0.7
                })
            
        except Exception as e:
            logger.error(f"IFC parsing error: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É
            try:
                text_content = file_content.decode('utf-8', errors='ignore')
                elements.append({
                    "element_type": "text",
                    "element_content": f"IFC —Ñ–∞–π–ª (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è): {text_content[:500]}",
                    "page_number": 1,
                    "confidence_score": 0.3
                })
            except:
                elements.append({
                    "element_type": "error",
                    "element_content": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ IFC —Ñ–∞–π–ª–∞: {str(e)}",
                    "page_number": 1,
                    "confidence_score": 0.1
                })
        
        return elements
    
    def save_to_database(self, filename: str, original_filename: str, file_type: str,
                         file_size: int, document_hash: str, inspection_result: DocumentInspectionResult,
                         category: str = "other", document_type: str = "normative") -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîç [DEBUG] DocumentParser: save_to_database called")
        logger.info(f"üîç [DEBUG] DocumentParser: document_type: {document_type}")
        logger.info(f"üîç [DEBUG] DocumentParser: pages to save: {len(inspection_result.document_pages_results)}")
        
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                if document_type == "checkable":
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    logger.info(f"üîç [DEBUG] DocumentParser: Saving as checkable document...")
                    review_deadline = datetime.now() + timedelta(days=2)
                    logger.info(f"üîç [DEBUG] DocumentParser: Review deadline: {review_deadline}")
                    
                    cursor.execute("""
                        INSERT INTO checkable_documents 
                        (filename, original_filename, file_type, file_size, document_hash, 
                         processing_status, category, review_deadline)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (filename, original_filename, file_type, file_size, document_hash, 
                          "completed", category, review_deadline))
                    document_id = cursor.fetchone()["id"]
                    logger.info(f"üîç [DEBUG] DocumentParser: Checkable document saved with ID: {document_id}")
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                    token_count = self.calculate_document_tokens(inspection_result)
                    
                    cursor.execute("""
                        INSERT INTO uploaded_documents 
                        (filename, original_filename, file_type, file_size, document_hash, 
                         processing_status, category, token_count)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (filename, original_filename, file_type, file_size, document_hash, 
                          "completed", category, token_count))
                    document_id = cursor.fetchone()["id"]
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"üîç [DEBUG] DocumentParser: Starting to save {len(inspection_result.document_pages_results)} elements...")
                for i, page_result in enumerate(inspection_result.document_pages_results):
                    logger.debug(f"üîç [DEBUG] DocumentParser: Saving element {i+1}/{len(inspection_result.document_pages_results)}, page {page_result.page_number}")
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    metadata = {
                        "page_type": page_result.page_type,
                        "processing_method": page_result.page_text_method,
                        "format_info": {
                            "format_name": page_result.page_format,
                            "width_mm": page_result.page_width_mm,
                            "height_mm": page_result.page_height_mm,
                            "orientation": page_result.page_orientation,
                            "a4_sheets": page_result.page_a4_sheets_equivalent
                        }
                    }
                    
                    if page_result.page_ocr_confidence is not None:
                        metadata["ocr_confidence"] = page_result.page_ocr_confidence
                    if page_result.page_ocr_method is not None:
                        metadata["ocr_method"] = page_result.page_ocr_method
                    
                    if document_type == "checkable":
                        logger.debug(f"üîç [DEBUG] DocumentParser: Inserting into checkable_elements, text length: {len(page_result.page_text)}")
                        cursor.execute("""
                            INSERT INTO checkable_elements 
                            (checkable_document_id, element_type, element_content, page_number, confidence_score, element_metadata)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            document_id,
                            "text",
                            page_result.page_text,
                            page_result.page_number,
                            page_result.page_text_confidence,
                            json.dumps(metadata)
                        ))
                    else:
                        logger.debug(f"üîç [DEBUG] DocumentParser: Inserting into extracted_elements, text length: {len(page_result.page_text)}")
                        cursor.execute("""
                            INSERT INTO extracted_elements 
                            (uploaded_document_id, element_type, element_content, page_number, confidence_score, metadata)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            document_id,
                            "text",
                            page_result.page_text,
                            page_result.page_number,
                            page_result.page_text_confidence,
                            json.dumps(metadata)
                        ))
                
                self.db_conn.commit()
                logger.info(f"üîç [DEBUG] DocumentParser: Database commit successful")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if document_type == "checkable":
                    logger.info(f"üîç [DEBUG] DocumentParser: Starting automatic norm control check...")
                    try:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                        document_content = "\n\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
                        logger.debug(f"üîç [DEBUG] DocumentParser: Document content length for norm control: {len(document_content)} characters")
                        
                        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
                        asyncio.create_task(self.perform_norm_control_check(document_id, document_content))
                        logger.info(f"üîç [DEBUG] DocumentParser: Started automatic norm control check for document {document_id}")
                    except Exception as e:
                        logger.error(f"üîç [DEBUG] DocumentParser: Failed to start norm control check for document {document_id}: {e}")
                
                logger.info(f"üîç [DEBUG] DocumentParser: save_to_database completed successfully, returning document_id: {document_id}")
                return document_id
                
        except Exception as e:
            self.db_conn.rollback()
            logger.error(f"Database save error: {e}")
            raise
    
    def create_initial_document_record(self, filename: str, file_type: str, file_size: int, document_hash: str, file_path: str, category: str = "other") -> int:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        def _create_record(conn):
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    INSERT INTO uploaded_documents 
                    (filename, original_filename, file_type, file_size, document_hash, processing_status, file_path, category)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (filename, filename, file_type, file_size, document_hash, "uploaded", file_path, category))
                
                document_id = cursor.fetchone()["id"]
                logger.info(f"Created initial document record with ID: {document_id}")
                return document_id
        
        try:
            return self.execute_in_transaction(_create_record)
        except Exception as e:
            logger.error(f"Error creating initial document record: {e}")
            raise
    
    def update_document_status(self, document_id: int, status: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s
                    WHERE id = %s
                """, (status, document_id))
                logger.info(f"Updated document {document_id} status to: {status}")
        
        try:
            self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Error updating document status: {e}")
            raise
    
    def update_checkable_document_status(self, document_id: int, status: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE checkable_documents 
                    SET processing_status = %s
                    WHERE id = %s
                """, (status, document_id))
                logger.info(f"Updated checkable document {document_id} status to: {status}")
        
        try:
            self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Error updating checkable document status: {e}")
            raise
    
    def save_elements_to_database(self, document_id: int, inspection_result: DocumentInspectionResult):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        def _save_elements(conn):
            with conn.cursor() as cursor:
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                logger.info(f"Cleared old elements for document {document_id}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"Saving new document information {document_id}")
                logger.info(f"Saving new document information {inspection_result.document_pages_results}")
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"üîç [DEBUG] DocumentParser: Starting to save {len(inspection_result.document_pages_results)} page results")
                for i, page_result in enumerate(inspection_result.document_pages_results):
                    logger.info(f"üîç [DEBUG] DocumentParser: Saving page {page_result.page_number} (index {i}), text length: {len(page_result.page_text or '')}")
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    metadata = {
                        "page_type": page_result.page_type,
                        "processing_method": page_result.page_text_method,
                        "format_info": {
                            "format_name": page_result.page_format,
                            "width_mm": page_result.page_width_mm,
                            "height_mm": page_result.page_height_mm,
                            "orientation": page_result.page_orientation,
                            "a4_sheets": page_result.page_a4_sheets_equivalent
                        }
                    }
                    
                    if page_result.page_ocr_confidence is not None:
                        metadata["ocr_confidence"] = page_result.page_ocr_confidence
                    if page_result.page_ocr_method is not None:
                        metadata["ocr_method"] = page_result.page_ocr_method
                    
                    cursor.execute("""
                        INSERT INTO extracted_elements 
                        (uploaded_document_id, element_type, element_content, page_number, confidence_score, metadata)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        document_id,
                        "text",
                        page_result.page_text,
                        page_result.page_number,
                        page_result.page_text_confidence,
                        json.dumps(metadata)
                    ))
                
                logger.info(f"Saved {len(inspection_result.document_pages_results)} elements for document {document_id}")
        
        try:
            self.execute_in_transaction(_save_elements)
        except Exception as e:
            logger.error(f"Error saving elements to database: {e}")
            raise
    
    def update_document_completion(self, document_id: int, elements_count: int, token_count: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        def _update_completion(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s, token_count = %s
                    WHERE id = %s
                """, ("completed", token_count, document_id))
                logger.info(f"Updated document {document_id} completion: {elements_count} elements, {token_count} tokens")
        
        try:
            self.execute_in_transaction(_update_completion)
        except Exception as e:
            logger.error(f"Error updating document completion: {e}")
            raise
    
    async def index_to_rag_service_async(self, document_id: int, document_title: str, inspection_result: DocumentInspectionResult):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ RAG-—Å–µ—Ä–≤–∏—Å"""
        try:
            logger.info(f"Starting async RAG indexing for document {document_id}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            elements = []
            for page_result in inspection_result.document_pages_results:
                elements.append({
                    "element_type": "text",
                    "element_content": page_result.page_text,
                    "page_number": page_result.page_number,
                    "confidence_score": page_result.page_text_confidence
                })
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            combined_text = ""
            for page_result in inspection_result.document_pages_results:
                if page_result.page_text:
                    combined_text += page_result.page_text + "\n\n"
            
            # –®–∞–≥ 4: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG-—Å–µ—Ä–≤–∏—Å
            trace_logger.log_indexing_start(document_id, len(inspection_result.document_pages_results))
            trace_logger.log_step("INDEXING_START", f"–û—Ç–ø—Ä–∞–≤–∫–∞ –≤ RAG-—Å–µ—Ä–≤–∏—Å", document_id)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ RAG-—Å–µ—Ä–≤–∏—Å
            async with httpx.AsyncClient(timeout=300.0) as client:
                response = await client.post(
                    f"{RAG_SERVICE_URL}/index",
                    data={
                        "document_id": document_id,
                        "document_title": document_title,
                        "content": combined_text,
                        "chapter": "",
                        "section": "",
                        "page_number": 1
                    }
                )
                
                if response.status_code == 200:
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
                    tokens_count = self.calculate_document_tokens(inspection_result)
                    trace_logger.log_indexing_success(document_id, 0.0, tokens_count)  # –í—Ä–µ–º—è –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                    trace_logger.log_step("INDEXING_SUCCESS", f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, {tokens_count} —Ç–æ–∫–µ–Ω–æ–≤", document_id)
                    logger.info(f"Successfully indexed document {document_id} to RAG service")
                else:
                    trace_logger.log_error("INDEXING", f"–û—à–∏–±–∫–∞ RAG-—Å–µ—Ä–≤–∏—Å–∞: {response.status_code}", document_id)
                    logger.error(f"Failed to index document {document_id} to RAG service: {response.status_code}")
                    
        except Exception as e:
            logger.error(f"Error in async RAG indexing for document {document_id}: {e}")
    
    def calculate_document_tokens(self, inspection_result: DocumentInspectionResult) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            total_tokens = 0
            encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
            
            for page_result in inspection_result.document_pages_results:
                if page_result.page_text:
                    tokens = encoding.encode(page_result.page_text)
                    total_tokens += len(tokens)
            
            logger.info(f"Calculated {total_tokens} tokens for document")
            return total_tokens
            
        except Exception as e:
            logger.error(f"Error calculating document tokens: {e}")
            return 0
            logger.error(f"Database save error: {e}")
            raise

    def save_checkable_document(self, filename: str, original_filename: str, file_type: str, 
                               file_size: int, document_hash: str, inspection_result: DocumentInspectionResult, 
                               category: str = "other") -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üîç [DEBUG] DocumentParser: save_checkable_document called")
        logger.info(f"üîç [DEBUG] DocumentParser: filename: {filename}")
        logger.info(f"üîç [DEBUG] DocumentParser: original_filename: {original_filename}")
        logger.info(f"üîç [DEBUG] DocumentParser: file_type: {file_type}")
        logger.info(f"üîç [DEBUG] DocumentParser: file_size: {file_size}")
        logger.info(f"üîç [DEBUG] DocumentParser: document_hash: {document_hash}")
        logger.info(f"üîç [DEBUG] DocumentParser: category: {category}")
        logger.info(f"üîç [DEBUG] DocumentParser: inspection_result pages: {len(inspection_result.document_pages_results)}")
        
        document_id = self.save_to_database(filename, original_filename, file_type, file_size, 
                                   document_hash, inspection_result, category, "checkable")
        
        logger.info(f"üîç [DEBUG] DocumentParser: save_checkable_document completed, document_id: {document_id}")
        return document_id

    def cleanup_expired_documents(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        def _cleanup(conn):
            with conn.cursor() as cursor:
                cursor.execute("SELECT cleanup_expired_checkable_documents()")
                result = cursor.fetchone()
                return result[0] if result else 0
        
        try:
            return self.execute_in_transaction(_cleanup)
        except Exception as e:
            logger.error(f"Cleanup error: {e}")
            return 0

    def get_checkable_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥–ª–µ–∂–∞—â–∏—Ö –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é"""
        def _get_documents(conn):
            logger.debug(f"üîç [DATABASE] _get_documents called")
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, original_filename, file_type, file_size, upload_date, 
                               processing_status, category, review_deadline, review_status, 
                               assigned_reviewer
                        FROM checkable_documents 
                        ORDER BY upload_date DESC
                    """)
                    documents = cursor.fetchall()
                    logger.debug(f"üîç [DATABASE] Retrieved {len(documents)} checkable documents")
                    return [dict(doc) for doc in documents]
            except Exception as db_error:
                logger.error(f"üîç [DATABASE] Error in _get_documents: {db_error}")
                raise
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
            if check_memory_pressure():
                logger.warning("üîç [MEMORY] High memory pressure detected, performing cleanup")
                cleanup_memory()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            logger.info(f"üîç [DATABASE] Starting read-only transaction for get_checkable_documents")
            result = self.execute_in_read_only_transaction(_get_documents)
            logger.info(f"üîç [DATABASE] Successfully retrieved {len(result)} checkable documents using read-only transaction")
            return result
        except Exception as e:
            logger.error(f"üîç [DATABASE] Get checkable documents error: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ –ø–∞–¥–µ–Ω–∏—è
            return []

    def get_checkable_document(self, document_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        def _get_document(conn):
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, original_filename, file_type, file_size, upload_date, 
                               processing_status, category, review_deadline, review_status, 
                               assigned_reviewer
                        FROM checkable_documents 
                        WHERE id = %s
                    """, (document_id,))
                    document = cursor.fetchone()
                    return dict(document) if document else None
            except Exception as db_error:
                logger.error(f"üîç [DATABASE] Error in _get_document: {db_error}")
                raise
        
        try:
            logger.debug(f"üîç [DATABASE] Starting read-only transaction for get_checkable_document {document_id}")
            result = self.execute_in_read_only_transaction(_get_document)
            logger.debug(f"üîç [DATABASE] Successfully retrieved checkable document {document_id} using read-only transaction")
            return result
        except Exception as e:
            logger.error(f"Get checkable document error: {e}")
            return None

    def get_norm_control_result_by_document_id(self, document_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –ø–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _get_norm_control_result(conn):
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, analysis_status, total_findings, critical_findings, warning_findings, 
                               info_findings, analysis_date
                        FROM norm_control_results
                        WHERE checkable_document_id = %s
                        ORDER BY analysis_date DESC
                        LIMIT 1
                    """, (document_id,))
                    result = cursor.fetchone()
                    return dict(result) if result else None
            except Exception as db_error:
                logger.error(f"üîç [DATABASE] Error in _get_norm_control_result: {db_error}")
                raise
        
        try:
            logger.debug(f"üîç [DATABASE] Starting read-only transaction for get_norm_control_result_by_document_id {document_id}")
            result = self.execute_in_read_only_transaction(_get_norm_control_result)
            logger.debug(f"üîç [DATABASE] Successfully retrieved norm control result for document {document_id} using read-only transaction")
            return result
        except Exception as e:
            logger.error(f"Get norm control result error: {e}")
            return None

    def get_page_results_by_document_id(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _get_page_results(conn):
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT 
                            f.id,
                            f.finding_type,
                            f.severity_level,
                            f.category,
                            f.title,
                            f.description,
                            f.recommendation,
                            f.confidence_score,
                            f.created_at
                        FROM findings f
                        JOIN norm_control_results ncr ON f.norm_control_result_id = ncr.id
                        WHERE ncr.checkable_document_id = %s
                        ORDER BY f.severity_level DESC, f.created_at
                    """, (document_id,))
                    results = cursor.fetchall()
                    return [dict(result) for result in results]
            except Exception as db_error:
                logger.error(f"üîç [DATABASE] Error in _get_page_results: {db_error}")
                raise
        
        try:
            logger.debug(f"üîç [DATABASE] Starting read-only transaction for get_page_results_by_document_id {document_id}")
            result = self.execute_in_read_only_transaction(_get_page_results)
            logger.debug(f"üîç [DATABASE] Successfully retrieved page results for document {document_id} using read-only transaction")
            return result
        except Exception as e:
            logger.error(f"Get page results error: {e}")
            return []

    def get_review_report_by_norm_control_id(self, norm_control_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ –ø–æ ID —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, reviewer_name, conclusion, report_date
                    FROM review_reports
                    WHERE norm_control_result_id = %s
                    ORDER BY report_date DESC
                    LIMIT 1
                """, (norm_control_id,))
                result = cursor.fetchone()
                return dict(result) if result else None
        except Exception as e:
            logger.error(f"Get review report error: {e}")
            return None

    def get_findings_by_norm_control_id(self, norm_control_result_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö findings –ø–æ ID —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT f.*, 
                           dc.clause_title as normative_document_title,
                           dc.clause_number as normative_clause_number
                    FROM findings f
                    LEFT JOIN document_clauses dc ON f.related_clause_id = dc.id
                    WHERE f.norm_control_result_id = %s
                    ORDER BY f.severity_level DESC, f.id ASC
                """, (norm_control_result_id,))
                results = cursor.fetchall()
                return [dict(result) for result in results]
        except Exception as e:
            logger.error(f"Get findings error: {e}")
            return []

    def get_findings_by_document_id(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö findings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT f.*, 
                           dc.clause_title as normative_document_title,
                           dc.clause_number as normative_clause_number,
                           ncr.analysis_date
                    FROM findings f
                    JOIN norm_control_results ncr ON f.norm_control_result_id = ncr.id
                    LEFT JOIN document_clauses dc ON f.related_clause_id = dc.id
                    WHERE ncr.checkable_document_id = %s
                    ORDER BY f.severity_level DESC, f.id ASC
                """, (document_id,))
                results = cursor.fetchall()
                return [dict(result) for result in results]
        except Exception as e:
            logger.error(f"Get findings by document error: {e}")
            return []

    def get_document_info(self, pdf_reader) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ PDF –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            info = pdf_reader.metadata
            if info:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF
                title = info.get('/Title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
                author = info.get('/Author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')
                subject = info.get('/Subject', '')
                creator = info.get('/Creator', '')
                producer = info.get('/Producer', '')
                creation_date = info.get('/CreationDate', '')
                mod_date = info.get('/ModDate', '')
                
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                document_number = ""
                if title:
                    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "A9.5.MTH.04" –∏–ª–∏ "–ö–ñ-01" –∏ —Ç.–¥.
                    import re
                    number_match = re.search(r'[A-Z–ê-–Ø]{1,3}[-\d\.]+[A-Z–ê-–Ø]*\d*', title)
                    if number_match:
                        document_number = number_match.group(0)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                document_type = "pdf"
                engineering_stage = "–ü–î"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                document_mark = ""
                document_status = "IFR"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                
                # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞—Ä–∫—É –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                if title:
                    title_upper = title.upper()
                    if any(mark in title_upper for mark in ['–ö–ñ', 'KZH']):
                        document_mark = "–ö–ñ"
                    elif any(mark in title_upper for mark in ['–ö–ú', 'KM']):
                        document_mark = "–ö–ú"
                    elif any(mark in title_upper for mark in ['–ê–°', 'AS']):
                        document_mark = "–ê–°"
                    elif any(mark in title_upper for mark in ['–¢–•', 'TX']):
                        document_mark = "–¢–•"
                    elif any(mark in title_upper for mark in ['–ö–°', 'KS']):
                        document_mark = "–ö–°"
                    elif any(mark in title_upper for mark in ['–ö–ü', 'KP']):
                        document_mark = "–ö–ü"
                    elif any(mark in title_upper for mark in ['–ö–†', 'KR']):
                        document_mark = "–ö–†"
                
                return {
                    "name": title,
                    "type": document_type,
                    "engineering_stage": engineering_stage,
                    "mark": document_mark,
                    "number": document_number,
                    "date": creation_date or mod_date,
                    "author": author,
                    "reviewer": "",  # –ù–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ PDF
                    "approver": "",  # –ù–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ PDF
                    "status": document_status,
                    "size": 0  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                }
            else:
                # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                return {
                    "name": "–î–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
                    "type": "pdf",
                    "engineering_stage": "–ü–î",
                    "mark": "",
                    "number": "",
                    "date": "",
                    "author": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä",
                    "reviewer": "",
                    "approver": "",
                    "status": "IFR",
                    "size": 0
                }
        except Exception as e:
            logger.error(f"Error extracting document info: {e}")
            return {
                "name": "–î–æ–∫—É–º–µ–Ω—Ç —Å –æ—à–∏–±–∫–æ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö",
                "type": "pdf",
                "engineering_stage": "–ü–î",
                "mark": "",
                "number": "",
                "date": "",
                "author": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä",
                "reviewer": "",
                "approver": "",
                "status": "IFR",
                "size": 0
            }

    def get_document_format_statistics(self, document_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT metadata->>'format_info' as format_info
                    FROM checkable_elements
                    WHERE checkable_document_id = %s
                    ORDER BY page_number
                """, (document_id,))
                elements = cursor.fetchall()
                
                if not elements:
                    return {"error": "Document not found or no elements"}
                
                stats = {
                    "total_pages": len(elements),
                    "total_a4_sheets": 0.0,
                    "formats": {},
                    "orientations": {"portrait": 0, "landscape": 0},
                    "page_types": {"vector": 0, "scanned": 0, "unknown": 0},
                    "pages": []
                }
                
                for element in elements:
                    try:
                        format_info = json.loads(element["format_info"]) if element["format_info"] else {}
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        page_info = {
                            "page_number": format_info.get("page_number", 0),
                            "format": format_info.get("format_name", "unknown"),
                            "width_mm": format_info.get("width_mm", 0),
                            "height_mm": format_info.get("height_mm", 0),
                            "orientation": format_info.get("orientation", "unknown"),
                            "a4_sheets": format_info.get("a4_sheets", 0)
                        }
                        stats["pages"].append(page_info)
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        a4_sheets = format_info.get("a4_sheets", 0)
                        if a4_sheets is not None:
                            stats["total_a4_sheets"] += a4_sheets
                        
                        format_name = format_info.get("format_name", "unknown")
                        if format_name not in stats["formats"]:
                            stats["formats"][format_name] = 0
                        stats["formats"][format_name] += 1
                        
                        orientation = format_info.get("orientation", "unknown")
                        if orientation in stats["orientations"]:
                            stats["orientations"][orientation] += 1
                        
                    except Exception as e:
                        logger.warning(f"Error parsing format info: {e}")
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        stats["pages"].append({
                            "page_number": 0,
                            "format": "unknown",
                            "width_mm": 0,
                            "height_mm": 0,
                            "orientation": "unknown",
                            "a4_sheets": 0
                        })
                        continue
                
                # –û–∫—Ä—É–≥–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
                stats["total_a4_sheets"] = round(stats["total_a4_sheets"], 2)
                
                return stats
                
        except Exception as e:
            logger.error(f"Get document format statistics error: {e}")
            return {"error": str(e)}

    def determine_document_category(self, filename: str, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        if any(keyword in filename_lower for keyword in ['gost', '–≥–æ—Å—Ç']):
            return 'gost'
        elif any(keyword in filename_lower for keyword in ['sp', '—Å–Ω']):
            return 'sp'
        elif any(keyword in filename_lower for keyword in ['snip', '—Å–Ω–∏–ø']):
            return 'snip'
        elif any(keyword in filename_lower for keyword in ['tr', '—Ç—Ä']):
            return 'tr'
        elif any(keyword in content_lower for keyword in ['–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π', '–∫–æ—Ä–ø', 'corporate']):
            return 'corporate'
        else:
            return 'other'

    def update_review_status(self, document_id: int, status: str, reviewer: str = None, notes: str = None) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE checkable_documents 
                    SET review_status = %s, assigned_reviewer = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE id = %s
                """, (status, reviewer, document_id))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Update review status error: {e}")
            return False

    async def delete_normative_document(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        def _delete_document(conn):
            with conn.cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                cursor.execute("""
                    SELECT original_filename, document_hash 
                    FROM uploaded_documents 
                    WHERE id = %s
                """, (document_id,))
                doc_info = cursor.fetchone()
                
                if not doc_info:
                    logger.warning(f"Document {document_id} not found")
                    return False, None
                
                # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                elements_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
                cursor.execute("DELETE FROM norm_control_results WHERE uploaded_document_id = %s", (document_id,))
                results_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM uploaded_documents WHERE id = %s", (document_id,))
                document_deleted = cursor.rowcount
                
                logger.info(f"Deleted normative document {document_id} ({doc_info[0]}): "
                          f"{elements_deleted} elements, {results_deleted} results, {document_deleted} document")
                
                return document_deleted > 0, doc_info[0]
        
        try:
            success, filename = self.execute_in_transaction(_delete_document)
            
            if success:
                # –£–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ RAG —Å–µ—Ä–≤–∏—Å–∞
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.delete(f"{RAG_SERVICE_URL}/index-documentes/document/{document_id}")
                        if response.status_code == 200:
                            logger.info(f"Successfully deleted indexes for document {document_id}")
                        else:
                            logger.warning(f"Failed to delete indexes for document {document_id}: {response.status_code}")
                except Exception as e:
                    logger.error(f"Error deleting indexes for document {document_id}: {e}")
            
            return success
                
        except Exception as e:
            logger.error(f"Delete normative document error: {e}")
            return False

    def delete_checkable_document(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        def _delete_document(conn):
            with conn.cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                cursor.execute("""
                    SELECT original_filename, document_hash 
                    FROM checkable_documents 
                    WHERE id = %s
                """, (document_id,))
                doc_info = cursor.fetchone()
                
                if not doc_info:
                    logger.warning(f"Checkable document {document_id} not found")
                    return False
                
                # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM checkable_elements WHERE checkable_document_id = %s", (document_id,))
                elements_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º –æ—Ç—á–µ—Ç—ã –æ –ø—Ä–æ–≤–µ—Ä–∫–µ
                cursor.execute("DELETE FROM review_reports WHERE checkable_document_id = %s", (document_id,))
                reports_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
                cursor.execute("DELETE FROM norm_control_results WHERE checkable_document_id = %s", (document_id,))
                results_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM checkable_documents WHERE id = %s", (document_id,))
                document_deleted = cursor.rowcount
                
                logger.info(f"Deleted checkable document {document_id} ({doc_info[0]}): "
                          f"{elements_deleted} elements, {reports_deleted} reports, "
                          f"{results_deleted} results, {document_deleted} document")
                
                return document_deleted > 0
        
        try:
            return self.execute_in_transaction(_delete_document)
        except Exception as e:
            logger.error(f"Delete checkable document error: {e}")
            return False
    
    async def index_to_rag_service(self, document_id: int, document_title: str, elements: List[Dict[str, Any]]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG-—Å–µ—Ä–≤–∏—Å"""
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
            content = "\n\n".join([elem["element_content"] for elem in elements])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª–∞–≤—É –∏ —Ä–∞–∑–¥–µ–ª –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            chapter = ""
            section = ""
            for elem in elements:
                if elem["element_type"] in ["project_info", "building", "storey"]:
                    chapter = elem["element_content"][:100]
                    break
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ RAG-—Å–µ—Ä–≤–∏—Å
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{RAG_SERVICE_URL}/index",
                    data={
                        "document_id": document_id,
                        "document_title": document_title,
                        "content": content,
                        "chapter": chapter,
                        "section": section,
                        "page_number": 1
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"Successfully indexed document {document_id} in RAG service: {result}")
                else:
                    logger.error(f"Failed to index document {document_id} in RAG service: {response.text}")
                    
        except Exception as e:
            logger.error(f"RAG indexing error for document {document_id}: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å, –µ—Å–ª–∏ RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å

    def get_system_settings(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT setting_key, setting_value, setting_description, setting_type, is_public, updated_at
                    FROM system_settings
                    WHERE is_public = true
                    ORDER BY setting_key
                """)
                settings = cursor.fetchall()
                return [dict(setting) for setting in settings]
        except Exception as e:
            logger.error(f"Get system settings error: {e}")
            return []

    def get_system_setting(self, setting_key: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            with self.db_conn.cursor() as cursor:
                cursor.execute("""
                    SELECT setting_value
                    FROM system_settings
                    WHERE setting_key = %s AND is_public = true
                """, (setting_key,))
                result = cursor.fetchone()
                return result[0] if result else None
        except Exception as e:
            logger.error(f"Get system setting error: {e}")
            return None

    def update_system_setting(self, setting_key: str, setting_value: str) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _update_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE system_settings
                    SET setting_value = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE setting_key = %s AND is_public = true
                """, (setting_value, setting_key))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_update_setting)
        except Exception as e:
            logger.error(f"Update system setting error: {e}")
            return False

    def create_system_setting(self, setting_key: str, setting_value: str, 
                            setting_description: str, setting_type: str = "text") -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _create_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO system_settings (setting_key, setting_value, setting_description, setting_type)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (setting_key) DO UPDATE SET
                    setting_value = EXCLUDED.setting_value,
                    setting_description = EXCLUDED.setting_description,
                    setting_type = EXCLUDED.setting_type,
                    updated_at = CURRENT_TIMESTAMP
                """, (setting_key, setting_value, setting_description, setting_type))
                return True
        
        try:
            return self.execute_in_transaction(_create_setting)
        except Exception as e:
            logger.error(f"Create system setting error: {e}")
            return False

    def delete_system_setting(self, setting_key: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _delete_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM system_settings
                    WHERE setting_key = %s AND is_public = true
                """, (setting_key,))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_delete_setting)
        except Exception as e:
            logger.error(f"Delete system setting error: {e}")
            return False

    def get_normcontrol_prompt_template(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ —Å–∏—Å—Ç–µ–º—ã –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            normcontrol_prompt = self.get_system_setting("normcontrol_prompt")
            if not normcontrol_prompt:
                logger.warning("normcontrol_prompt –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç")
                normcontrol_prompt = "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            prompt_template = self.get_system_setting("normcontrol_prompt_template")
            if prompt_template:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω
                prompt_template = prompt_template.replace("{normcontrol_prompt}", normcontrol_prompt)
                return prompt_template
            
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç –Ω–∞–ø—Ä—è–º—É—é
            # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü
            processed_prompt = normcontrol_prompt.replace("{document_content}", "{page_content}")
            processed_prompt = processed_prompt.replace("{normative_docs}", "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
            
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –ø—Ä–æ–º–ø—Ç–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            processed_prompt = processed_prompt.replace("{", "{{").replace("}", "}}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è JSON –æ—Ç–≤–µ—Ç–∞
            template = f"""
{processed_prompt}

–°–û–î–ï–†–ñ–ò–ú–û–ï –°–¢–†–ê–ù–ò–¶–´:
{{page_content}}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
2. –í–ê–ñ–ù–û: –û—Ç–≤–µ—Ç—å—Ç–µ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
3. –ù–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ –ø–æ—è—Å–Ω–µ–Ω–∏–π –≤–Ω–µ JSON

–¢–†–ï–ë–£–ï–ú–´–ô –§–û–†–ú–ê–¢ JSON:
{{
  "page_number": –ù–û–ú–ï–†_–°–¢–†–ê–ù–ò–¶–´,
  "overall_status": "pass|fail|uncertain",
  "confidence": 0.0-1.0,
  "total_findings": —á–∏—Å–ª–æ,
  "critical_findings": —á–∏—Å–ª–æ,
  "warning_findings": —á–∏—Å–ª–æ,
  "info_findings": —á–∏—Å–ª–æ,
  "compliance_percentage": 0-100,
  "findings": [],
  "summary": "–æ–±—â–∏–π_–≤—ã–≤–æ–¥_–ø–æ_—Å—Ç—Ä–∞–Ω–∏—Ü–µ",
  "recommendations": "–æ–±—â–∏–µ_—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏_–ø–æ_—É–ª—É—á—à–µ–Ω–∏—é"
}}

–ü–†–ò–ú–ï–† –û–¢–í–ï–¢–ê:
{{
  "page_number": 1,
  "overall_status": "pass",
  "confidence": 0.85,
  "total_findings": 0,
  "critical_findings": 0,
  "warning_findings": 0,
  "info_findings": 0,
  "compliance_percentage": 95,
  "findings": [],
  "summary": "–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º",
  "recommendations": "–î–æ–∫—É–º–µ–Ω—Ç –æ—Ñ–æ—Ä–º–ª–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ"
}}
"""
            
            return template
            
        except Exception as e:
            logger.error(f"Get normcontrol prompt template error: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."

    def combine_page_results(self, document_id: int, page_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        try:
            logger.info(f"üîç [DEBUG] DocumentParser: Combining results from {len(page_results)} pages for document {document_id}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–∏–µ —Å—á–µ—Ç—á–∏–∫–∏
            total_findings = 0
            critical_findings = 0
            warning_findings = 0
            info_findings = 0
            all_findings = []
            successful_pages = 0
            failed_pages = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for page_result in page_results:
                page_number = page_result.get("page_number", 0)
                
                if page_result.get("status") == "success":
                    successful_pages += 1
                    result_data = page_result.get("result", {})
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–º–µ—á–∞–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_findings = result_data.get("findings", [])
                    for finding in page_findings:
                        finding["page_number"] = page_number
                        all_findings.append(finding)
                        
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ —Ç–∏–ø–∞–º
                        severity = finding.get("severity", "info")
                        if severity == "critical":
                            critical_findings += 1
                        elif severity == "warning":
                            warning_findings += 1
                        else:
                            info_findings += 1
                    
                    total_findings += len(page_findings)
                    logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: {len(page_findings)} findings")
                    
                else:
                    failed_pages += 1
                    logger.warning(f"üîç [DEBUG] DocumentParser: Page {page_number} failed: {page_result.get('error', 'Unknown error')}")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            combined_result = {
                "document_id": document_id,
                "total_pages": len(page_results),
                "successful_pages": successful_pages,
                "failed_pages": failed_pages,
                "total_findings": total_findings,
                "critical_findings": critical_findings,
                "warning_findings": warning_findings,
                "info_findings": info_findings,
                "findings": all_findings,
                "status": "completed" if failed_pages == 0 else "completed_with_errors"
            }
            
            logger.info(f"üîç [DEBUG] DocumentParser: Combined result: {total_findings} total findings, "
                       f"{critical_findings} critical, {warning_findings} warnings, {info_findings} info")
            
            return combined_result
            
        except Exception as e:
            logger.error(f"üîç [DEBUG] DocumentParser: Error combining page results: {e}")
            return {
                "document_id": document_id,
                "status": "error",
                "error": str(e)
            }

    def split_document_into_pages(self, document_id: int) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π PDF"""
        logger.info(f"üîç [DEBUG] DocumentParser: Starting split_document_into_pages for document {document_id}")
        try:
            pages = []
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT page_number, element_content, element_type, confidence_score
                    FROM checkable_elements
                    WHERE checkable_document_id = %s
                    ORDER BY page_number, id
                """, (document_id,))
                elements = cursor.fetchall()
            
            if not elements:
                logger.warning(f"No elements found for document {document_id}")
                return []
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            current_page = None
            current_page_content = []
            current_page_elements = []
            
            for element in elements:
                page_number = element["page_number"]
                
                if current_page is None:
                    current_page = page_number
                    current_page_content = []
                    current_page_elements = []
                
                if page_number != current_page:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    pages.append({
                        "page_number": current_page,
                        "content": "\n\n".join(current_page_content),
                        "char_count": len("\n\n".join(current_page_content)),
                        "element_count": len(current_page_elements),
                        "elements": current_page_elements
                    })
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    current_page = page_number
                    current_page_content = []
                    current_page_elements = []
                
                # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç –∫ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                current_page_content.append(element["element_content"])
                current_page_elements.append({
                    "type": element["element_type"],
                    "content": element["element_content"],
                    "confidence": element["confidence_score"]
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if current_page is not None:
                pages.append({
                    "page_number": current_page,
                    "content": "\n\n".join(current_page_content),
                    "char_count": len("\n\n".join(current_page_content)),
                    "element_count": len(current_page_elements),
                    "elements": current_page_elements
                })
            
            logger.info(f"üîç [DEBUG] DocumentParser: Split document {document_id} into {len(pages)} pages based on PDF structure")
            logger.info(f"üîç [DEBUG] DocumentParser: Page details for document {document_id}:")
            for page in pages:
                logger.info(f"üîç [DEBUG] DocumentParser: - Page {page['page_number']}: {page['char_count']} chars, {page['element_count']} elements")
                logger.info(f"üîç [DEBUG] DocumentParser: - Page {page['page_number']} content preview: {page['content'][:100]}...")
            
            logger.info(f"üîç [DEBUG] DocumentParser: split_document_into_pages completed for document {document_id}")
            
            return pages
            
        except Exception as e:
            logger.error(f"Error splitting document into pages: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []

    async def perform_norm_control_check_for_page(self, document_id: int, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = time.time()
        page_number = page_data["page_number"]
        
        # –®–∞–≥ 5: –ù–∞—á–∞–ª–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        trace_logger.log_normcontrol_start(document_id, 1)  # 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        trace_logger.log_step("NORMCONTROL_PAGE_START", f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}, –∫–æ–Ω—Ç–µ–Ω—Ç: {len(page_data['content'])} —Å–∏–º–≤–æ–ª–æ–≤", document_id)
        
        logger.info(f"üîç [DEBUG] DocumentParser: Starting norm control check for document {document_id}, page {page_data['page_number']}")
        logger.info(f"üîç [DEBUG] DocumentParser: Page parameters:")
        logger.info(f"üîç [DEBUG] DocumentParser: - Page number: {page_data['page_number']}")
        logger.info(f"üîç [DEBUG] DocumentParser: - Content length: {len(page_data['content'])} characters")
        logger.info(f"üîç [DEBUG] DocumentParser: - Element count: {page_data['element_count']}")
        logger.info(f"üîç [DEBUG] DocumentParser: - Content preview: {page_data['content'][:100]}...")
        
        try:
            page_number = page_data["page_number"]
            page_content = page_data["content"]
            
            logger.info(f"üîç [DEBUG] DocumentParser: Starting norm control check for document {document_id}, page {page_number}")
            logger.info(f"üîç [DEBUG] DocumentParser: Page content length: {len(page_content)} characters")
            
            # ===== –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–ú–ü–¢–ê –î–õ–Ø LLM –ò–ó –°–ò–°–¢–ï–ú–´ –ù–ê–°–¢–†–û–ï–ö =====
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ —Å–∏—Å—Ç–µ–º—ã –Ω–∞—Å—Ç—Ä–æ–µ–∫
            prompt_template = self.get_normcontrol_prompt_template()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞
            # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            prompt = prompt_template.replace("{page_content}", page_content)
            
            # ===== –û–¢–ü–†–ê–í–ö–ê –ó–ê–ü–†–û–°–ê –ö LLM –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –°–¢–†–ê–ù–ò–¶–´ =====
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM —á–µ—Ä–µ–∑ gateway –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            llm_start_time = time.time()
            trace_logger.log_llm_request(document_id, page_number, len(prompt))
            trace_logger.log_step("LLM_REQUEST", f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}", document_id)
            
            logger.info(f"Sending request to LLM for page {page_number}...")
            logger.info(f"Prompt length: {len(prompt)} characters")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç 10 –º–∏–Ω—É—Ç –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            timeout = httpx.Timeout(600.0, connect=30.0)
            async with httpx.AsyncClient(verify=False, timeout=timeout) as client:
                response = await client.post(
                    "http://ollama:11434/api/generate",
                    headers={
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "llama3.1-optimized-v2:latest",
                        "prompt": f"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.\n\n{prompt}",
                        "stream": False
                    }
                )
                
                # ===== –ü–û–õ–£–ß–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê –ü–†–û–í–ï–†–ö–ò –û–¢ LLM =====
                if response.status_code == 200:
                    result = response.json()
                    content = result["response"]
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
                    llm_response_time = time.time() - llm_start_time
                    trace_logger.log_llm_response(document_id, page_number, llm_response_time, len(content))
                    trace_logger.log_step("LLM_RESPONSE", f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç LLM –∑–∞ {llm_response_time:.2f}—Å", document_id)
                    
                    # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç –æ—Ç LLM
                    try:
                        import json
                        import re
                        
                        logger.info(f"üîç [DEBUG] DocumentParser: Raw LLM response length: {len(content)}")
                        logger.info(f"üîç [DEBUG] DocumentParser: Raw LLM response preview: {content[:200]}...")
                        
                        # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                        cleaned_content = content.strip()
                        
                        # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–µ–∂–¥—É —Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏)
                        json_match = re.search(r'\{.*\}', cleaned_content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group(0)
                            logger.info(f"üîç [DEBUG] DocumentParser: Found JSON match: {json_str[:100]}...")
                            check_result = json.loads(json_str)
                        else:
                            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç
                            logger.info(f"üîç [DEBUG] DocumentParser: No JSON match found, trying to parse entire response")
                            check_result = json.loads(cleaned_content)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è
                        required_fields = ["page_number", "overall_status", "confidence", "total_findings"]
                        missing_fields = [field for field in required_fields if field not in check_result]
                        
                        if missing_fields:
                            logger.warning(f"üîç [DEBUG] DocumentParser: Missing required fields: {missing_fields}")
                            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            check_result = {
                                "page_number": page_number,
                                "overall_status": "uncertain",
                                "confidence": 0.5,
                                "total_findings": 0,
                                "critical_findings": 0,
                                "warning_findings": 0,
                                "info_findings": 0,
                                "compliance_percentage": 50,
                                "findings": [],
                                "summary": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏",
                                "recommendations": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
                            }
                        
                        logger.info(f"üîç [DEBUG] DocumentParser: Successfully parsed JSON result")
                        return {
                            "status": "success",
                            "result": check_result,
                            "raw_response": content
                        }
                    except json.JSONDecodeError as e:
                        logger.error(f"üîç [DEBUG] DocumentParser: JSON parsing error: {e}")
                        logger.error(f"üîç [DEBUG] DocumentParser: Raw response: {content}")
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
                        default_result = {
                            "page_number": page_number,
                            "overall_status": "uncertain",
                            "confidence": 0.3,
                            "total_findings": 0,
                            "critical_findings": 0,
                            "warning_findings": 0,
                            "info_findings": 0,
                            "compliance_percentage": 30,
                            "findings": [],
                            "summary": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM",
                            "recommendations": "–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
                        }
                        
                        return {
                            "status": "success",
                            "result": default_result,
                            "raw_response": content
                        }
                else:
                    logger.error(f"LLM request failed: {response.status_code} - {response.text}")
                    return {
                        "status": "error",
                        "error": f"LLM request failed: {response.status_code}"
                    }
                    
        except Exception as e:
            logger.error(f"Norm control check error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def perform_norm_control_check(self, document_id: int, document_content: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º LLM"""
        try:
            logger.info(f"Starting norm control check for document {document_id}")
            logger.info(f"Document content length: {len(document_content)} characters")
            
            # ===== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–†–û–í–ï–†–ö–ò –ù–û–†–ú–û–ö–û–ù–¢–†–û–õ–Ø –° LLM =====
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π PDF
            pages = self.split_document_into_pages(document_id)
            logger.info(f"Document split into {len(pages)} pages based on PDF structure")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–¥–µ–ª—å–Ω–æ
            page_results = []
            total_findings = 0
            total_critical_findings = 0
            total_warning_findings = 0
            total_info_findings = 0
            all_findings = []
            
            for page_data in pages:
                logger.info(f"Processing page {page_data['page_number']} of {len(pages)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                # ===== –í–´–ó–û–í –ü–†–û–í–ï–†–ö–ò –°–¢–†–ê–ù–ò–¶–´ –° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï–ú LLM =====
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
                page_result = await self.perform_norm_control_check_for_page(document_id, page_data)
                
                if page_result["status"] == "success":
                    result_data = page_result["result"]
                    page_results.append(result_data)
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    total_findings += result_data.get("total_findings", 0)
                    total_critical_findings += result_data.get("critical_findings", 0)
                    total_warning_findings += result_data.get("warning_findings", 0)
                    total_info_findings += result_data.get("info_findings", 0)
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ findings
                    page_findings = result_data.get("findings", [])
                    for finding in page_findings:
                        finding["page_number"] = page_data["page_number"]
                    all_findings.extend(page_findings)
                    
                    logger.info(f"Page {page_data['page_number']} processed successfully")
                else:
                    logger.error(f"Failed to process page {page_data['page_number']}: {page_result.get('error', 'Unknown error')}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_results.append({
                        "page_number": page_data["page_number"],
                        "overall_status": "error",
                        "confidence": 0.0,
                        "total_findings": 0,
                        "critical_findings": 0,
                        "warning_findings": 0,
                        "info_findings": 0,
                        "findings": [],
                        "summary": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_result.get('error', 'Unknown error')}",
                        "compliance_percentage": 0
                    })
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            overall_status = "pass"
            if total_critical_findings > 0:
                overall_status = "fail"
            elif total_warning_findings > 0:
                overall_status = "uncertain"
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            total_pages = len(pages)
            successful_pages = len([r for r in page_results if r.get("overall_status") == "pass"])
            compliance_percentage = (successful_pages / total_pages * 100) if total_pages > 0 else 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
            combined_result = {
                "overall_status": overall_status,
                "confidence": 0.8,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ
                "total_findings": total_findings,
                "critical_findings": total_critical_findings,
                "warning_findings": total_warning_findings,
                "info_findings": total_info_findings,
                "total_pages": total_pages,
                "successful_pages": successful_pages,
                "page_results": page_results,
                "findings": all_findings,
                "summary": f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü. –ù–∞–π–¥–µ–Ω–æ {total_findings} –Ω–∞—Ä—É—à–µ–Ω–∏–π.",
                "compliance_percentage": compliance_percentage,
                "recommendations": f"–û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {total_critical_findings} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π, {total_warning_findings} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π, {total_info_findings} –∑–∞–º–µ—á–∞–Ω–∏–π."
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–†–û–í–ï–†–ö–ò LLM =====
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            await self.save_norm_control_result(document_id, combined_result)
            
            return {
                "status": "success",
                "result": combined_result,
                "pages_processed": len(pages)
            }
                    
        except Exception as e:
            logger.error(f"Norm control check error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def save_norm_control_result(self, document_id: int, check_result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –æ—Ç LLM –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        # –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        total_findings = check_result.get("total_findings", 0)
        trace_logger.log_report_generation(document_id, total_findings)
        trace_logger.log_step("REPORT_GENERATION", f"–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å {total_findings} –Ω–∞—Ö–æ–¥–∫–∞–º–∏", document_id)
        def _save_result(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO norm_control_results 
                    (checkable_document_id, analysis_date, analysis_type, model_used, analysis_status,
                     total_findings, critical_findings, warning_findings, info_findings)
                    VALUES (%s, CURRENT_TIMESTAMP, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,
                    "norm_control",
                    "llama3.1:8b",
                    check_result.get("overall_status", "uncertain"),
                    check_result.get("total_findings", 0),
                    check_result.get("critical_findings", 0),
                    check_result.get("warning_findings", 0),
                    check_result.get("info_findings", 0)
                ))
                
                result_id = cursor.fetchone()[0]
                logger.info(f"Saved norm control result {result_id} for document {document_id}")
                return result_id
        
        try:
            result_id = self.execute_in_transaction(_save_result)
            
            # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ï–¢–ê–õ–¨–ù–´–• FINDINGS =====
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º –Ω–∞—Ä—É—à–µ–Ω–∏–∏
            findings = check_result.get("findings", [])
            if findings:
                await self.save_findings_detailed(result_id, findings, document_id)
                logger.info(f"Saved {len(findings)} detailed findings for result {result_id}")
            
            # ===== –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–ï–¢–ê –û –ü–†–û–í–ï–†–ö–ï LLM =====
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM
            await self.create_review_report(document_id, result_id, check_result)
            
            return result_id
                
        except Exception as e:
            logger.error(f"Save norm control result error: {e}")
            raise

    async def save_findings_detailed(self, result_id: int, findings: List[Dict[str, Any]], document_id: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞–∂–¥–æ–º –Ω–∞—Ä—É—à–µ–Ω–∏–∏ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        import json
        
        def _save_findings(conn):
            with conn.cursor() as cursor:
                for finding in findings:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –Ω–∞—Ä—É—à–µ–Ω–∏—è
                    finding_type = finding.get('type', 'violation')
                    if finding_type == 'critical':
                        finding_type = 'violation'
                        severity_level = 5
                    elif finding_type == 'warning':
                        finding_type = 'warning'
                        severity_level = 3
                    elif finding_type == 'info':
                        finding_type = 'info'
                        severity_level = 1
                    else:
                        severity_level = 2
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞
                    code = finding.get('code', '')
                    category = self.determine_finding_category(code, finding.get('description', ''))
                    
                    # –ò—â–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–π –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (clause_id)
                    clause_id = self.find_related_clause_id(finding, cursor)
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –º–µ—Å—Ç–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                    element_reference = {
                        "page_number": finding.get('page_number', 1),
                        "finding_code": code,
                        "location": finding.get('location', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                        "element_type": finding.get('element_type', 'text'),
                        "bounding_box": finding.get('bounding_box', None)
                    }
                    
                    cursor.execute("""
                        INSERT INTO findings 
                        (norm_control_result_id, finding_type, severity_level, category,
                         title, description, recommendation, related_clause_id,
                         related_clause_text, element_reference, rule_applied, confidence_score)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        result_id,
                        finding_type,
                        severity_level,
                        category,
                        finding.get('title', finding.get('description', '–ù–∞—Ä—É—à–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π'))[:200],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                        finding.get('description', ''),
                        finding.get('recommendation', ''),
                        clause_id,
                        finding.get('clause_text', ''),
                        json.dumps(element_reference),
                        code,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–¥ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ
                        finding.get('confidence_score', 1.0)
                    ))
                    
                    logger.debug(f"Saved finding: {code} - {finding.get('description', '')[:50]}...")
        
        try:
            return self.execute_in_transaction(_save_findings)
        except Exception as e:
            logger.error(f"Save findings detailed error: {e}")
            raise

    def determine_finding_category(self, code: str, description: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è"""
        code_upper = code.upper()
        description_lower = description.lower()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–æ–≤
        if 'NC-' in code_upper:
            return 'compliance'
        elif 'FIRE' in code_upper or '–ø–æ–∂–∞—Ä' in description_lower:
            return 'safety'
        elif 'ENERGY' in code_upper or '—ç–Ω–µ—Ä–≥' in description_lower:
            return 'energy_efficiency'
        elif 'STRUCTURE' in code_upper or '–∫–æ–Ω—Å—Ç—Ä—É–∫' in description_lower:
            return 'structural'
        elif 'FORMAT' in code_upper or '—Ñ–æ—Ä–º–∞—Ç' in description_lower:
            return 'formatting'
        elif 'TECH' in code_upper or '—Ç–µ—Ö–Ω–∏—á' in description_lower:
            return 'technical'
        else:
            return 'compliance'

    def find_related_clause_id(self, finding: Dict[str, Any], cursor) -> Optional[int]:
        """–ü–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∫–æ–¥—É –∏ –æ–ø–∏—Å–∞–Ω–∏—é"""
        try:
            code = finding.get('code', '')
            description = finding.get('description', '')
            
            # –ò—â–µ–º –ø–æ –∫–æ–¥—É
            if code:
                cursor.execute("""
                    SELECT id FROM document_clauses 
                    WHERE clause_id ILIKE %s OR clause_number ILIKE %s
                    LIMIT 1
                """, (f'%{code}%', f'%{code}%'))
                result = cursor.fetchone()
                if result:
                    return result[0]
            
            # –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
            if description:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                keywords = self.extract_keywords(description)
                if keywords:
                    # –ò—â–µ–º –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –∏ —Ç–µ–∫—Å—Ç–∞—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    for keyword in keywords[:3]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
                        cursor.execute("""
                            SELECT id FROM document_clauses 
                            WHERE clause_title ILIKE %s OR clause_text ILIKE %s
                            LIMIT 1
                        """, (f'%{keyword}%', f'%{keyword}%'))
                        result = cursor.fetchone()
                        if result:
                            return result[0]
            
            return None
            
        except Exception as e:
            logger.error(f"Error finding related clause: {e}")
            return None

    def extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        import re
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        stop_words = {'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–Ω–∞—Ä—É—à–µ–Ω–∏–µ', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ', '–¥–æ–∫—É–º–µ–Ω—Ç', '–ø—Ä–æ–µ–∫—Ç', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ'}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
        keywords = [word for word in words if word not in stop_words]
        return list(set(keywords))[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

    async def create_review_report(self, document_id: int, result_id: int, check_result: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM"""
        def _create_report(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO review_reports 
                    (checkable_document_id, norm_control_result_id, report_date, review_type,
                     overall_status, reviewer_name, conclusion)
                    VALUES (%s, %s, CURRENT_TIMESTAMP, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,
                    result_id,
                    "automatic",
                    check_result.get("overall_status", "uncertain"),
                    "AI System",
                    f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {check_result.get('summary', '')}"
                ))
                
                report_id = cursor.fetchone()[0]
                logger.info(f"Created review report {report_id} for document {document_id}")
                return report_id
        
        try:
            return self.execute_in_transaction(_create_report)
        except Exception as e:
            logger.error(f"Create review report error: {e}")
            raise

# –ú–æ–¥–µ–ª–∏ Pydantic
class StatusUpdateRequest(BaseModel):
    status: str
    reviewer: str = None
    notes: str = None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
parser = DocumentParser()

@app.post("/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    category: str = Form("other")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"üîç [DEBUG] DocumentParser: Upload started for file: {file.filename}")
    print(f"üîç [DEBUG] DocumentParser: File size: {file.size} bytes")
    print(f"üîç [DEBUG] DocumentParser: Content type: {file.content_type}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if file.size and file.size > MAX_NORMATIVE_DOCUMENT_SIZE:
        print(f"üîç [DEBUG] DocumentParser: File too large: {file.size} bytes (max: {MAX_NORMATIVE_DOCUMENT_SIZE})")
        raise HTTPException(
            status_code=413, 
            detail=f"File too large. Maximum size is {MAX_NORMATIVE_DOCUMENT_SIZE // (1024*1024)}MB. Your file is {file.size // (1024*1024)}MB"
        )
    
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        print(f"üîç [DEBUG] DocumentParser: Reading file content...")
        file_content = await file.read()
        file_size = len(file_content)
        print(f"üîç [DEBUG] DocumentParser: File content read, size: {file_size} bytes")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        print(f"üîç [DEBUG] DocumentParser: Detecting file type...")
        file_type = parser.detect_file_type(file_content)
        print(f"üîç [DEBUG] DocumentParser: Detected file type: {file_type}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞
        print(f"üîç [DEBUG] DocumentParser: Calculating file hash...")
        document_hash = parser.calculate_file_hash(file_content)
        print(f"üîç [DEBUG] DocumentParser: File hash: {document_hash}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        print(f"üîç [DEBUG] DocumentParser: Checking for duplicates...")
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("SELECT id FROM uploaded_documents WHERE document_hash = %s", (document_hash,))
            existing_doc = cursor.fetchone()
            if existing_doc:
                print(f"üîç [DEBUG] DocumentParser: Document already exists with ID: {existing_doc['id']}")
                raise HTTPException(status_code=400, detail="Document already exists")
        print(f"üîç [DEBUG] DocumentParser: No duplicates found")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
        print(f"üîç [DEBUG] DocumentParser: Saving file to disk...")
        file_path = f"/app/uploads/{document_hash}_{file.filename}"
        with open(file_path, "wb") as f:
            f.write(file_content)
        print(f"üîç [DEBUG] DocumentParser: File saved to: {file_path}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å—Ç–∞—Ç—É—Å–æ–º "processing"
        print(f"üîç [DEBUG] DocumentParser: Creating initial database record...")
        document_id = parser.create_initial_document_record(
            file.filename,
            file_type,
            file_size,
            document_hash,
            file_path,
            category
        )
        print(f"üîç [DEBUG] DocumentParser: Created document record with ID: {document_id}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ –∑–∞–≥—Ä—É–∑–∫–µ –±–µ–∑ –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        print(f"üîç [DEBUG] DocumentParser: Document uploaded successfully, processing will start separately")
        
        result = {
            "document_id": document_id,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "status": "uploaded",
            "message": "Document uploaded successfully. Ready for processing.",
            "upload_complete": True
        }
        print(f"üîç [DEBUG] DocumentParser: Upload initiated successfully: {result}")
        return result
        
    except HTTPException as e:
        print(f"üîç [DEBUG] DocumentParser: HTTPException in upload: {e.status_code} - {e.detail}")
        raise e
    except Exception as e:
        print(f"üîç [DEBUG] DocumentParser: Unexpected error in upload: {type(e).__name__}: {str(e)}")
        import traceback
        print(f"üîç [DEBUG] DocumentParser: Traceback: {traceback.format_exc()}")
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents/{document_id}/process")
async def start_document_processing(document_id: int):
    """–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        print(f"üîç [DEBUG] DocumentParser: Starting processing for document {document_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, file_type, file_path, processing_status
                FROM uploaded_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
        
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        
        if document['processing_status'] != 'uploaded':
            raise HTTPException(status_code=400, detail="Document is not ready for processing")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        print(f"üîç [DEBUG] DocumentParser: Adding background task for document {document_id}")
        background_tasks = BackgroundTasks()
        background_tasks.add_task(
            process_document_async,
            document_id=document_id,
            file_path=document['file_path'],
            file_type=document['file_type'],
            filename=document['original_filename']
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        parser.update_document_status(document_id, "processing")
        
        return {
            "document_id": document_id,
            "status": "processing",
            "message": "Document processing started successfully."
        }
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"üîç [DEBUG] DocumentParser: Error starting processing: {e}")
        logger.error(f"Start processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_document_async(document_id: int, file_path: str, file_type: str, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"üîç [DEBUG] DocumentParser: Starting async processing for document {document_id}")
    print(f"üîç [DEBUG] DocumentParser: File type: {file_type}, filename: {filename}")
    print(f"üîç [DEBUG] DocumentParser: File path: {file_path}")
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        print(f"üîç [DEBUG] DocumentParser: Updating status to 'processing' for document {document_id}")
        parser.update_document_status(document_id, "processing")
        print(f"üîç [DEBUG] DocumentParser: Status updated successfully")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞
        print(f"üîç [DEBUG] DocumentParser: Reading file from disk: {file_path}")
        with open(file_path, "rb") as f:
            file_content = f.read()
        print(f"üîç [DEBUG] DocumentParser: File content size: {len(file_content)} bytes")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print(f"üîç [DEBUG] DocumentParser: Parsing document {document_id}...")
        if file_type == "pdf":
            elements = parser.parse_pdf(file_content)
        else:
            raise ValueError(f"Unsupported file type: {file_type}")
        
        print(f"üîç [DEBUG] DocumentParser: Parsing completed for document {document_id}, elements count: {len(elements.document_pages_results)}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        print(f"üîç [DEBUG] DocumentParser: Saving elements to database for document {document_id}...")
        parser.save_elements_to_database(document_id, elements)
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        print(f"üîç [DEBUG] DocumentParser: Calculating tokens for document {document_id}...")
        total_tokens = parser.calculate_document_tokens(elements)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        print(f"üîç [DEBUG] DocumentParser: Updating document record for {document_id}...")
        parser.update_document_completion(document_id, len(elements.document_pages_results), total_tokens)
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ RAG-—Å–µ—Ä–≤–∏—Å
        print(f"üîç [DEBUG] DocumentParser: Starting RAG indexing for document {document_id}...")
        asyncio.create_task(
            parser.index_to_rag_service_async(
                document_id=document_id,
                document_title=filename,
                inspection_result=elements
            )
        )
        
        print(f"üîç [DEBUG] DocumentParser: Async processing completed for document {document_id}")
        
    except Exception as e:
        print(f"üîç [DEBUG] DocumentParser: Error in async processing for document {document_id}: {str(e)}")
        import traceback
        print(f"üîç [DEBUG] DocumentParser: Async processing traceback: {traceback.format_exc()}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "error"
        try:
            parser.update_document_status(document_id, "error")
        except Exception as update_error:
            print(f"üîç [DEBUG] DocumentParser: Failed to update error status: {update_error}")
        
        logger.error(f"Async processing error for document {document_id}: {e}")

async def process_checkable_document_async(document_id: int, document_content: str, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
    start_time = time.time()
    
    # –®–∞–≥ 3: –ù–∞—á–∞–ª–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    trace_logger.log_step("ASYNC_PROCESSING_START", f"–î–æ–∫—É–º–µ–Ω—Ç {document_id}, –∫–æ–Ω—Ç–µ–Ω—Ç: {len(document_content)} —Å–∏–º–≤–æ–ª–æ–≤", document_id)
    
    logger.info(f"üîç [DEBUG] DocumentParser: Starting async processing for checkable document {document_id}")
    logger.info(f"üîç [DEBUG] DocumentParser: Async processing parameters:")
    logger.info(f"üîç [DEBUG] DocumentParser: - Document ID: {document_id}")
    logger.info(f"üîç [DEBUG] DocumentParser: - Document content size: {len(document_content)} characters")
    logger.info(f"üîç [DEBUG] DocumentParser: - Filename: {filename}")
    log_memory_usage("start of async processing")
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        logger.info(f"üîç [DEBUG] DocumentParser: Updating document {document_id} status to 'processing'")
        parser.update_checkable_document_status(document_id, "processing")
        logger.info(f"üîç [DEBUG] DocumentParser: Updated document {document_id} status to 'processing'")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        logger.info(f"üîç [DEBUG] DocumentParser: Splitting document {document_id} into pages")
        pages = parser.split_document_into_pages(document_id)
        logger.info(f"üîç [DEBUG] DocumentParser: Document {document_id} split into {len(pages)} pages")
        
        if not pages:
            logger.warning(f"üîç [DEBUG] DocumentParser: No pages found for document {document_id}, using fallback")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            result = await parser.perform_norm_control_check(document_id, document_content)
        else:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            logger.info(f"üîç [DEBUG] DocumentParser: Starting page-by-page norm control check for document {document_id}")
            all_page_results = []
            
            for i, page_data in enumerate(pages):
                page_number = page_data["page_number"]
                logger.info(f"üîç [DEBUG] DocumentParser: Processing page {page_number}/{len(pages)} for document {document_id}")
                
                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result = await parser.perform_norm_control_check_for_page(document_id, page_data)
                    page_result["page_number"] = page_number
                    all_page_results.append(page_result)
                    
                    logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number} processed successfully")
                    
                except Exception as page_error:
                    logger.error(f"üîç [DEBUG] DocumentParser: Error processing page {page_number}: {page_error}")
                    all_page_results.append({
                        "page_number": page_number,
                        "status": "error",
                        "error": str(page_error)
                    })
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            logger.info(f"üîç [DEBUG] DocumentParser: Combining results from {len(all_page_results)} pages")
            result = parser.combine_page_results(document_id, all_page_results)
        
        logger.info(f"üîç [DEBUG] DocumentParser: Norm control check completed for document {document_id}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "completed"
        parser.update_checkable_document_status(document_id, "completed")
        logger.info(f"üîç [DEBUG] DocumentParser: Updated document {document_id} status to 'completed'")
        
        logger.info(f"üîç [DEBUG] DocumentParser: Async processing completed successfully for document {document_id}")
        
    except Exception as e:
        logger.error(f"üîç [DEBUG] DocumentParser: Error in async processing for checkable document {document_id}: {str(e)}")
        import traceback
        logger.error(f"üîç [DEBUG] DocumentParser: Async processing traceback: {traceback.format_exc()}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "error"
        try:
            parser.update_checkable_document_status(document_id, "error")
        except Exception as update_error:
            logger.error(f"üîç [DEBUG] DocumentParser: Failed to update error status: {update_error}")

@app.get("/documents")
async def list_documents():
    """–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, file_type, file_size, upload_date, processing_status, token_count, category
                FROM uploaded_documents
                ORDER BY upload_date DESC
            """)
            documents = cursor.fetchall()
            
        return {"documents": [dict(doc) for doc in documents]}
        
    except Exception as e:
        logger.error(f"List documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/elements")
async def get_document_elements(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, element_type, element_content, page_number, confidence_score, created_at
                FROM extracted_elements
                WHERE uploaded_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
        return {"elements": [dict(elem) for elem in elements]}
        
    except Exception as e:
        logger.error(f"Get elements error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents/{document_id}/process")
async def process_document_manual(document_id: int, background_tasks: BackgroundTasks):
    """–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        print(f"üîç [DEBUG] DocumentParser: Manual processing requested for document {document_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, file_type, file_size, document_hash, processing_status
                FROM uploaded_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
            
        if document["processing_status"] == "completed":
            return {"message": "Document already processed", "status": "completed"}
            
        print(f"üîç [DEBUG] DocumentParser: Document info: {document}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT file_path FROM uploaded_documents WHERE id = %s
            """, (document_id,))
            result = cursor.fetchone()
            file_path = result.get("file_path") if result else None
            
        if not file_path:
            raise HTTPException(status_code=404, detail="File not found on disk")
            
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        background_tasks.add_task(
            process_document_async,
            document_id=document_id,
            file_path=file_path,
            file_type=document["file_type"],
            filename=document["original_filename"]
        )
        
        return {"message": "Processing started", "document_id": document_id}
        
    except Exception as e:
        logger.error(f"Manual processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}")
async def delete_normative_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = await parser.delete_normative_document(document_id)
        
        if success:
            return {"status": "success", "message": f"Document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete normative document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/checkable-documents/{document_id}")
async def delete_checkable_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = parser.delete_checkable_document(document_id)
        
        if success:
            return {"status": "success", "message": f"Checkable document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete checkable document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/chat")
async def upload_chat_file(
    file: UploadFile = File(...)
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —á–∞—Ç–µ"""
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file_content = await file.read()
        file_size = len(file_content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 10MB –¥–ª—è —á–∞—Ç–∞)
        if file_size > 10 * 1024 * 1024:
            raise HTTPException(status_code=413, detail="File too large. Maximum size is 10MB")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        file_type = parser.detect_file_type(file_content)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞
        elements = parser.parse_file(file_content, file_type)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        text_content = "\n\n".join([elem.get("element_content", "") for elem in elements if elem.get("element_content")])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —á–∞—Ç–∞ (—É—á–∏—Ç—ã–≤–∞—è –ª–∏–º–∏—Ç Ollama ~4000 —Ç–æ–∫–µ–Ω–æ–≤)
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –Ω–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        # 4000 —Ç–æ–∫–µ–Ω–æ–≤ ‚âà 12000 —Å–∏–º–≤–æ–ª–æ–≤ (3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
        max_chars = 10000  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è Ollama
        if len(text_content) > max_chars:
            text_content = text_content[:max_chars] + "\n\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞. –ú–∞–∫—Å–∏–º—É–º 10000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —á–∞—Ç–∞]"
        
        return {
            "success": True,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "content": text_content,
            "elements_count": len(elements.document_pages_results)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat file upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/checkable")
async def upload_checkable_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º —Ç—Ä–µ–π—Å–ª–æ–≥–æ–º"""
    start_time = time.time()
    
    # –®–∞–≥ 1: –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    trace_logger.log_upload_start(file.filename, file.size or 0)
    trace_logger.log_step("UPLOAD_START", f"–§–∞–π–ª: {file.filename}, —Ä–∞–∑–º–µ—Ä: {file.size} –±–∞–π—Ç")
    
    logger.info(f"üîç [DEBUG] DocumentParser: upload_checkable_document started for file: {file.filename}")
    logger.info(f"üîç [DEBUG] DocumentParser: Content-Type: {file.content_type}")
    logger.info(f"üîç [DEBUG] DocumentParser: File size from UploadFile: {file.size}")
    logger.info(f"üîç [DEBUG] DocumentParser: Processing mode: ASYNC")
    logger.info(f"üîç [DEBUG] DocumentParser: Memory limit: {MAX_CHECKABLE_DOCUMENT_SIZE // (1024*1024)}MB")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if file.size and file.size > MAX_CHECKABLE_DOCUMENT_SIZE:
        logger.warning(f"üîç [DEBUG] DocumentParser: File too large: {file.size} bytes (max: {MAX_CHECKABLE_DOCUMENT_SIZE})")
        raise HTTPException(
            status_code=413, 
            detail=f"File too large. Maximum size is {MAX_CHECKABLE_DOCUMENT_SIZE // (1024*1024)}MB. Your file is {file.size // (1024*1024)}MB"
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    log_memory_usage("before file processing")
    available_memory = get_available_memory()
    file_size_mb = file.size / (1024 * 1024) if file.size else 0
    
    if available_memory < file_size_mb * 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3x –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.warning(f"üîç [DEBUG] DocumentParser: Insufficient memory for file {file_size_mb:.1f}MB. Available: {available_memory:.1f}MB")
        raise HTTPException(
            status_code=507,  # Insufficient Storage
            detail=f"Insufficient memory for processing. File size: {file_size_mb:.1f}MB, Available memory: {available_memory:.1f}MB"
        )
    
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Reading file content...")
        file_content = await file.read()
        file_size = len(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: File content read successfully, size: {file_size} bytes")
        logger.info(f"üîç [DEBUG] DocumentParser: Content preview (first 100 bytes): {file_content[:100]}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Detecting file type...")
        file_type = parser.detect_file_type(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: Detected file type: {file_type}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Calculating file hash...")
        document_hash = parser.calculate_file_hash(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: Calculated hash: {document_hash}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        logger.info(f"üîç [DEBUG] DocumentParser: Checking for duplicate document...")
        with parser.db_conn.cursor() as cursor:
            cursor.execute("SELECT id FROM checkable_documents WHERE document_hash = %s", (document_hash,))
            existing_doc = cursor.fetchone()
            if existing_doc:
                logger.warning(f"üîç [DEBUG] DocumentParser: Document already exists with hash: {document_hash}, existing ID: {existing_doc[0]}")
                raise HTTPException(status_code=400, detail="Document already exists")
            else:
                logger.info(f"üîç [DEBUG] DocumentParser: No duplicate found, proceeding with upload")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Starting document parsing for type: {file_type}")
        
        if file_type == "pdf":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing PDF document...")
            logger.info(f"üîç [DEBUG] DocumentParser: PDF size: {file_size / (1024*1024):.1f}MB")
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
            import asyncio
            pdf_timeout = 1200 if file_size > 10 * 1024 * 1024 else 600  # 20 –º–∏–Ω –¥–ª—è —Ñ–∞–π–ª–æ–≤ > 10MB, 10 –º–∏–Ω –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                inspection_result = await asyncio.wait_for(
                    asyncio.to_thread(parser.parse_pdf, file_content),
                    timeout=pdf_timeout
                )
                logger.info(f"üîç [DEBUG] DocumentParser: PDF parsing completed successfully")
                logger.info(f"üîç [DEBUG] DocumentParser: Parsed {len(inspection_result.document_pages_results)} pages")
                logger.info(f"üîç [DEBUG] DocumentParser: Document stats - Total: {inspection_result.document_pages}, Vector: {inspection_result.document_pages_vector}, Scanned: {inspection_result.document_pages_scanned}")
                
            except asyncio.TimeoutError:
                logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing timeout after {pdf_timeout} seconds")
                raise HTTPException(
                    status_code=408,  # Request Timeout
                    detail=f"PDF parsing timeout after {pdf_timeout} seconds. File is too large or complex."
                )
            except Exception as pdf_error:
                logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing error: {pdf_error}")
                raise HTTPException(
                    status_code=500,
                    detail=f"PDF parsing failed: {str(pdf_error)}"
                )

        elif file_type == "docx":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing DOCX document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è docx
            elements = parser.parse_docx(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: DOCX parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "docx_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed DOCX element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: DOCX processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "dwg":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing DWG document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è dwg
            elements = parser.parse_dwg(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: DWG parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "dwg_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed DWG element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: DWG processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "ifc":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing IFC document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è ifc
            elements = parser.parse_ifc(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: IFC parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "ifc_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed IFC element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: IFC processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "txt":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing TXT document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è txt
            elements = parser.parse_text(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: TXT parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "txt_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed TXT element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: TXT processing completed, created {len(inspection_result.document_pages_results)} page results")
        else:
            logger.error(f"üîç [DEBUG] DocumentParser: Unsupported file type: {file_type}")
            raise HTTPException(status_code=400, detail=f"Unsupported file type: {file_type}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Determining document category...")
        content_text = "\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
        logger.debug(f"üîç [DEBUG] DocumentParser: Content text length: {len(content_text)} characters")
        category = parser.determine_document_category(file.filename, content_text)
        logger.info(f"üîç [DEBUG] DocumentParser: Determined category: {category}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        logger.info(f"üîç [DEBUG] DocumentParser: Saving checkable document to database...")
        document_id = parser.save_checkable_document(
            file.filename,
            file.filename,
            file_type,
            file_size,
            document_hash,
            inspection_result,
            category
        )
        logger.info(f"üîç [DEBUG] DocumentParser: Document saved successfully with ID: {document_id}")
        
        # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
        logger.info(f"üîç [DEBUG] DocumentParser: Starting async norm control check...")
        document_content = "\n\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
        background_tasks.add_task(
            process_checkable_document_async,
            document_id=document_id,
            document_content=document_content,
            filename=file.filename
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞
        response_data = {
            "document_id": document_id,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "pages_count": len(inspection_result.document_pages_results),
            "category": category,
            "status": "processing",  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å "completed" –Ω–∞ "processing"
            "review_deadline": (datetime.now() + timedelta(days=2)).isoformat(),
            "document_stats": {
                "total_pages": inspection_result.document_pages,
                "vector_pages": inspection_result.document_pages_vector,
                "scanned_pages": inspection_result.document_pages_scanned,
                "unknown_pages": inspection_result.document_pages_unknown,
                "a4_sheets_equivalent": inspection_result.document_pages_total_a4_sheets_equivalent
            },
            "message": "Document uploaded successfully. Norm control check started in background."
        }
        
        # –®–∞–≥ 2: –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        processing_time = time.time() - start_time
        trace_logger.log_upload_success(file.filename, document_id, processing_time)
        trace_logger.log_step("UPLOAD_SUCCESS", f"–î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {processing_time:.2f}—Å")
        
        logger.info(f"üîç [DEBUG] DocumentParser: Upload completed successfully")
        logger.info(f"üîç [DEBUG] DocumentParser: Response data: {response_data}")
        
        return response_data
        
    except HTTPException as http_ex:
        logger.warning(f"üîç [DEBUG] DocumentParser: HTTPException raised: {http_ex.status_code} - {http_ex.detail}")
        raise
    except Exception as e:
        logger.error(f"üîç [DEBUG] DocumentParser: Upload checkable document error: {e}")
        import traceback
        logger.error(f"üîç [DEBUG] DocumentParser: Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents")
async def list_checkable_documents():
    """–°–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    logger.info(f"üîç [API] List checkable documents called")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ shutdown
        if is_shutting_down:
            logger.warning("üîç [API] Service is shutting down, rejecting checkable documents request")
            raise HTTPException(status_code=503, detail="Service is shutting down")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞–º—è—Ç—å
        if check_memory_pressure():
            logger.warning("üîç [API] High memory pressure detected during checkable documents request")
            cleanup_memory()
        
        logger.info(f"üîç [API] Request list of checked documents via document parser.")
        documents = parser.get_checkable_documents()
        logger.info(f"üîç [API] Successfully retrieved {len(documents)} checkable documents")
        return {"documents": documents}
        
    except HTTPException as e:
        # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º HTTP –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–∞–∫ –µ—Å—Ç—å
        logger.info(f"üîç [API] HTTP excaption: {e.status_code} - {e.detail}")
        raise
    except Exception as e:
        logger.error(f"üîç [API] List checkable documents error: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏ 500
        return {"documents": [], "warning": "Service temporarily unavailable"}

@app.get("/checkable-documents/{document_id}/elements")
async def get_checkable_document_elements(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        logger.info(f"üîç [API] Get checkable document elements called")
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, element_type, element_content, page_number, confidence_score, created_at
                FROM checkable_elements
                WHERE checkable_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
        return {"elements": [dict(elem) for elem in elements]}
        
    except Exception as e:
        logger.error(f"Get checkable elements error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/checkable-documents/{document_id}/status")
async def update_checkable_document_status(
    document_id: int,
    request: StatusUpdateRequest
):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        logger.info(f"üîç [API] Update checkable document {document_id} status {request.status}")

        success = parser.update_review_status(document_id, request.status, request.reviewer, request.notes)
        if success:
            return {"status": "success", "message": f"Document {document_id} status updated"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except Exception as e:
        logger.error(f"Update status error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/cleanup-expired")
async def cleanup_expired_documents():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        deleted_count = parser.cleanup_expired_documents()
        return {
            "status": "success",
            "deleted_count": deleted_count,
            "message": f"Deleted {deleted_count} expired documents"
        }
        
    except Exception as e:
        logger.error(f"Cleanup error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex-documents")
async def reindex_documents():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, token_count
                FROM uploaded_documents
                WHERE processing_status = 'completed'
                ORDER BY upload_date DESC
            """)
            documents = cursor.fetchall()
        
        total_documents = len(documents)
        total_tokens = sum(doc['token_count'] or 0 for doc in documents)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        updated_count = 0
        new_total_tokens = 0
        
        for doc in documents:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –±–ª–æ–∫–µ
                with parser.db_conn.cursor(cursor_factory=RealDictCursor) as element_cursor:
                    element_cursor.execute("""
                        SELECT element_content
                        FROM extracted_elements
                        WHERE uploaded_document_id = %s
                    """, (doc['id'],))
                    elements = element_cursor.fetchall()
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
                total_doc_tokens = 0
                for element in elements:
                    if element['element_content']:
                        total_doc_tokens += parser.count_tokens(element['element_content'])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                with parser.db_conn.cursor() as update_cursor:
                    update_cursor.execute("""
                        UPDATE uploaded_documents
                        SET token_count = %s
                        WHERE id = %s
                    """, (total_doc_tokens, doc['id']))
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ RAG —Å–µ—Ä–≤–∏—Å
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                    with parser.db_conn.cursor(cursor_factory=RealDictCursor) as rag_cursor:
                        rag_cursor.execute("""
                            SELECT element_type, element_content, page_number
                            FROM extracted_elements
                            WHERE uploaded_document_id = %s
                            ORDER BY page_number, id
                        """, (doc['id'],))
                        rag_elements = rag_cursor.fetchall()
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ RAG —Å–µ—Ä–≤–∏—Å
                    await parser.index_to_rag_service(
                        document_id=doc['id'],
                        document_title=doc['original_filename'],
                        elements=[dict(elem) for elem in rag_elements]
                    )
                    logger.info(f"Successfully indexed document {doc['id']} in RAG service")
                except Exception as rag_error:
                    logger.error(f"Failed to index document {doc['id']} in RAG service: {rag_error}")
                
                new_total_tokens += total_doc_tokens
                updated_count += 1
                
            except Exception as e:
                logger.error(f"Error updating tokens for document {doc['id']}: {e}")
                parser.db_conn.rollback()
                continue
        
        parser.db_conn.commit()
        
        return {
            "status": "success",
            "total_documents": total_documents,
            "updated_documents": updated_count,
            "old_total_tokens": total_tokens,
            "new_total_tokens": new_total_tokens,
            "message": f"Reindexed {updated_count} documents with {new_total_tokens} total tokens"
        }
        
    except Exception as e:
        logger.error(f"Reindex error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/tokens")
async def get_document_tokens(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, token_count, file_size, upload_date
                FROM uploaded_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
            if not document:
                raise HTTPException(status_code=404, detail="Document not found")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT element_type, element_content, page_number
                FROM extracted_elements
                WHERE uploaded_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ —Ç–∏–ø–∞–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            token_stats = {
                "total_tokens": document['token_count'] or 0,
                "elements_count": len(elements),
                "by_type": {},
                "by_page": {}
            }
            
            for element in elements:
                element_type = element['element_type']
                page_number = element['page_number'] or 1
                content = element['element_content'] or ""
                
                if content:
                    tokens = parser.count_tokens(content)
                    
                    # –ü–æ —Ç–∏–ø–∞–º
                    if element_type not in token_stats["by_type"]:
                        token_stats["by_type"][element_type] = {"count": 0, "tokens": 0}
                    token_stats["by_type"][element_type]["count"] += 1
                    token_stats["by_type"][element_type]["tokens"] += tokens
                    
                    # –ü–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                    if page_number not in token_stats["by_page"]:
                        token_stats["by_page"][page_number] = {"count": 0, "tokens": 0}
                    token_stats["by_page"][page_number]["count"] += 1
                    token_stats["by_page"][page_number]["tokens"] += tokens
            
            return {
                "document": dict(document),
                "token_statistics": token_stats
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document tokens error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# –ú–æ–¥–µ–ª–∏ Pydantic –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
class SettingUpdateRequest(BaseModel):
    setting_value: str

class SettingCreateRequest(BaseModel):
    setting_key: str
    setting_value: str
    setting_description: str
    setting_type: str = "text"

@app.get("/settings/prompt-templates")
async def get_prompt_templates():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    try:
        logger.info("Getting prompt templates...")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        normcontrol_prompt = parser.get_system_setting("normcontrol_prompt")
        logger.info(f"Normcontrol prompt: {normcontrol_prompt[:100] if normcontrol_prompt else 'None'}...")
        
        # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        prompt_template = parser.get_system_setting("normcontrol_prompt_template")
        logger.info(f"Prompt template: {prompt_template[:100] if prompt_template else 'None'}...")
        
        templates = {
            "normcontrol_prompt": normcontrol_prompt,
            "normcontrol_prompt_template": prompt_template,
            "has_custom_template": prompt_template is not None
        }
        
        logger.info("Successfully retrieved prompt templates")
        return {"status": "success", "templates": templates}
    except Exception as e:
        logger.error(f"Get prompt templates error: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/settings/prompt-templates")
async def update_prompt_template(request: Dict[str, Any]):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞"""
    try:
        template_key = request.get("template_key")
        template_value = request.get("template_value")
        template_description = request.get("template_description", "")
        
        if not template_key or not template_value:
            raise HTTPException(status_code=400, detail="Missing template_key or template_value")
        
        success = parser.create_system_setting(
            template_key, 
            template_value, 
            template_description, 
            "prompt_template"
        )
        
        if success:
            return {"status": "success", "message": f"Template {template_key} updated successfully"}
        else:
            raise HTTPException(status_code=500, detail="Failed to update template")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update prompt template error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/settings")
async def get_settings():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        settings = parser.get_system_settings()
        return {"settings": settings}
    except Exception as e:
        logger.error(f"Get settings error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/settings/{setting_key}")
async def get_setting(setting_key: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        setting_value = parser.get_system_setting(setting_key)
        if setting_value is None:
            raise HTTPException(status_code=404, detail="Setting not found")
        return {"setting_key": setting_key, "setting_value": setting_value}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/settings/{setting_key}")
async def update_setting(setting_key: str, request: SettingUpdateRequest):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.update_system_setting(setting_key, request.setting_value)
        if success:
            return {"status": "success", "message": f"Setting {setting_key} updated successfully"}
        else:
            raise HTTPException(status_code=404, detail="Setting not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/settings")
async def create_setting(request: SettingCreateRequest):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.create_system_setting(
            request.setting_key,
            request.setting_value,
            request.setting_description,
            request.setting_type
        )
        if success:
            return {"status": "success", "message": f"Setting {request.setting_key} created successfully"}
        else:
            raise HTTPException(status_code=500, detail="Failed to create setting")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/settings/{setting_key}")
async def delete_setting(setting_key: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.delete_system_setting(setting_key)
        if success:
            return {"status": "success", "message": f"Setting {setting_key} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Setting not found or cannot be deleted")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/report")
async def get_checkable_document_report(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            cursor.execute("""
                SELECT id, original_filename, file_type, upload_date, review_deadline, review_status
                FROM checkable_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
            if not document:
                raise HTTPException(status_code=404, detail="Document not found")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            cursor.execute("""
                SELECT id, analysis_date, analysis_status,
                       total_findings, critical_findings, warning_findings, info_findings
                FROM norm_control_results
                WHERE checkable_document_id = %s
                ORDER BY analysis_date DESC
                LIMIT 1
            """, (document_id,))
            norm_result = cursor.fetchone()
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ findings
            findings = parser.get_findings_by_document_id(document_id)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç—ã –æ –ø—Ä–æ–≤–µ—Ä–∫–µ
            cursor.execute("""
                SELECT id, report_date, overall_status, reviewer_name, conclusion
                FROM review_reports
                WHERE checkable_document_id = %s
                ORDER BY report_date DESC
            """, (document_id,))
            reports = cursor.fetchall()
            
            return {
                "document": dict(document),
                "norm_control_result": dict(norm_result) if norm_result else None,
                "findings": findings,
                "review_reports": [dict(report) for report in reports]
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document report error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/findings")
async def get_document_findings(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö findings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document = parser.get_checkable_document(document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ findings
        findings = parser.get_findings_by_document_id(document_id)
        
        return {
            "document_id": document_id,
            "findings": findings,
            "total_findings": len(findings),
            "findings_by_category": {
                category: len([f for f in findings if f.get('category') == category])
                for category in set(f.get('category', 'compliance') for f in findings)
            },
            "findings_by_severity": {
                severity: len([f for f in findings if f.get('severity_level') == severity])
                for severity in set(f.get('severity_level', 1) for f in findings)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document findings error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/checkable-documents/{document_id}/check")
async def trigger_norm_control_check(document_id: int):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT element_content
                FROM checkable_elements
                WHERE checkable_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
        
        if not elements:
            raise HTTPException(status_code=404, detail="Document content not found")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        document_content = "\n\n".join([elem["element_content"] for elem in elements])
        
        # ===== –ó–ê–ü–£–°–ö –ü–†–û–í–ï–†–ö–ò –ù–û–†–ú–û–ö–û–ù–¢–†–û–õ–Ø –° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï–ú LLM =====
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
        result = await parser.perform_norm_control_check(document_id, document_content)
        
        return {
            "status": "success",
            "message": "Norm control check completed",
            "result": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Trigger norm control check error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/format-statistics")
async def get_document_format_statistics(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        stats = parser.get_document_format_statistics(document_id)
        return {"status": "success", "statistics": stats}
    except Exception as e:
        logger.error(f"Get document format statistics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/stats")
async def get_documents_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                    COUNT(CASE WHEN processing_status = 'pending' THEN 1 END) as pending_documents,
                    COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents,
                    COUNT(DISTINCT category) as unique_categories,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
            """)
            doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            cursor.execute("""
                SELECT 
                    category,
                    COUNT(*) as count,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
                WHERE category IS NOT NULL AND category != ''
                GROUP BY category
                ORDER BY count DESC
            """)
            categories = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_elements,
                    COUNT(DISTINCT uploaded_document_id) as documents_with_elements
                FROM extracted_elements
            """)
            elements_stats = cursor.fetchone()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            total_docs = doc_stats["total_documents"] or 0
            completed_docs = doc_stats["completed_documents"] or 0
            indexing_progress = (completed_docs / total_docs * 100) if total_docs > 0 else 0
            
        return {
            "status": "success",
            "statistics": {
                "total_documents": total_docs,
                "indexed_documents": completed_docs,
                "indexing_progress_percent": round(indexing_progress, 1),
                "categories_count": doc_stats["unique_categories"] or 0,
                "total_tokens": doc_stats["total_tokens"] or 0,
                "total_elements": elements_stats["total_elements"] or 0,
                "documents_with_elements": elements_stats["documents_with_elements"] or 0,
                "categories": [dict(cat) for cat in categories]
            }
        }
        
    except Exception as e:
        logger.error(f"Get documents stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus"""
    try:
        db_conn = parser.get_db_connection()
        with db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                    COUNT(CASE WHEN processing_status = 'pending' THEN 1 END) as pending_documents,
                    COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents,
                    COUNT(CASE WHEN file_type = 'pdf' THEN 1 END) as pdf_documents,
                    COUNT(CASE WHEN file_type = 'docx' THEN 1 END) as docx_documents,
                    COUNT(CASE WHEN file_type = 'dwg' THEN 1 END) as dwg_documents,
                    COUNT(CASE WHEN file_type = 'txt' THEN 1 END) as txt_documents,
                    SUM(file_size) as total_size_bytes,
                    AVG(file_size) as avg_file_size_bytes,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
            """)
            doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_checkable_documents,
                    COUNT(CASE WHEN review_status = 'pending' THEN 1 END) as pending_reviews,
                    COUNT(CASE WHEN review_status = 'completed' THEN 1 END) as completed_reviews,
                    COUNT(CASE WHEN review_status = 'in_progress' THEN 1 END) as in_progress_reviews,
                    COUNT(CASE WHEN review_status = 'overdue' THEN 1 END) as overdue_reviews
                FROM checkable_documents
            """)
            checkable_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_elements,
                    COUNT(CASE WHEN element_type = 'text' THEN 1 END) as text_elements,
                    COUNT(CASE WHEN element_type = 'table' THEN 1 END) as table_elements,
                    COUNT(CASE WHEN element_type = 'figure' THEN 1 END) as figure_elements,
                    COUNT(CASE WHEN element_type = 'stamp' THEN 1 END) as stamp_elements
                FROM extracted_elements
            """)
            elements_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_norm_control_results,
                    COUNT(CASE WHEN analysis_status = 'completed' THEN 1 END) as completed_checks,
                    COUNT(CASE WHEN analysis_status = 'pending' THEN 1 END) as pending_checks,
                    COUNT(CASE WHEN analysis_status = 'error' THEN 1 END) as error_checks,
                    SUM(total_findings) as total_findings,
                    SUM(critical_findings) as critical_findings,
                    SUM(warning_findings) as warning_findings,
                    SUM(info_findings) as info_findings
                FROM norm_control_results
            """)
            norm_control_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç—á–µ—Ç–∞–º
            cursor.execute("""
                SELECT COUNT(*) as total_review_reports
                FROM review_reports
            """)
            reports_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞)
            cursor.execute("""
                SELECT 
                    COUNT(*) as documents_last_24h
                FROM uploaded_documents 
                WHERE upload_date >= NOW() - INTERVAL '24 hours'
            """)
            time_stats = cursor.fetchone()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics_lines = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        metrics_lines.append(f"# HELP document_parser_documents_total Total number of documents")
        metrics_lines.append(f"# TYPE document_parser_documents_total counter")
        metrics_lines.append(f"document_parser_documents_total {doc_stats['total_documents'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_documents_completed Total number of completed documents")
        metrics_lines.append(f"# TYPE document_parser_documents_completed counter")
        metrics_lines.append(f"document_parser_documents_completed {doc_stats['completed_documents'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_documents_pending Total number of pending documents")
        metrics_lines.append(f"# TYPE document_parser_documents_pending counter")
        metrics_lines.append(f"document_parser_documents_pending {doc_stats['pending_documents'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_documents_error Total number of error documents")
        metrics_lines.append(f"# TYPE document_parser_documents_error counter")
        metrics_lines.append(f"document_parser_documents_error {doc_stats['error_documents'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        metrics_lines.append(f"# HELP document_parser_documents_by_type Documents by file type")
        metrics_lines.append(f"# TYPE document_parser_documents_by_type counter")
        metrics_lines.append(f'document_parser_documents_by_type{{type="pdf"}} {doc_stats["pdf_documents"] or 0}')
        metrics_lines.append(f'document_parser_documents_by_type{{type="docx"}} {doc_stats["docx_documents"] or 0}')
        metrics_lines.append(f'document_parser_documents_by_type{{type="dwg"}} {doc_stats["dwg_documents"] or 0}')
        metrics_lines.append(f'document_parser_documents_by_type{{type="txt"}} {doc_stats["txt_documents"] or 0}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–º–µ—Ä–∞
        metrics_lines.append(f"# HELP document_parser_total_size_bytes Total size of all documents in bytes")
        metrics_lines.append(f"# TYPE document_parser_total_size_bytes gauge")
        metrics_lines.append(f"document_parser_total_size_bytes {doc_stats['total_size_bytes'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_avg_file_size_bytes Average file size in bytes")
        metrics_lines.append(f"# TYPE document_parser_avg_file_size_bytes gauge")
        metrics_lines.append(f"document_parser_avg_file_size_bytes {float(doc_stats['avg_file_size_bytes'] or 0)}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
        metrics_lines.append(f"# HELP document_parser_total_tokens Total number of tokens")
        metrics_lines.append(f"# TYPE document_parser_total_tokens counter")
        metrics_lines.append(f"document_parser_total_tokens {doc_stats['total_tokens'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        metrics_lines.append(f"# HELP document_parser_checkable_documents_total Total number of checkable documents")
        metrics_lines.append(f"# TYPE document_parser_checkable_documents_total counter")
        metrics_lines.append(f"document_parser_checkable_documents_total {checkable_stats['total_checkable_documents'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_checkable_documents_pending_reviews Pending reviews")
        metrics_lines.append(f"# TYPE document_parser_checkable_documents_pending_reviews counter")
        metrics_lines.append(f"document_parser_checkable_documents_pending_reviews {checkable_stats['pending_reviews'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_checkable_documents_completed_reviews Completed reviews")
        metrics_lines.append(f"# TYPE document_parser_checkable_documents_completed_reviews counter")
        metrics_lines.append(f"document_parser_checkable_documents_completed_reviews {checkable_stats['completed_reviews'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        metrics_lines.append(f"# HELP document_parser_elements_total Total number of extracted elements")
        metrics_lines.append(f"# TYPE document_parser_elements_total counter")
        metrics_lines.append(f"document_parser_elements_total {elements_stats['total_elements'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_elements_by_type Elements by type")
        metrics_lines.append(f"# TYPE document_parser_elements_by_type counter")
        metrics_lines.append(f'document_parser_elements_by_type{{type="text"}} {elements_stats["text_elements"] or 0}')
        metrics_lines.append(f'document_parser_elements_by_type{{type="table"}} {elements_stats["table_elements"] or 0}')
        metrics_lines.append(f'document_parser_elements_by_type{{type="figure"}} {elements_stats["figure_elements"] or 0}')
        metrics_lines.append(f'document_parser_elements_by_type{{type="stamp"}} {elements_stats["stamp_elements"] or 0}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
        metrics_lines.append(f"# HELP document_parser_norm_control_results_total Total norm control results")
        metrics_lines.append(f"# TYPE document_parser_norm_control_results_total counter")
        metrics_lines.append(f"document_parser_norm_control_results_total {norm_control_stats['total_norm_control_results'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_norm_control_findings_total Total findings")
        metrics_lines.append(f"# TYPE document_parser_norm_control_findings_total counter")
        metrics_lines.append(f"document_parser_norm_control_findings_total {norm_control_stats['total_findings'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_norm_control_findings_critical Critical findings")
        metrics_lines.append(f"# TYPE document_parser_norm_control_findings_critical counter")
        metrics_lines.append(f"document_parser_norm_control_findings_critical {norm_control_stats['critical_findings'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_norm_control_findings_warning Warning findings")
        metrics_lines.append(f"# TYPE document_parser_norm_control_findings_warning counter")
        metrics_lines.append(f"document_parser_norm_control_findings_warning {norm_control_stats['warning_findings'] or 0}")
        
        metrics_lines.append(f"# HELP document_parser_norm_control_findings_info Info findings")
        metrics_lines.append(f"# TYPE document_parser_norm_control_findings_info counter")
        metrics_lines.append(f"document_parser_norm_control_findings_info {norm_control_stats['info_findings'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç—á–µ—Ç–æ–≤
        metrics_lines.append(f"# HELP document_parser_reports_total Total number of reports")
        metrics_lines.append(f"# TYPE document_parser_reports_total counter")
        metrics_lines.append(f"document_parser_reports_total {reports_stats['total_review_reports'] or 0}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        metrics_lines.append(f"# HELP document_parser_documents_last_24h Documents processed in last 24 hours")
        metrics_lines.append(f"# TYPE document_parser_documents_last_24h counter")
        metrics_lines.append(f"document_parser_documents_last_24h {time_stats['documents_last_24h'] or 0}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        from fastapi.responses import Response
        return Response(
            content="\n".join(metrics_lines),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
        
    except Exception as e:
        logger.error(f"Get metrics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def safe_text(text: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ PDF —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã"""
    if text is None:
        return ""
    
    # –ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å - –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –±—É–¥–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è
    # —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —à—Ä–∏—Ñ—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode
    return str(text)

def format_long_filename(filename: str, max_chars_per_line: int = 50) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫ –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è"""
    if len(filename) <= max_chars_per_line:
        return filename
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
    name, ext = os.path.splitext(filename)
    
    # –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, –≤–∫–ª—é—á–∞–µ–º –µ–≥–æ –≤ –ø–µ—Ä–µ–Ω–æ—Å
    if len(ext) > 10:
        name = filename
        ext = ""
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ max_chars_per_line —Å–∏–º–≤–æ–ª–æ–≤
    lines = []
    current_line = ""
    
    for char in name:
        current_line += char
        if len(current_line) >= max_chars_per_line:
            lines.append(current_line)
            current_line = ""
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à—É—é—Å—è —á–∞—Å—Ç—å
    if current_line:
        lines.append(current_line)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–µ
    if lines and ext:
        lines[-1] += ext
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º
    return "<br/>".join(lines)

def get_russian_font():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —à—Ä–∏—Ñ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.pdfbase.cidfonts import UnicodeCIDFont
        import os
        
        # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π –∫ —à—Ä–∏—Ñ—Ç–∞–º
        font_paths = [
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
            '/usr/share/fonts/TTF/DejaVuSans.ttf',
            '/usr/share/fonts/TTF/LiberationSans-Regular.ttf',
            '/usr/share/fonts/dejavu/DejaVuSans.ttf',
            '/usr/share/fonts/liberation/LiberationSans-Regular.ttf'
        ]
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —à—Ä–∏—Ñ—Ç
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    if 'DejaVu' in font_path:
                        pdfmetrics.registerFont(TTFont('DejaVuSans', font_path))
                        logger.info(f"Registered DejaVuSans font from {font_path}")
                        return 'DejaVuSans'
                    elif 'Liberation' in font_path:
                        pdfmetrics.registerFont(TTFont('LiberationSans', font_path))
                        logger.info(f"Registered LiberationSans font from {font_path}")
                        return 'LiberationSans'
                except Exception as e:
                    logger.warning(f"Failed to register font {font_path}: {e}")
                    continue
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ Unicode —à—Ä–∏—Ñ—Ç—ã ReportLab
        try:
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Unicode —à—Ä–∏—Ñ—Ç
            pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
            logger.info("Registered STSong-Light Unicode font")
            return 'STSong-Light'
        except Exception as e:
            logger.warning(f"Failed to register Unicode font: {e}")
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π
        logger.info("Using built-in Helvetica font")
        return 'Helvetica'
        
    except Exception as e:
        logger.error(f"Error in get_russian_font: {e}")
        return 'Helvetica'

def generate_docx_report_from_template(document: Dict, norm_control_result: Dict, page_results: List[Dict], review_report: Dict) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–∞ DOCX"""
    try:
        from docx import Document
        from docx.shared import Inches
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.oxml.shared import OxmlElement, qn
        from docx.oxml.ns import nsdecls
        from docx.oxml import parse_xml
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —à–∞–±–ª–æ–Ω
        template_path = "/app/report_format/–û–¢–ß–ï–¢_file_name.docx"
        doc = Document(template_path)
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        for paragraph in doc.paragraphs:
            # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã
            if "{{DOCUMENT_NAME}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{DOCUMENT_NAME}}", document['original_filename'])
            if "{{DOCUMENT_TYPE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{DOCUMENT_TYPE}}", document['file_type'].upper())
            if "{{FILE_SIZE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{FILE_SIZE}}", f"{document['file_size'] / 1024:.1f} KB")
            if "{{UPLOAD_DATE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{UPLOAD_DATE}}", document['upload_date'].strftime("%d.%m.%Y %H:%M"))
            if "{{CATEGORY}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{CATEGORY}}", document['category'])
            if "{{STATUS}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{STATUS}}", document['processing_status'])
            
            # –ó–∞–º–µ–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            if norm_control_result:
                if "{{TOTAL_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{TOTAL_FINDINGS}}", str(norm_control_result['total_findings'] or 0))
                if "{{CRITICAL_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{CRITICAL_FINDINGS}}", str(norm_control_result['critical_findings'] or 0))
                if "{{WARNING_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{WARNING_FINDINGS}}", str(norm_control_result['warning_findings'] or 0))
                if "{{INFO_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{INFO_FINDINGS}}", str(norm_control_result['info_findings'] or 0))
                if "{{ANALYSIS_DATE}}" in paragraph.text:
                    analysis_date = norm_control_result['analysis_date'].strftime("%d.%m.%Y %H:%M") if norm_control_result['analysis_date'] else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
                    paragraph.text = paragraph.text.replace("{{ANALYSIS_DATE}}", analysis_date)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        if page_results:
            doc.add_paragraph("–î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú", style='Heading 2')
            
            for page_result in page_results:
                page_num = page_result.get('page_number', 'N/A')
                status = page_result.get('status', 'N/A')
                findings_count = len(page_result.get('findings', []))
                
                p = doc.add_paragraph()
                p.add_run(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: ").bold = True
                p.add_run(f"–°—Ç–∞—Ç—É—Å: {status}, –ù–∞–π–¥–µ–Ω–æ –∑–∞–º–µ—á–∞–Ω–∏–π: {findings_count}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–º–µ—á–∞–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                findings = page_result.get('findings', [])
                for finding in findings:
                    finding_p = doc.add_paragraph()
                    finding_p.add_run(f"‚Ä¢ {finding.get('type', 'Unknown')}: ").bold = True
                    finding_p.add_run(finding.get('description', 'No description'))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return buffer.getvalue()
        
    except Exception as e:
        logger.error(f"Error generating DOCX report: {e}")
        # Fallback –∫ PDF –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        return generate_pdf_report(document, norm_control_result, page_results, review_report)

def extract_project_info_from_filename(filename: str) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    import re
    
    project_info = {
        'project_name': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
        'engineering_stage': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'document_mark': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'revision': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'page_count': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'
    }
    
    try:
        # –ü—Ä–∏–º–µ—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        # –ï110-0038-–£–ö–ö_24.848-–†–î-01-02.12.032-–ê–†_0_0_RU_IFC.pdf
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ö–ñ, –ö–ú, –ê–°, –¢–• –∏ —Ç.–¥.)
        mark_pattern = r'[–ê-–Ø]{2}'
        mark_match = re.search(mark_pattern, filename)
        if mark_match:
            project_info['document_mark'] = mark_match.group(0)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞
        project_pattern = r'(\d{4}-\d{4})'
        project_match = re.search(project_pattern, filename)
        if project_match:
            project_info['project_name'] = f"–ü—Ä–æ–µ–∫—Ç {project_match.group(1)}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–≤–∏–∑–∏—é
        revision_pattern = r'_(\d+)_(\d+)_'
        revision_match = re.search(revision_pattern, filename)
        if revision_match:
            project_info['revision'] = f"{revision_match.group(1)}.{revision_match.group(2)}"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–¥–∏—é –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—Ä–∫–µ
        if project_info['document_mark'] in ['–ö–ñ', '–ö–ú']:
            project_info['engineering_stage'] = '–ö–ñ/–ö–ú'
        elif project_info['document_mark'] in ['–ê–°', '–¢–•']:
            project_info['engineering_stage'] = '–ê–°/–¢–•'
        elif project_info['document_mark'] in ['–ö–°', '–ö–ü']:
            project_info['engineering_stage'] = '–ö–°/–ö–ü'
        
    except Exception as e:
        logger.error(f"Error extracting project info: {e}")
    
    return project_info

def group_findings_by_pages(page_results: List[Dict]) -> Dict[int, List[Dict]]:
    """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–º–µ—á–∞–Ω–∏–π –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
    page_findings = {}
    
    for finding in page_results:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∑–∞–º–µ—á–∞–Ω–∏—è
        page_num = finding.get('page_number', 1)
        if page_num not in page_findings:
            page_findings[page_num] = []
        page_findings[page_num].append(finding)
    
    return page_findings

def get_severity_text(severity_level: int) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–º–µ—á–∞–Ω–∏—è"""
    severity_map = {
        3: "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ",
        2: "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", 
        1: "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ"
    }
    return severity_map.get(severity_level, "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")

def generate_conclusion_from_findings(norm_control_result: Dict, findings: List[Dict]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö findings"""
    conclusion_parts = []
    
    total_findings = len(findings)
    critical_findings = sum(1 for f in findings if f.get('severity_level', 1) >= 4)
    warning_findings = sum(1 for f in findings if f.get('severity_level', 1) in [2, 3])
    info_findings = sum(1 for f in findings if f.get('severity_level', 1) == 1)
    
    if total_findings == 0:
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.")
        conclusion_parts.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø—Ä–∏–Ω—è—Ç–∏—é –±–µ–∑ –∑–∞–º–µ—á–∞–Ω–∏–π.")
    else:
        if critical_findings > 0:
            conclusion_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {critical_findings} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.")
            conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–Ω—è—Ç –≤ —Ç–µ–∫—É—â–µ–º –≤–∏–¥–µ.")
        
        if warning_findings > 0:
            conclusion_parts.append(f"–í—ã—è–≤–ª–µ–Ω–æ {warning_findings} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç—Ä–∞–Ω–∏—Ç—å.")
        
        if info_findings > 0:
            conclusion_parts.append(f"–ù–∞–π–¥–µ–Ω–æ {info_findings} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = {}
        for finding in findings:
            category = finding.get('category', 'compliance')
            if category not in categories:
                categories[category] = 0
            categories[category] += 1
        
        if categories:
            conclusion_parts.append("–û—Å–Ω–æ–≤–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π:")
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                category_name = {
                    'compliance': '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º',
                    'safety': '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å',
                    'energy_efficiency': '—ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
                    'structural': '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è',
                    'formatting': '–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ',
                    'technical': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'
                }.get(category, category)
                conclusion_parts.append(f"- {category_name}: {count} –Ω–∞—Ä—É—à–µ–Ω–∏–π")
        
        conclusion_parts.append("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–Ω–µ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É.")
    
    return " ".join(conclusion_parts)

def generate_conclusion(norm_control_result: Dict, page_summary: Dict[int, List[Dict]]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    if not norm_control_result:
        return "–ê–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
    
    total_findings = norm_control_result.get('total_findings', 0)
    critical_findings = norm_control_result.get('critical_findings', 0)
    warning_findings = norm_control_result.get('warning_findings', 0)
    
    total_pages = len(page_summary)
    pages_with_critical = sum(1 for findings in page_summary.values() 
                             if any(f.get('severity_level') == 3 for f in findings))
    pages_with_warnings = sum(1 for findings in page_summary.values() 
                             if any(f.get('severity_level') in [1, 2] for f in findings))
    
    conclusion_parts = []
    
    if critical_findings > 0:
        conclusion_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {critical_findings} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –Ω–∞ {pages_with_critical} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –ø–µ—Ä–µ–¥ –ø—Ä–∏–Ω—è—Ç–∏–µ–º.")
    elif warning_findings > 0:
        conclusion_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {warning_findings} –∑–∞–º–µ—á–∞–Ω–∏–π –Ω–∞ {pages_with_warnings} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–Ω—è—Ç —Å —É—á–µ—Ç–æ–º —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–º–µ—á–∞–Ω–∏–π.")
    else:
        conclusion_parts.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø—Ä–∏–Ω—è—Ç–∏—é.")
    
    conclusion_parts.append(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    conclusion_parts.append(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–µ—á–∞–Ω–∏–π: {total_findings}")
    
    return " ".join(conclusion_parts)

def generate_pdf_report_with_findings(document: Dict, norm_control_result: Dict, findings: List[Dict], review_report: Dict) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö findings"""
    try:
        # –°–æ–∑–¥–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è PDF
        buffer = io.BytesIO()
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        story = []
        
        # –ü–æ–ª—É—á–∞–µ–º —à—Ä–∏—Ñ—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        font_name = get_russian_font()
        
        # –°—Ç–∏–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontName=font_name,
            fontSize=18,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName=font_name,
            fontSize=14,
            spaceAfter=20,
            textColor=colors.darkblue
        )
        subheading_style = ParagraphStyle(
            'CustomSubHeading',
            parent=styles['Heading3'],
            fontName=font_name,
            fontSize=12,
            spaceAfter=15,
            textColor=colors.darkblue
        )
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10
        )
        small_style = ParagraphStyle(
            'CustomSmall',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=8
        )
        
        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
        filename = document.get('original_filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª')
        story.append(Paragraph(safe_text("–û–¢–ß–ï–¢ –û–ë –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ü–†–û–í–ï–†–ö–ï"), title_style))
        story.append(Paragraph(safe_text(f'"{filename}"'), title_style))
        story.append(Spacer(1, 30))
        
        # 2. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        story.append(Paragraph(safe_text("1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ï–ö–¢–ï –ò –î–û–ö–£–ú–ï–ù–¢–ï"), heading_style))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        project_info = extract_project_info_from_filename(filename)
        
        project_data = [
            [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"), safe_text(project_info.get('project_name', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'))],
            [safe_text("–°—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"), safe_text(project_info.get('engineering_stage', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ú–∞—Ä–∫–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('document_mark', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–†–µ–≤–∏–∑–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('revision', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"), safe_text(str(project_info.get('page_count', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')))],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"), safe_text(format_long_filename(filename))],
            [safe_text("–¢–∏–ø —Ñ–∞–π–ª–∞"), safe_text(document.get('file_type', '').upper())],
            [safe_text("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞"), safe_text(f"{document.get('file_size', 0) / 1024:.1f} KB")],
            [safe_text("–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏"), safe_text(document.get('upload_date', '').strftime("%d.%m.%Y %H:%M") if document.get('upload_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")],
            [safe_text("–î–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏"), safe_text(norm_control_result.get('analysis_date', '').strftime("%d.%m.%Y %H:%M") if norm_control_result and norm_control_result.get('analysis_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")]
        ]
        
        project_table = Table(project_data, colWidths=[2.5*inch, 3.5*inch])
        project_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('FONTSIZE', (0, 0), (-1, -1), 10),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(project_table)
        story.append(Spacer(1, 20))
        
        # 3. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ findings
        story.append(Paragraph(safe_text("2. –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ù–ê–†–£–®–ï–ù–ò–ô"), heading_style))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º findings –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        findings_by_category = {}
        for finding in findings:
            category = finding.get('category', 'compliance')
            if category not in findings_by_category:
                findings_by_category[category] = []
            findings_by_category[category].append(finding)
        
        summary_headers = [
            safe_text("–ö–∞—Ç–µ–≥–æ—Ä–∏—è"), 
            safe_text("–ö—Ä–∏—Ç–∏—á."), 
            safe_text("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"), 
            safe_text("–ò–Ω—Ñ–æ"), 
            safe_text("–í—Å–µ–≥–æ")
        ]
        
        summary_data = [summary_headers]
        total_critical = 0
        total_warnings = 0
        total_info = 0
        
        for category, category_findings in findings_by_category.items():
            critical_count = sum(1 for f in category_findings if f.get('severity_level') >= 4)
            warning_count = sum(1 for f in category_findings if f.get('severity_level') in [2, 3])
            info_count = sum(1 for f in category_findings if f.get('severity_level') == 1)
            total_count = len(category_findings)
            
            total_critical += critical_count
            total_warnings += warning_count
            total_info += info_count
            
            category_name = {
                'compliance': '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–º',
                'safety': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å',
                'energy_efficiency': '–≠–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
                'structural': '–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è',
                'formatting': '–û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ',
                'technical': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'
            }.get(category, category)
            
            summary_data.append([
                safe_text(category_name),
                safe_text(str(critical_count)),
                safe_text(str(warning_count)),
                safe_text(str(info_count)),
                safe_text(str(total_count))
            ])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
        summary_data.append([
            safe_text("–ò–¢–û–ì–û"),
            safe_text(str(total_critical)),
            safe_text(str(total_warnings)),
            safe_text(str(total_info)),
            safe_text(str(len(findings)))
        ])
        
        summary_table = Table(summary_data, colWidths=[2*inch, 1*inch, 1.5*inch, 1*inch, 1*inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('FONTSIZE', (0, 0), (-1, -1), 9),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -2), colors.beige),
            ('BACKGROUND', (0, -1), (-1, -1), colors.lightblue),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(summary_table)
        story.append(Spacer(1, 20))
        
        # 4. –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ findings
        story.append(Paragraph(safe_text("3. –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –ù–ê–†–£–®–ï–ù–ò–Ø–ú"), heading_style))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º findings –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        findings_by_page = {}
        for finding in findings:
            element_ref = finding.get('element_reference', {})
            if isinstance(element_ref, str):
                import json
                try:
                    element_ref = json.loads(element_ref)
                except:
                    element_ref = {}
            
            page_number = element_ref.get('page_number', 1)
            if page_number not in findings_by_page:
                findings_by_page[page_number] = []
            findings_by_page[page_number].append(finding)
        
        for page_num, page_findings in sorted(findings_by_page.items()):
            if page_findings:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏—è–º–∏
                story.append(Paragraph(safe_text(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}"), subheading_style))
                
                for finding in page_findings:
                    severity_text = get_severity_text(finding.get('severity_level', 1))
                    normative_doc = finding.get('normative_document_title', '–ù–µ —É–∫–∞–∑–∞–Ω')
                    normative_clause = finding.get('normative_clause_number', '–ù–µ —É–∫–∞–∑–∞–Ω')
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                    element_ref = finding.get('element_reference', {})
                    if isinstance(element_ref, str):
                        import json
                        try:
                            element_ref = json.loads(element_ref)
                        except:
                            element_ref = {}
                    
                    location = element_ref.get('location', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
                    
                    finding_text = f"""
                    <b>–ö–æ–¥:</b> {finding.get('rule_applied', '–ù–µ —É–∫–∞–∑–∞–Ω')} | 
                    <b>–í–∞–∂–Ω–æ—Å—Ç—å:</b> {severity_text} | 
                    <b>–ö–∞—Ç–µ–≥–æ—Ä–∏—è:</b> {finding.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}
                    <br/>
                    <b>–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:</b> {normative_doc} (–ø—É–Ω–∫—Ç {normative_clause})
                    <br/>
                    <b>–ú–µ—Å—Ç–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:</b> {location}
                    <br/>
                    <b>–ù–∞–∑–≤–∞–Ω–∏–µ:</b> {finding.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {finding.get('description', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:</b> {finding.get('recommendation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    """
                    
                    story.append(Paragraph(safe_text(finding_text), normal_style))
                    story.append(Spacer(1, 10))
        
        # 5. –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        story.append(Paragraph(safe_text("4. –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò"), heading_style))
        
        if norm_control_result:
            results_data = [
                [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
                [safe_text("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π"), safe_text(str(norm_control_result.get('total_findings', 0)))],
                [safe_text("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è"), safe_text(str(norm_control_result.get('critical_findings', 0)))],
                [safe_text("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"), safe_text(str(norm_control_result.get('warning_findings', 0)))],
                [safe_text("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è"), safe_text(str(norm_control_result.get('info_findings', 0)))],
                [safe_text("–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞"), safe_text(norm_control_result.get('analysis_status', '–ù–µ —É–∫–∞–∑–∞–Ω'))]
            ]
            
            results_table = Table(results_data, colWidths=[2.5*inch, 3.5*inch])
            results_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
                ('FONTSIZE', (0, 0), (-1, -1), 10),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(results_table)
            story.append(Spacer(1, 20))
        
        # 6. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        story.append(Paragraph(safe_text("5. –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï"), heading_style))
        
        conclusion = generate_conclusion_from_findings(norm_control_result, findings)
        story.append(Paragraph(safe_text(conclusion), normal_style))
        
        # 7. –ü–æ–¥–ø–∏—Å—å –∏ –¥–∞—Ç–∞
        story.append(Spacer(1, 30))
        story.append(Paragraph(safe_text(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}"), small_style))
        story.append(Paragraph(safe_text("–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"), small_style))
        
        # –°—Ç—Ä–æ–∏–º PDF
        doc.build(story)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        pdf_content = buffer.getvalue()
        buffer.close()
        
        return pdf_content
        
    except Exception as e:
        logger.error(f"PDF generation error: {e}")
        raise

def generate_pdf_report(document: Dict, norm_control_result: Dict, page_results: List[Dict], review_report: Dict) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π (—É—Å—Ç–∞—Ä–µ–≤—à–∞—è –≤–µ—Ä—Å–∏—è)"""
    try:
        # –°–æ–∑–¥–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è PDF
        buffer = io.BytesIO()
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        story = []
        
        # –ü–æ–ª—É—á–∞–µ–º —à—Ä–∏—Ñ—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        font_name = get_russian_font()
        
        # –°—Ç–∏–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontName=font_name,
            fontSize=18,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName=font_name,
            fontSize=14,
            spaceAfter=20,
            textColor=colors.darkblue
        )
        subheading_style = ParagraphStyle(
            'CustomSubHeading',
            parent=styles['Heading3'],
            fontName=font_name,
            fontSize=12,
            spaceAfter=15,
            textColor=colors.darkblue
        )
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10
        )
        small_style = ParagraphStyle(
            'CustomSmall',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=8
        )
        
        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
        filename = document.get('original_filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª')
        story.append(Paragraph(safe_text("–û–¢–ß–ï–¢ –û–ë –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ü–†–û–í–ï–†–ö–ï"), title_style))
        story.append(Paragraph(safe_text(f'"{filename}"'), title_style))
        story.append(Spacer(1, 30))
        
        # 2. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        story.append(Paragraph(safe_text("1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ï–ö–¢–ï –ò –î–û–ö–£–ú–ï–ù–¢–ï"), heading_style))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        project_info = extract_project_info_from_filename(filename)
        
        project_data = [
            [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"), safe_text(project_info.get('project_name', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'))],
            [safe_text("–°—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"), safe_text(project_info.get('engineering_stage', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ú–∞—Ä–∫–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('document_mark', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–†–µ–≤–∏–∑–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('revision', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"), safe_text(str(project_info.get('page_count', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')))],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"), safe_text(format_long_filename(filename))],
            [safe_text("–¢–∏–ø —Ñ–∞–π–ª–∞"), safe_text(document.get('file_type', '').upper())],
            [safe_text("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞"), safe_text(f"{document.get('file_size', 0) / 1024:.1f} KB")],
            [safe_text("–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏"), safe_text(document.get('upload_date', '').strftime("%d.%m.%Y %H:%M") if document.get('upload_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")],
            [safe_text("–î–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏"), safe_text(norm_control_result.get('analysis_date', '').strftime("%d.%m.%Y %H:%M") if norm_control_result and norm_control_result.get('analysis_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")]
        ]
        
        project_table = Table(project_data, colWidths=[2.5*inch, 3.5*inch])
        project_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('FONTSIZE', (0, 0), (-1, -1), 10),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(project_table)
        story.append(Spacer(1, 20))
        
        # 3. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        story.append(Paragraph(safe_text("2. –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú"), heading_style))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_summary = group_findings_by_pages(page_results)
        
        summary_headers = [
            safe_text("‚Ññ —Å—Ç—Ä."), 
            safe_text("–ü—Ä–æ–≤–µ—Ä–µ–Ω–∞"), 
            safe_text("–ö—Ä–∏—Ç–∏—á."), 
            safe_text("–ó–∞–º–µ—á–∞–Ω–∏—è"), 
            safe_text("–í—ã–≤–æ–¥"), 
            safe_text("–°—Ç–∞—Ç—É—Å")
        ]
        
        summary_data = [summary_headers]
        total_critical = 0
        total_warnings = 0
        total_pages = len(page_summary)
        pages_approved = 0
        pages_rejected = 0
        
        for page_num, page_findings in sorted(page_summary.items()):
            critical_count = sum(1 for f in page_findings if f.get('severity_level') == 3)
            warning_count = sum(1 for f in page_findings if f.get('severity_level') in [1, 2])
            total_critical += critical_count
            total_warnings += warning_count
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if critical_count > 0:
                status = safe_text("–û—Ç–∫–ª–æ–Ω–µ–Ω–∞")
                page_status = safe_text("–ù–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
                pages_rejected += 1
            elif warning_count > 0:
                status = safe_text("–£—Å–ª–æ–≤–Ω–æ –ø—Ä–∏–Ω—è—Ç–∞")
                page_status = safe_text("–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
                pages_approved += 1
            else:
                status = safe_text("–ü—Ä–∏–Ω—è—Ç–∞")
                page_status = safe_text("–û–¥–æ–±—Ä–µ–Ω–∞")
                pages_approved += 1
            
            summary_data.append([
                safe_text(str(page_num)),
                safe_text("–î–∞"),
                safe_text(str(critical_count)),
                safe_text(str(warning_count)),
                safe_text(status),
                safe_text(page_status)
            ])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
        summary_data.append([
            safe_text("–ò–¢–û–ì–û"),
            safe_text(str(total_pages)),
            safe_text(str(total_critical)),
            safe_text(str(total_warnings)),
            safe_text(f"–ü—Ä–∏–Ω—è—Ç–æ: {pages_approved}, –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {pages_rejected}"),
            safe_text("" if total_critical == 0 else "–¢—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
        ])
        
        summary_table = Table(summary_data, colWidths=[0.8*inch, 1*inch, 0.8*inch, 1*inch, 1.5*inch, 1.5*inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('FONTSIZE', (0, 0), (-1, -1), 9),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -2), colors.beige),
            ('BACKGROUND', (0, -1), (-1, -1), colors.lightblue),
            ('FONTNAME', (0, -1), (-1, -1), font_name),
            ('FONTSIZE', (0, -1), (-1, -1), 9),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(summary_table)
        story.append(Spacer(1, 20))
        
        # 4. –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        story.append(Paragraph(safe_text("3. –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú"), heading_style))
        
        for page_num, page_findings in sorted(page_summary.items()):
            if page_findings:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∑–∞–º–µ—á–∞–Ω–∏—è–º–∏
                story.append(Paragraph(safe_text(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}"), subheading_style))
                
                for finding in page_findings:
                    severity_text = get_severity_text(finding.get('severity_level', 1))
                    clause_id = finding.get('clause_id', '–ù–µ —É–∫–∞–∑–∞–Ω')
                    location = finding.get('location', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
                    
                    finding_text = f"""
                    <b>–¢–∏–ø:</b> {finding.get('finding_type', '–ù–µ —É–∫–∞–∑–∞–Ω')} | 
                    <b>–í–∞–∂–Ω–æ—Å—Ç—å:</b> {severity_text} | 
                    <b>Clause ID:</b> {clause_id} | 
                    <b>–ú–µ—Å—Ç–æ:</b> {location}
                    <br/>
                    <b>–ù–∞–∑–≤–∞–Ω–∏–µ:</b> {finding.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {finding.get('description', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:</b> {finding.get('recommendation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    """
                    
                    story.append(Paragraph(safe_text(finding_text), normal_style))
                    story.append(Spacer(1, 10))
        
        # 5. –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        story.append(Paragraph(safe_text("4. –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò"), heading_style))
        
        if norm_control_result:
            results_data = [
                [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
                [safe_text("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–µ—á–∞–Ω–∏–π"), safe_text(str(norm_control_result.get('total_findings', 0)))],
                [safe_text("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è"), safe_text(str(norm_control_result.get('critical_findings', 0)))],
                [safe_text("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"), safe_text(str(norm_control_result.get('warning_findings', 0)))],
                [safe_text("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è"), safe_text(str(norm_control_result.get('info_findings', 0)))],
                [safe_text("–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞"), safe_text(norm_control_result.get('analysis_status', '–ù–µ —É–∫–∞–∑–∞–Ω'))]
            ]
            
            results_table = Table(results_data, colWidths=[2.5*inch, 3.5*inch])
            results_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, -1), font_name),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
                ('FONTSIZE', (0, 0), (-1, -1), 10),  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(results_table)
            story.append(Spacer(1, 20))
        
        # 6. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        story.append(Paragraph(safe_text("5. –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï"), heading_style))
        
        conclusion = generate_conclusion(norm_control_result, page_summary)
        story.append(Paragraph(safe_text(conclusion), normal_style))
        
        # 7. –ü–æ–¥–ø–∏—Å—å –∏ –¥–∞—Ç–∞
        story.append(Spacer(1, 30))
        story.append(Paragraph(safe_text(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}"), small_style))
        story.append(Paragraph(safe_text("–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"), small_style))
        
        # –°—Ç—Ä–æ–∏–º PDF
        doc.build(story)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        pdf_content = buffer.getvalue()
        buffer.close()
        
        return pdf_content
        
    except Exception as e:
        logger.error(f"PDF generation error: {e}")
        raise

@app.get("/checkable-documents/{document_id}/download-report")
async def download_report_pdf(document_id: int):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document = parser.get_checkable_document(document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
        norm_control_result = parser.get_norm_control_result_by_document_id(document_id)
        if not norm_control_result:
            raise HTTPException(status_code=404, detail="Norm control results not found")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ findings –∏–∑ –ë–î
        findings = parser.get_findings_by_norm_control_id(norm_control_result['id'])
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_results = parser.get_page_results_by_document_id(document_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞
        review_report = parser.get_review_report_by_norm_control_id(norm_control_result['id'])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö findings
        report_content = generate_pdf_report_with_findings(document, norm_control_result, findings, review_report)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º PDF —Ñ–∞–π–ª
        from fastapi.responses import Response
        media_type = "application/pdf"
        filename = f"norm_control_report_{document_id}.pdf"
        
        return Response(
            content=report_content,
            media_type=media_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating PDF report for document {document_id}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/ollama/api/tags")
async def get_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        import httpx
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://ollama:11434/api/tags")
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"Retrieved {len(data.get('models', []))} models from Ollama")
                return data
            else:
                logger.error(f"Ollama API error: {response.status_code} - {response.text}")
                return {"models": []}
                
    except Exception as e:
        logger.error(f"Error getting models from Ollama: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        return {
            "models": [
                {
                    "name": "llama3.1-optimized-v2:latest",
                    "size": 4900000000,
                    "modified_at": "2025-08-24T00:00:00Z"
                }
            ]
        }

@app.post("/ollama/api/generate")
async def generate_response(request: Request):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM —á–µ—Ä–µ–∑ Ollama"""
    try:
        import httpx
        import json
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞
        body = await request.json()
        model = body.get("model", "llama3.1:8b")
        prompt = body.get("prompt", "")
        stream = body.get("stream", False)
        
        logger.info(f"Generating response for model: {model}, prompt length: {len(prompt)}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç 2 –º–∏–Ω—É—Ç—ã –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò
        timeout = httpx.Timeout(120.0, connect=30.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                "http://ollama:11434/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": stream
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"Generated response successfully, length: {len(data.get('response', ''))}")
                return data
            else:
                logger.error(f"Ollama generation error: {response.status_code} - {response.text}")
                raise HTTPException(status_code=500, detail="LLM generation failed")
                
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/ollama/status")
async def get_ollama_status():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ Ollama –¥–ª—è –¥–∞—à–±–æ–∞—Ä–¥–∞"""
    try:
        import httpx
        import psutil
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://ollama:11434/api/tags")
            models_data = response.json() if response.status_code == 200 else {"models": []}
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ Ollama
        ollama_process = None
        for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent']):
            if 'ollama' in proc.info['name'].lower():
                ollama_process = proc
                break
        
        memory_usage = "0 GB / 8 GB"
        cpu_usage = "0%"
        
        if ollama_process:
            try:
                memory_mb = ollama_process.info['memory_info'].rss / (1024 * 1024)
                memory_usage = f"{memory_mb:.1f} GB / 8 GB"
                cpu_usage = f"{ollama_process.cpu_percent():.1f}%"
            except:
                pass
        
        return {
            "service_health": "healthy" if response.status_code == 200 else "unhealthy",
            "uptime": "02:15:30",  # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞—Å—á–µ—Ç uptime
            "last_heartbeat": datetime.now().isoformat(),
            "memory_usage": memory_usage,
            "cpu_usage": cpu_usage,
            "active_connections": len(models_data.get("models", [])),
            "models_count": len(models_data.get("models", []))
        }
        
    except Exception as e:
        logger.error(f"Error getting Ollama status: {e}")
        return {
            "service_health": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/ollama/performance")
async def get_ollama_performance():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Ollama"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        db_conn = parser.get_db_connection()
        with db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
            cursor.execute("""
                SELECT 
                    COUNT(*) as requests_last_hour,
                    AVG(EXTRACT(EPOCH FROM (analysis_date - upload_date))) as avg_response_time
                FROM norm_control_results ncr
                JOIN checkable_documents cd ON ncr.checkable_document_id = cd.id
                WHERE ncr.analysis_date >= NOW() - INTERVAL '1 hour'
            """)
            performance_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_requests,
                    COUNT(CASE WHEN analysis_status = 'completed' THEN 1 END) as successful_requests
                FROM norm_control_results
                WHERE analysis_date >= NOW() - INTERVAL '1 hour'
            """)
            success_stats = cursor.fetchone()
        
        total_requests = success_stats["total_requests"] or 0
        successful_requests = success_stats["successful_requests"] or 0
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "requests_last_hour": performance_stats["requests_last_hour"] or 0,
            "average_response_time": round(float(performance_stats["avg_response_time"] or 0), 2),
            "success_rate": round(success_rate, 1),
            "timeout_rate": round(100 - success_rate, 1),
            "tokens_generated": 12500,  # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
            "tokens_per_second": 45.2   # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞—Å—á–µ—Ç
        }
        
    except Exception as e:
        logger.error(f"Error getting Ollama performance: {e}")
        return {
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/normcontrol/analytics")
async def get_normcontrol_analytics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –¥–∞—à–±–æ–∞—Ä–¥–∞"""
    try:
        db_conn = parser.get_db_connection()
        with db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(CASE WHEN review_status = 'pending' THEN 1 END) as pending_reviews,
                    COUNT(CASE WHEN review_status = 'completed' THEN 1 END) as completed_reviews,
                    COUNT(CASE WHEN review_status = 'in_progress' THEN 1 END) as in_progress_reviews
                FROM checkable_documents
            """)
            doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞—Ö–æ–¥–æ–∫
            cursor.execute("""
                SELECT 
                    SUM(total_findings) as total_findings,
                    SUM(critical_findings) as critical_findings,
                    SUM(warning_findings) as warning_findings,
                    SUM(info_findings) as info_findings
                FROM norm_control_results
            """)
            findings_stats = cursor.fetchone()
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            cursor.execute("""
                SELECT 
                    AVG(EXTRACT(EPOCH FROM (analysis_date - upload_date))) as avg_processing_time,
                    COUNT(*) as documents_processed_today
                FROM norm_control_results ncr
                JOIN checkable_documents cd ON ncr.checkable_document_id = cd.id
                WHERE ncr.analysis_date >= CURRENT_DATE
            """)
            performance_stats = cursor.fetchone()
        
        total_documents = doc_stats["total_documents"] or 0
        completed_reviews = doc_stats["completed_reviews"] or 0
        compliance_rate = (completed_reviews / total_documents * 100) if total_documents > 0 else 0
        
        total_findings = findings_stats["total_findings"] or 0
        avg_findings = (total_findings / total_documents) if total_documents > 0 else 0
        
        return {
            "total_documents": total_documents,
            "compliance_rate": round(compliance_rate, 1),
            "average_findings": round(avg_findings, 2),
            "processing_efficiency": 92.3,  # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞—Å—á–µ—Ç
            "overview": {
                "pending_reviews": doc_stats["pending_reviews"] or 0,
                "completed_reviews": completed_reviews,
                "in_progress_reviews": doc_stats["in_progress_reviews"] or 0
            },
            "results": {
                "total_findings": total_findings,
                "critical_findings": findings_stats["critical_findings"] or 0,
                "warning_findings": findings_stats["warning_findings"] or 0,
                "info_findings": findings_stats["info_findings"] or 0
            },
            "performance": {
                "average_processing_time": round(float(performance_stats["avg_processing_time"] or 0) / 60, 1),  # –≤ –º–∏–Ω—É—Ç–∞—Ö
                "documents_processed_today": performance_stats["documents_processed_today"] or 0
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting normcontrol analytics: {e}")
        return {
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    health_start_time = datetime.now()
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "checks": {},
        "uptime": str(datetime.now() - startup_time),
        "process_id": os.getpid()
    }
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è shutdown
        if is_shutting_down:
            health_status["status"] = "shutting_down"
            health_status["checks"]["shutdown"] = "in_progress"
            logger.warning(f"üîç [HEALTH] Service is shutting down, health check at {health_start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        try:
            memory_info = get_memory_usage()
            if "error" not in memory_info:
                health_status["checks"]["memory"] = {
                    "rss_mb": round(memory_info['rss_mb'], 1),
                    "percent": round(memory_info['percent'], 1),
                    "available_mb": round(memory_info['available_mb'], 1)
                }
                if memory_info['percent'] > 90:
                    health_status["status"] = "degraded"
                    health_status["checks"]["memory"]["status"] = "high_usage"
                    logger.warning(f"üîç [HEALTH] High memory usage detected: {memory_info['percent']:.1f}%")
                else:
                    health_status["checks"]["memory"]["status"] = "ok"
            else:
                health_status["checks"]["memory"] = {"status": "error", "error": memory_info["error"]}
                logger.error(f"üîç [HEALTH] Memory check error: {memory_info['error']}")
        except Exception as mem_error:
            health_status["checks"]["memory"] = {"status": "error", "error": str(mem_error)}
            logger.error(f"üîç [HEALTH] Memory check exception: {mem_error}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ PostgreSQL
        try:
            db_conn = parser.get_db_connection()
            with db_conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                cursor.fetchone()
            health_status["checks"]["postgresql"] = {"status": "ok"}
        except Exception as pg_error:
            health_status["checks"]["postgresql"] = {"status": "error", "error": str(pg_error)}
            health_status["status"] = "unhealthy"
            logger.error(f"üîç [HEALTH] PostgreSQL check failed: {pg_error}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        try:
            parser.qdrant_client.get_collections()
            health_status["checks"]["qdrant"] = {"status": "ok"}
        except Exception as qd_error:
            health_status["checks"]["qdrant"] = {"status": "error", "error": str(qd_error)}
            health_status["status"] = "unhealthy"
            logger.error(f"üîç [HEALTH] Qdrant check failed: {qd_error}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        if health_status["status"] == "healthy" and any(check.get("status") == "error" for check in health_status["checks"].values()):
            health_status["status"] = "degraded"
            logger.warning(f"üîç [HEALTH] Service status degraded due to component errors")
        
        health_end_time = datetime.now()
        health_duration = (health_end_time - health_start_time).total_seconds()
        health_status["check_duration_ms"] = round(health_duration * 1000, 2)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç health check
        if health_status["status"] == "healthy":
            logger.debug(f"üîç [HEALTH] Health check passed in {health_duration:.3f}s")
        elif health_status["status"] == "degraded":
            logger.warning(f"üîç [HEALTH] Health check degraded in {health_duration:.3f}s")
        else:
            logger.error(f"üîç [HEALTH] Health check failed in {health_duration:.3f}s")
        
        if health_status["status"] == "unhealthy":
            return JSONResponse(
                status_code=503,
                content=health_status
            )
        elif health_status["status"] == "degraded":
            return JSONResponse(
                status_code=200,
                content=health_status
            )
        else:
            return health_status
        
    except Exception as e:
        health_end_time = datetime.now()
        health_duration = (health_end_time - health_start_time).total_seconds()
        logger.error(f"üîç [HEALTH] Health check error after {health_duration:.3f}s: {e}")
        return JSONResponse(
            status_code=503,
            content={
                "status": "unhealthy", 
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "check_duration_ms": round(health_duration * 1000, 2)
            }
        )

# –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
@app.get("/test-prompt-templates")
async def test_prompt_templates():
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω–æ–≤"""
    return {"status": "success", "message": "Test endpoint works"}

async def shutdown_event_handler():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""
    global is_shutting_down
    shutdown_start_time = datetime.now()
    logger.info(f"üîç [SHUTDOWN] Starting graceful shutdown at {shutdown_start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    logger.info(f"üîç [SHUTDOWN] Process ID: {os.getpid()}, Uptime: {shutdown_start_time - startup_time}")
    is_shutting_down = True
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ shutdown
    try:
        memory_info = get_memory_usage()
        if "error" not in memory_info:
            logger.info(f"üîç [SHUTDOWN] Memory usage before shutdown: RSS: {memory_info['rss_mb']:.1f}MB, VMS: {memory_info['vms_mb']:.1f}MB, Percent: {memory_info['percent']:.1f}%")
    except Exception as e:
        logger.warning(f"üîç [SHUTDOWN] Could not get memory info: {e}")
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    logger.info("üîç [SHUTDOWN] Waiting 5 seconds for current requests to complete...")
    await asyncio.sleep(5)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
    try:
        if parser.db_conn and not parser.db_conn.closed:
            parser.db_conn.close()
            logger.info("üîç [SHUTDOWN] PostgreSQL connection closed successfully")
        else:
            logger.info("üîç [SHUTDOWN] PostgreSQL connection was already closed")
    except Exception as e:
        logger.error(f"üîç [SHUTDOWN] Error closing PostgreSQL connection: {e}")
    
    try:
        if parser.qdrant_client:
            parser.qdrant_client.close()
            logger.info("üîç [SHUTDOWN] Qdrant connection closed successfully")
        else:
            logger.info("üîç [SHUTDOWN] Qdrant connection was already closed")
    except Exception as e:
        logger.error(f"üîç [SHUTDOWN] Error closing Qdrant connection: {e}")
    
    shutdown_end_time = datetime.now()
    shutdown_duration = shutdown_end_time - shutdown_start_time
    logger.info(f"üîç [SHUTDOWN] Graceful shutdown completed at {shutdown_end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    logger.info(f"üîç [SHUTDOWN] Total shutdown duration: {shutdown_duration.total_seconds():.3f} seconds")

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ shutdown
app.add_event_handler("shutdown", shutdown_event_handler)

if __name__ == "__main__":
    import uvicorn
    startup_time = datetime.now()
    logger.info(f"üîç [STARTUP] Starting Document Parser Service at {startup_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    logger.info(f"üîç [STARTUP] Process ID: {os.getpid()}, Parent PID: {os.getppid()}")
    logger.info(f"üîç [STARTUP] Working directory: {os.getcwd()}")
    logger.info(f"üîç [STARTUP] Python version: {sys.version}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ
    try:
        import platform
        logger.info(f"üîç [STARTUP] Platform: {platform.platform()}")
        logger.info(f"üîç [STARTUP] Architecture: {platform.architecture()}")
        logger.info(f"üîç [STARTUP] Machine: {platform.machine()}")
    except Exception as e:
        logger.warning(f"üîç [STARTUP] Could not get platform info: {e}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    try:
        memory_info = get_memory_usage()
        if "error" not in memory_info:
            logger.info(f"üîç [STARTUP] Initial memory usage: RSS: {memory_info['rss_mb']:.1f}MB, VMS: {memory_info['vms_mb']:.1f}MB, Percent: {memory_info['percent']:.1f}%")
    except Exception as e:
        logger.warning(f"üîç [STARTUP] Could not get initial memory info: {e}")
    
    uvicorn.run(app, host="0.0.0.0", port=8001)
