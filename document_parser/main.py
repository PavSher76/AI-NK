import os
import hashlib
import json
import logging
import io
import psutil
import gc
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams
import magic
import PyPDF2
from docx import Document
import re
import httpx
import asyncio
import tiktoken

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
import sys
sys.path.append('/app')
try:
    from config import MAX_CHECKABLE_DOCUMENT_SIZE, MAX_NORMATIVE_DOCUMENT_SIZE, LLM_REQUEST_TIMEOUT
except ImportError:
    # Fallback –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    MAX_CHECKABLE_DOCUMENT_SIZE = int(os.getenv("MAX_CHECKABLE_DOCUMENT_SIZE", "104857600"))  # 100 –ú–ë
    MAX_NORMATIVE_DOCUMENT_SIZE = int(os.getenv("MAX_NORMATIVE_DOCUMENT_SIZE", "209715200"))  # 200 –ú–ë
    LLM_REQUEST_TIMEOUT = int(os.getenv("LLM_REQUEST_TIMEOUT", "120"))

def get_memory_usage() -> Dict[str, float]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
    process = psutil.Process()
    memory_info = process.memory_info()
    memory_percent = process.memory_percent()
    
    return {
        "rss_mb": memory_info.rss / 1024 / 1024,  # RSS –≤ –ú–ë
        "vms_mb": memory_info.vms / 1024 / 1024,  # VMS –≤ –ú–ë
        "percent": memory_percent,  # –ü—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        "available_mb": psutil.virtual_memory().available / 1024 / 1024  # –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å –≤ –ú–ë
    }

def get_available_memory() -> float:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –ú–ë"""
    return psutil.virtual_memory().available / 1024 / 1024

def log_memory_usage(context: str = ""):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    memory_info = get_memory_usage()
    logger.info(f"üîç [DEBUG] DocumentParser: Memory usage {context}: "
               f"RSS: {memory_info['rss_mb']:.1f}MB, "
               f"VMS: {memory_info['vms_mb']:.1f}MB, "
               f"Percent: {memory_info['percent']:.1f}%, "
               f"Available: {memory_info['available_mb']:.1f}MB")

def cleanup_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    gc.collect()
    logger.info(f"üîç [DEBUG] DocumentParser: Memory cleanup completed")

# OCR imports
import pytesseract
from PIL import Image
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import tempfile
import math

from datetime import datetime, timedelta

# PDF generation imports
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_RIGHT, TA_JUSTIFY
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont


# —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
class DocumentInspectionResult:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_id = None # ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_name = None # –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ "A9.5.MTH.04"
        self.document_type = None # pdf, docx, dwg, txt
        self.document_engineering_stage = None # –ü–î, –†–î, –¢–≠–û
        self.document_mark = None # –ú–∞—Ä–∫–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: –ö–ñ, –ö–ú, –ê–°, –¢–•, –ö–°, –ö–ü, –ö–†, –ö–†–°, –ö–†–°-1, –ö–†–°-2, –ö–†–°-3, –ö–†–°-4, –ö–†–°-5, –ö–†–°-6, –ö–†–°-7, –ö–†–°-8, –ö–†–°-9, –ö–†–°-10
        self.document_number = None # –ù–æ–º–µ—Ä –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_date = None # –î–∞—Ç–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_author = None # –†–ê–ó–†–ê–ë–û–¢–ê–õ –∫–æ–º–ø–ª–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_reviewer = None # –ü–†–û–í–ï–†–ò–õ –∫–æ–º–ø–ª–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_approver = None # –ì–ò–ü –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.document_status = None # –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: IFR, IFD, IFC, IFS, IFT, IFT-1, IFT-2, IFT-3, IFT-4, IFT-5, IFT-6, IFT-7, IFT-8, IFT-9, IFT-10
        self.document_size = None # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        self.document_pages = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_vector = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_scanned = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_unknown = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_total = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_pages_total_a4_sheets_equivalent = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4 —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        self.document_pages_with_violations = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏
        self.document_pages_clean = None # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        
        # –û–±—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        self.document_total_violations = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        self.document_critical_violations = None # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_major_violations = None # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_minor_violations = None # –ú–µ–ª–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.document_info_violations = None # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        self.document_compliance_percentage = None # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (0-100)
        self.document_violations_per_page_avg = None # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        # –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–æ–ª—è (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.document_total_errors_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_warnings_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_info_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_suggestions_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.document_total_corrections_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_pages_results = [] # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞

class DocumentPageInspectionResult:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_number = None # –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.page_type = None # vector, scanned, unknown
        self.page_format = None # A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, B0, B1, B2, B3, B4, B5, B6, B7, B8, B9, B10
        self.page_width_mm = None # –®–∏—Ä–∏–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º
        self.page_height_mm = None # –í—ã—Å–æ—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º
        self.page_orientation = None # portrait, landscape
        self.page_a4_sheets_equivalent = None # –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –ª–∏—Å—Ç–æ–≤ –ê4
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.page_text = None # –¢–µ–∫—Å—Ç –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_method = None # –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –ª–∏—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.page_text_length = None # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR (–¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
        self.page_ocr_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR
        self.page_ocr_method = None # –ú–µ—Ç–æ–¥ OCR
        self.page_ocr_all_results = [] # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR
        
        # –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_violations_count = None # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_critical_violations = None # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_major_violations = None # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_minor_violations = None # –ú–µ–ª–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_info_violations = None # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_compliance_percentage = None # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        # –î–µ—Ç–∞–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.page_violations_details = [] # –î–µ—Ç–∞–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ


class DocumentViolationDetail:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª–µ–π –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"""
    
    def __init__(self):
        self.violation_type = None # critical, major, minor, info
        self.violation_category = None # general_requirements, text_part, graphical_part, specifications, assembly_drawings, detail_drawings, schemes
        self.violation_description = None # –û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        self.violation_clause = None # –ü—É–Ω–∫—Ç –Ω–æ—Ä–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ì–û–°–¢ –† 21.1101-2013 –ø.5.2)
        self.violation_severity = None # –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (1-5)
        self.violation_recommendation = None # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
        self.violation_location = None # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        self.violation_confidence = None # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–∏ (0-1)




# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Document Parser Service", version="1.0.0")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤ –¥–æ 100MB
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class LargeFileMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if request.url.path == "/upload":
            request.scope["max_content_size"] = MAX_NORMATIVE_DOCUMENT_SIZE
        elif request.url.path == "/upload/checkable":
            request.scope["max_content_size"] = MAX_CHECKABLE_DOCUMENT_SIZE
        return await call_next(request)

app.add_middleware(LargeFileMiddleware)

# CORS middleware —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤—ã—à–µ

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "norms-db")
POSTGRES_DB = os.getenv("POSTGRES_DB", "norms_db")
POSTGRES_USER = os.getenv("POSTGRES_USER", "norms_user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "norms_password")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "http://rag-service:8003")

class TransactionContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏ PostgreSQL"""
    
    def __init__(self, connection):
        self.connection = connection
        self.cursor = None
    
    def __enter__(self):
        """–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
        self.cursor = self.connection.cursor()
        logger.debug("Transaction started")
        return self.connection
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º commit/rollback"""
        try:
            if exc_type is None:
                # –ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π - –∫–æ–º–º–∏—Ç–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                self.connection.commit()
                logger.debug("Transaction committed successfully")
            else:
                # –ï—Å—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è - –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                self.connection.rollback()
                logger.error(f"Transaction rolled back due to error: {exc_type.__name__}: {exc_val}")
        except Exception as e:
            logger.error(f"Error during transaction cleanup: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–∫–∞—Ç–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ—á–∏—Å—Ç–∫–∏
            try:
                self.connection.rollback()
            except:
                pass
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫—É—Ä—Å–æ—Ä
            if self.cursor:
                try:
                    self.cursor.close()
                except:
                    pass


class DocumentParser:
    def __init__(self):
        self.db_conn = None
        self.qdrant_client = None
        self.connect_databases()
    
    def connect_databases(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # PostgreSQL
            self.db_conn = psycopg2.connect(
                host=POSTGRES_HOST,
                database=POSTGRES_DB,
                user=POSTGRES_USER,
                password=POSTGRES_PASSWORD
            )
            logger.info("Connected to PostgreSQL")
            
            # Qdrant
            self.qdrant_client = qdrant_client.QdrantClient(
                host=QDRANT_HOST,
                port=QDRANT_PORT
            )
            logger.info("Connected to Qdrant")
            
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            raise
    
    def get_db_connection(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∞–∫—Ç–∏–≤–Ω–æ
            if self.db_conn is None or self.db_conn.closed:
                logger.info("Reconnecting to PostgreSQL...")
                self.db_conn = psycopg2.connect(
                    host=POSTGRES_HOST,
                    database=POSTGRES_DB,
                    user=POSTGRES_USER,
                    password=POSTGRES_PASSWORD
                )
                logger.info("Reconnected to PostgreSQL")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            with self.db_conn.cursor() as cursor:
                cursor.execute("SELECT 1")
            
            return self.db_conn
            
        except Exception as e:
            logger.error(f"Database connection check failed: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
            try:
                if self.db_conn and not self.db_conn.closed:
                    self.db_conn.close()
            except:
                pass
            
            self.db_conn = psycopg2.connect(
                host=POSTGRES_HOST,
                database=POSTGRES_DB,
                user=POSTGRES_USER,
                password=POSTGRES_PASSWORD
            )
            logger.info("Reconnected to PostgreSQL after error")
            return self.db_conn
    
    def transaction_context(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏"""
        return TransactionContext(self.get_db_connection())
    
    def execute_in_transaction(self, operation, *args, **kwargs):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º"""
        with self.transaction_context() as conn:
            return operation(conn, *args, **kwargs)
    def calculate_file_hash(self, file_content: bytes) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHA-256 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        return hashlib.sha256(file_content).hexdigest()

    def extract_text_from_image(self, image: Image.Image, page_number: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OCR"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL Image –≤ OpenCV —Ñ–æ—Ä–º–∞—Ç
            opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è OCR
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è
            # 1. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ—Ä–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            
            # 2. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞
            kernel = np.ones((1, 1), np.uint8)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
            
            # 3. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OCR –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            custom_config = r'--oem 3 --psm 6 -l rus+eng'
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
            results = []
            
            # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            try:
                text1 = pytesseract.image_to_string(image, config=custom_config)
                if text1.strip():
                    results.append(("original", text1.strip(), 0.7))
            except Exception as e:
                logger.warning(f"OCR failed on original image for page {page_number}: {e}")
            
            # 2. –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ—Ä–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            try:
                text2 = pytesseract.image_to_string(thresh, config=custom_config)
                if text2.strip():
                    results.append(("threshold", text2.strip(), 0.8))
            except Exception as e:
                logger.warning(f"OCR failed on threshold image for page {page_number}: {e}")
            
            # 3. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (CLAHE)
            try:
                text3 = pytesseract.image_to_string(enhanced, config=custom_config)
                if text3.strip():
                    results.append(("enhanced", text3.strip(), 0.9))
            except Exception as e:
                logger.warning(f"OCR failed on enhanced image for page {page_number}: {e}")
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if results:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                results.sort(key=lambda x: (len(x[1]), x[2]), reverse=True)
                best_method, best_text, confidence = results[0]
                
                logger.info(f"OCR completed for page {page_number} using {best_method} method, confidence: {confidence}")
                
                return {
                    "text": best_text,
                    "confidence": confidence,
                    "method": best_method,
                    "all_results": results
                }
            else:
                logger.warning(f"No OCR results for page {page_number}")
                return {
                    "text": "",
                    "confidence": 0.0,
                    "method": "none",
                    "all_results": []
                }
                
        except Exception as e:
            logger.error(f"OCR processing error for page {page_number}: {e}")
            return {
                "text": "",
                "confidence": 0.0,
                "method": "error",
                "all_results": []
            }

    def get_page_format_info(self, page) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ç–æ—á–∫–∞—Ö (1/72 –¥—é–π–º–∞)
            media_box = page.mediabox
            width_pt = float(media_box.width)
            height_pt = float(media_box.height)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–∏–ª–ª–∏–º–µ—Ç—Ä—ã (1 –¥—é–π–º = 25.4 –º–º, 1 —Ç–æ—á–∫–∞ = 1/72 –¥—é–π–º–∞)
            width_mm = width_pt * 25.4 / 72
            height_mm = height_pt * 25.4 / 72
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            format_name = self.determine_page_format(width_mm, height_mm)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
            a4_sheets = self.calculate_a4_sheets(width_mm, height_mm)
            
            return {
                "width_pt": width_pt,
                "height_pt": height_pt,
                "width_mm": round(width_mm, 2),
                "height_mm": round(height_mm, 2),
                "format_name": format_name,
                "a4_sheets": a4_sheets,
                "orientation": "portrait" if height_mm > width_mm else "landscape"
            }
            
        except Exception as e:
            logger.error(f"Error getting page format info: {e}")
            return {
                "width_pt": 0,
                "height_pt": 0,
                "width_mm": 0,
                "height_mm": 0,
                "format_name": "unknown",
                "a4_sheets": 0,
                "orientation": "unknown"
            }

    def determine_page_format(self, width_mm: float, height_mm: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤ –º–º (—à–∏—Ä–∏–Ω–∞ x –≤—ã—Å–æ—Ç–∞)
        formats = {
            "A0": (841, 1189),
            "A1": (594, 841),
            "A2": (420, 594),
            "A3": (297, 420),
            "A4": (210, 297),
            "A5": (148, 210),
            "A6": (105, 148),
            "A7": (74, 105),
            "A8": (52, 74),
            "A9": (37, 52),
            "A10": (26, 37),
            "B0": (1000, 1414),
            "B1": (707, 1000),
            "B2": (500, 707),
            "B3": (353, 500),
            "B4": (250, 353),
            "B5": (176, 250),
            "B6": (125, 176),
            "B7": (88, 125),
            "B8": (62, 88),
            "B9": (44, 62),
            "B10": (31, 44),
        }
        
        # –î–æ–ø—É—Å—Ç–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –º–º
        tolerance = 5
        
        for format_name, (std_width, std_height) in formats.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ–±–µ–∏—Ö –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è—Ö
            if (abs(width_mm - std_width) <= tolerance and abs(height_mm - std_height) <= tolerance) or \
               (abs(width_mm - std_height) <= tolerance and abs(height_mm - std_width) <= tolerance):
                return format_name
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π
        return f"custom_{int(width_mm)}x{int(height_mm)}"

    def calculate_a4_sheets(self, width_mm: float, height_mm: float) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Å—Ç–æ–≤ –ê4 (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º)"""
        try:
            # –ü–ª–æ—â–∞–¥—å –ê4 –≤ –º–º¬≤
            a4_area = 210 * 297  # 62,370 –º–º¬≤
            
            # –ü–ª–æ—â–∞–¥—å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–º¬≤
            page_area = width_mm * height_mm
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
            a4_sheets = page_area / a4_area
            
            # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 2 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            return round(a4_sheets, 2)
            
        except Exception as e:
            logger.error(f"Error calculating A4 sheets: {e}")
            return 0.0

    def detect_page_type(self, page) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = page.extract_text()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –ª–µ–≥–∫–æ –∏ –µ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π
            if text and len(text.strip()) > 50:
                return "vector"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if '/XObject' in page['/Resources']:
                xObject = page['/Resources']['/XObject'].get_object()
                for obj in xObject:
                    if xObject[obj]['/Subtype'] == '/Image':
                        return "scanned"
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å—á–∏—Ç–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π
            if not text or len(text.strip()) < 10:
                return "scanned"
            
            return "vector"
            
        except Exception as e:
            logger.warning(f"Error detecting page type: {e}")
            return "unknown"

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º cl100k_base –∫–æ–¥–∏—Ä–æ–≤–∫—É (GPT-4, GPT-3.5-turbo)
            encoding = tiktoken.get_encoding("cl100k_base")
            tokens = encoding.encode(text)
            return len(tokens)
        except Exception as e:
            logger.error(f"Token counting error: {e}")
            # Fallback: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –ø–æ —Å–ª–æ–≤–∞–º
            return len(text.split())

    def calculate_document_tokens(self, elements: List[Dict[str, Any]]) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        total_tokens = 0
        for element in elements:
            content = element.get("element_content", "")
            if content:
                total_tokens += self.count_tokens(content)
        return total_tokens
    
    def detect_file_type(self, file_content: bytes) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        print(f"üîç [DEBUG] DocumentParser: detect_file_type called with content length: {len(file_content)}")
        print(f"üîç [DEBUG] DocumentParser: Content preview: {file_content[:100]}...")
        
        mime_type = magic.from_buffer(file_content, mime=True)
        print(f"üîç [DEBUG] DocumentParser: Magic detected MIME type: {mime_type}")
        
        if mime_type == "application/pdf":
            print(f"üîç [DEBUG] DocumentParser: Detected as PDF")
            return "pdf"
        else:
            print(f"üîç [DEBUG] DocumentParser: Unsupported MIME type: {mime_type}")
            raise ValueError(f"Only PDF files are supported. Detected MIME type: {mime_type}")

    def parse_file(self, file_content: bytes, file_type: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
        if file_type == "pdf":
            return self.parse_pdf(file_content)
        elif file_type == "docx":
            return self.parse_docx(file_content)
        elif file_type == "dwg":
            return self.parse_dwg(file_content)
        elif file_type == "txt":
            return self.parse_text(file_content)
        else:
            raise ValueError(f"Unsupported file type for parsing: {file_type}")

    def parse_text(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        elements = []
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            text_content = file_content.decode('utf-8')
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            lines = text_content.split('\n')
            
            for i, line in enumerate(lines, 1):
                line = line.strip()
                if line:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    elements.append({
                        "element_type": "text",
                        "element_content": line,
                        "page_number": 1,
                        "confidence_score": 1.0
                    })
            
            return elements
            
        except Exception as e:
            logger.error(f"Text parsing error: {e}")
            raise
    
    def parse_pdf(self, file_content: bytes) -> DocumentInspectionResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        logger.info(f"üîç [DEBUG] DocumentParser: Starting PDF parsing for {len(file_content)} bytes")
        log_memory_usage("before PDF parsing")
        result = DocumentInspectionResult()
        document_format_stats = {
            "total_pages": 0,
            "total_a4_sheets": 0.0,
            "formats": {},
            "orientations": {"portrait": 0, "landscape": 0},
            "page_types": {"vector": 0, "scanned": 0, "unknown": 0}
        }
        
        try:
            logger.info(f"üîç [DEBUG] DocumentParser: Creating PDF reader from {len(file_content)} bytes")
            # –°–æ–∑–¥–∞–µ–º file-like –æ–±—ä–µ–∫—Ç –∏–∑ bytes
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            logger.info(f"üîç [DEBUG] DocumentParser: PDF reader created successfully")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            document_info = self.get_document_info(pdf_reader)
            result.document_name = document_info["name"]
            result.document_type = document_info["type"]
            result.document_engineering_stage = document_info["engineering_stage"]
            result.document_mark = document_info["mark"]
            result.document_number = document_info["number"]
            result.document_date = document_info["date"]
            result.document_author = document_info["author"]
            result.document_reviewer = document_info["reviewer"]
            result.document_approver = document_info["approver"]
            result.document_status = document_info["status"]
            result.document_size = document_info["size"]
            # –ø–æ–∫–∞–∂–µ–º –≤ –ª–æ–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            logger.info(f"Document info: {document_info}")
            
            
            document_format_stats["total_pages"] = len(pdf_reader.pages)
            logger.info(f"Processing PDF with {len(pdf_reader.pages)} pages")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR
            try:
                images = convert_from_bytes(file_content, dpi=300)
                logger.info(f"Converted PDF to {len(images)} images")
            except Exception as e:
                logger.warning(f"Failed to convert PDF to images: {e}")
                images = []
            
            logger.info(f"üîç [DEBUG] DocumentParser: Starting to process {len(pdf_reader.pages)} pages")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            logger.info(f"üîç [DEBUG] DocumentParser: PDF document parameters:")
            logger.info(f"üîç [DEBUG] DocumentParser: - Total pages: {len(pdf_reader.pages)}")
            logger.info(f"üîç [DEBUG] DocumentParser: - File size: {len(file_content)} bytes")
            logger.info(f"üîç [DEBUG] DocumentParser: - Average page size: {len(file_content) // len(pdf_reader.pages)} bytes")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            BATCH_SIZE = 5  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 5 —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑
            total_batches = (len(pdf_reader.pages) + BATCH_SIZE - 1) // BATCH_SIZE
            logger.info(f"üîç [DEBUG] DocumentParser: Batch processing settings:")
            logger.info(f"üîç [DEBUG] DocumentParser: - Batch size: {BATCH_SIZE} pages")
            logger.info(f"üîç [DEBUG] DocumentParser: - Total batches: {total_batches}")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –±–∞—Ç—á–∞–º–∏
            for batch_num in range(total_batches):
                start_page = batch_num * BATCH_SIZE
                end_page = min((batch_num + 1) * BATCH_SIZE, len(pdf_reader.pages))
                
                logger.info(f"üîç [DEBUG] DocumentParser: Starting batch {batch_num + 1}/{total_batches} (pages {start_page + 1}-{end_page})")
                log_memory_usage(f"before batch {batch_num + 1}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ
                for page_num in range(start_page, end_page):
                    page = pdf_reader.pages[page_num]
                    page_number = page_num + 1
                    
                    logger.info(f"üîç [DEBUG] DocumentParser: Processing page {page_number}/{len(pdf_reader.pages)} in batch {batch_num + 1}")
                    
                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result = DocumentPageInspectionResult()
                    page_result.page_number = page_number
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    try:
                        page_text = page.extract_text()
                        page_size = len(page_text) if page_text else 0
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number} parameters:")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Text size: {page_size} characters")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Has text: {bool(page_text)}")
                        logger.info(f"üîç [DEBUG] DocumentParser: - Text preview: {page_text[:100] if page_text else 'No text'}...")
                    except Exception as e:
                        logger.warning(f"üîç [DEBUG] DocumentParser: Failed to extract text from page {page_number}: {e}")
                        page_size = 0
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    format_info = self.get_page_format_info(page)
                    document_format_stats["total_a4_sheets"] += format_info["a4_sheets"]
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result.page_format = format_info["format_name"]
                    page_result.page_width_mm = format_info["width_mm"]
                    page_result.page_height_mm = format_info["height_mm"]
                    page_result.page_orientation = format_info["orientation"]
                    page_result.page_a4_sheets_equivalent = format_info["a4_sheets"]
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–æ—Ä–º–∞—Ç–æ–≤
                    format_name = format_info["format_name"]
                    if format_name not in document_format_stats["formats"]:
                        document_format_stats["formats"][format_name] = 0
                    document_format_stats["formats"][format_name] += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–π
                    orientation = format_info["orientation"]
                    document_format_stats["orientations"][orientation] += 1
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    logger.info(f"Page {page_number} format: {format_name} "
                              f"({format_info['width_mm']}x{format_info['height_mm']} mm, "
                              f"{orientation}, {format_info['a4_sheets']} A4 sheets)")
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_type = self.detect_page_type(page)
                    page_result.page_type = page_type
                    document_format_stats["page_types"][page_type] += 1
                    logger.info(f"Page {page_number} type: {page_type}")
                    
                    if page_type == "vector":
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        text = page.extract_text()
                        if text.strip():
                            page_result.page_text = text
                            page_result.page_text_confidence = 0.9
                            page_result.page_text_method = "direct_extraction"
                            page_result.page_text_length = len(text)
                            logger.info(f"Page {page_number}: Extracted {len(text)} characters (vector)")
                        else:
                            logger.warning(f"Page {page_number}: No text extracted from vector page")
                            page_result.page_text = ""
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "failed"
                            page_result.page_text_length = 0
                    
                    elif page_type == "scanned" and page_num < len(images):
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å OCR
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: Processing as scanned page with OCR")
                    
                        try:
                            logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: Starting OCR processing")
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR
                            ocr_result = self.extract_text_from_image(images[page_num], page_number)
                            logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: OCR processing completed")
                            
                            if ocr_result["text"]:
                                page_result.page_text = ocr_result["text"]
                                page_result.page_text_confidence = ocr_result["confidence"]
                                page_result.page_text_method = f"ocr_{ocr_result['method']}"
                                page_result.page_text_length = len(ocr_result["text"])
                                page_result.page_ocr_confidence = ocr_result["confidence"]
                                page_result.page_ocr_method = ocr_result["method"]
                                page_result.page_ocr_all_results = ocr_result.get("all_results", [])
                                logger.info(f"Page {page_number}: OCR extracted {len(ocr_result['text'])} characters "
                                          f"(confidence: {ocr_result['confidence']})")
                            else:
                                logger.warning(f"Page {page_number}: OCR failed to extract text")
                                page_result.page_text = f"[OCR –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}]"
                                page_result.page_text_confidence = 0.1
                                page_result.page_text_method = "ocr_failed"
                                page_result.page_text_length = 0
                        
                        except Exception as e:
                            logger.error(f"OCR processing failed for page {page_number}: {e}")
                            page_result.page_text = f"[–û—à–∏–±–∫–∞ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {str(e)}]"
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "ocr_error"
                            page_result.page_text_length = 0
                    
                    else:
                        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        logger.warning(f"Page {page_number}: Unknown type or no image available")
                        text = page.extract_text()
                        if text.strip():
                            page_result.page_text = text
                            page_result.page_text_confidence = 0.5
                            page_result.page_text_method = "fallback_extraction"
                            page_result.page_text_length = len(text)
                        else:
                            page_result.page_text = f"[–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}]"
                            page_result.page_text_confidence = 0.0
                            page_result.page_text_method = "failed"
                            page_result.page_text_length = 0
                
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    logger.info(f"üîç [DEBUG] DocumentParser: Adding page {page_result.page_number} to results. Total results before: {len(result.document_pages_results)}")
                    result.document_pages_results.append(page_result)
                    logger.info(f"üîç [DEBUG] DocumentParser: Added page {page_result.page_number} to results. Total results after: {len(result.document_pages_results)}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                    if page_result.page_text and len(page_result.page_text.strip()) > 0:
                        logger.info(f"üîç [DEBUG] DocumentParser: Page {page_result.page_number} has content ({len(page_result.page_text)} chars)")
                    else:
                        logger.warning(f"üîç [DEBUG] DocumentParser: Page {page_result.page_number} has no content!")
                    
                    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —à—Ç–∞–º–ø–æ–≤
                    # —Å –ø–æ–º–æ—â—å—é OpenCV –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    
                    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    log_memory_usage(f"processing page {page_number}")
                    
                    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –±–∞—Ç—á–µ
                    if (page_num - start_page) % 3 == 0:
                        cleanup_memory()
                
                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞
                logger.info(f"üîç [DEBUG] DocumentParser: Completed batch {batch_num + 1}/{total_batches}")
                log_memory_usage(f"after batch {batch_num + 1}")
                cleanup_memory()
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            result.document_pages = document_format_stats['total_pages']
            result.document_pages_total = document_format_stats['total_pages']
            result.document_pages_total_a4_sheets_equivalent = document_format_stats['total_a4_sheets']
            result.document_pages_vector = document_format_stats['page_types'].get('vector', 0)
            result.document_pages_scanned = document_format_stats['page_types'].get('scanned', 0)
            result.document_pages_unknown = document_format_stats['page_types'].get('unknown', 0)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info("=== PDF DOCUMENT FORMAT STATISTICS ===")
            logger.info(f"Total pages: {document_format_stats['total_pages']}")
            logger.info(f"Total A4 sheets equivalent: {document_format_stats['total_a4_sheets']:.2f}")
            logger.info(f"Page formats: {document_format_stats['formats']}")
            logger.info(f"Orientations: {document_format_stats['orientations']}")
            logger.info(f"Page types: {document_format_stats['page_types']}")
            logger.info("=====================================")
                
        except Exception as e:
            logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing error: {e}")
            import traceback
            logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing traceback: {traceback.format_exc()}")
            raise
        
        log_memory_usage("after PDF parsing")
        cleanup_memory()
        logger.info(f"üîç [DEBUG] DocumentParser: PDF parsing completed successfully. Total pages: {len(result.document_pages_results)}")
        return result
    
    def parse_docx(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ DOCX –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        elements = []
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_path = "/app/temp/temp.docx"
            with open(temp_path, "wb") as f:
                f.write(file_content)
            
            doc = Document(temp_path)
            
            for para in doc.paragraphs:
                if para.text.strip():
                    elements.append({
                        "element_type": "text",
                        "element_content": para.text,
                        "page_number": 1,  # DOCX –Ω–µ –∏–º–µ–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü
                        "confidence_score": 0.95
                    })
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                table_data = []
                for row in table.rows:
                    row_data = [cell.text for cell in row.cells]
                    table_data.append(row_data)
                
                if table_data:
                    elements.append({
                        "element_type": "table",
                        "element_content": json.dumps(table_data),
                        "page_number": 1,
                        "confidence_score": 0.9
                    })
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_path)
            
        except Exception as e:
            logger.error(f"DOCX parsing error: {e}")
            raise
        
        return elements
    
    def parse_dwg(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ DWG –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        elements = []
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DWG —Ñ–∞–π–ª–∞
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç ezdxf
            elements.append({
                "element_type": "text",
                "element_content": "DWG —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω (–ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)",
                "page_number": 1,
                "confidence_score": 0.5
            })
            
        except Exception as e:
            logger.error(f"DWG parsing error: {e}")
            raise
        
        return elements
    
    def parse_ifc(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ IFC –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        elements = []
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            text_content = file_content.decode('utf-8', errors='ignore')
            
            # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            entity_counts = {}
            project_info = None
            buildings = []
            storeys = []
            walls = []
            windows = []
            doors = []
            materials = []
            property_sets = []
            
            # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ IFC —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            entity_pattern = r'#(\d+)\s*=\s*([A-Z_]+)\s*\('
            name_pattern = r'Name\s*=\s*[\'"]([^\'"]*)[\'"]'
            description_pattern = r'Description\s*=\s*[\'"]([^\'"]*)[\'"]'
            object_type_pattern = r'ObjectType\s*=\s*[\'"]([^\'"]*)[\'"]'
            elevation_pattern = r'Elevation\s*=\s*([-\d.]+)'
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ IFC —Å—É—â–Ω–æ—Å—Ç–µ–π
            entity_matches = re.finditer(entity_pattern, text_content)
            
            for entity_match in entity_matches:
                entity_id = entity_match.group(1)
                entity_type = entity_match.group(2)
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –∑–∞–ø–∏—Å–∏ (–∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É)
                start_pos = entity_match.end()
                bracket_count = 0
                end_pos = start_pos
                
                for i, char in enumerate(text_content[start_pos:], start_pos):
                    if char == '(':
                        bracket_count += 1
                    elif char == ')':
                        bracket_count -= 1
                        if bracket_count < 0:
                            end_pos = i + 1
                            break
                
                entity_params = text_content[start_pos:end_pos]
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
                if entity_type == 'IFCPROJECT':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    project_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    project_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    project_info = f"–ü—Ä–æ–µ–∫—Ç: {project_name} - {project_desc}"
                    
                elif entity_type == 'IFCBUILDING':
                    name_match = re.search(name_pattern, entity_params)
                    building_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    buildings.append(f"–ó–¥–∞–Ω–∏–µ: {building_name}")
                    
                elif entity_type == 'IFCBUILDINGSTOREY':
                    name_match = re.search(name_pattern, entity_params)
                    elevation_match = re.search(elevation_pattern, entity_params)
                    storey_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    elevation = elevation_match.group(1) if elevation_match else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'
                    storeys.append(f"–≠—Ç–∞–∂: {storey_name} - –í—ã—Å–æ—Ç–∞: {elevation}")
                    
                elif entity_type == 'IFCWALL':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    wall_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    wall_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    walls.append(f"–°—Ç–µ–Ω–∞: {wall_name} - –¢–∏–ø: {wall_type}")
                    
                elif entity_type == 'IFCWINDOW':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    window_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    window_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    windows.append(f"–û–∫–Ω–æ: {window_name} - –¢–∏–ø: {window_type}")
                    
                elif entity_type == 'IFCDOOR':
                    name_match = re.search(name_pattern, entity_params)
                    type_match = re.search(object_type_pattern, entity_params)
                    door_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    door_type = type_match.group(1) if type_match else '–ù–µ —É–∫–∞–∑–∞–Ω'
                    doors.append(f"–î–≤–µ—Ä—å: {door_name} - –¢–∏–ø: {door_type}")
                    
                elif entity_type == 'IFCMATERIAL':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    material_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    material_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    materials.append(f"–ú–∞—Ç–µ—Ä–∏–∞–ª: {material_name} - {material_desc}")
                        
                elif entity_type == 'IFCPROPERTYSET':
                    name_match = re.search(name_pattern, entity_params)
                    desc_match = re.search(description_pattern, entity_params)
                    pset_name = name_match.group(1) if name_match else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    pset_desc = desc_match.group(1) if desc_match else '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'
                    property_sets.append(f"–ù–∞–±–æ—Ä —Å–≤–æ–π—Å—Ç–≤: {pset_name} - {pset_desc}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —ç–ª–µ–º–µ–Ω—Ç—ã
            if project_info:
                elements.append({
                    "element_type": "project_info",
                    "element_content": project_info,
                    "page_number": 1,
                    "confidence_score": 0.95
                })
            
            for building in buildings[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                elements.append({
                    "element_type": "building",
                    "element_content": building,
                    "page_number": 1,
                    "confidence_score": 0.9
                })
            
            for storey in storeys[:10]:
                elements.append({
                    "element_type": "storey",
                    "element_content": storey,
                    "page_number": 1,
                    "confidence_score": 0.9
                })
            
            for wall in walls[:20]:
                elements.append({
                    "element_type": "wall",
                    "element_content": wall,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for window in windows[:20]:
                elements.append({
                    "element_type": "window",
                    "element_content": window,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for door in doors[:20]:
                elements.append({
                    "element_type": "door",
                    "element_content": door,
                    "page_number": 1,
                    "confidence_score": 0.85
                })
            
            for material in materials[:10]:
                elements.append({
                    "element_type": "material",
                    "element_content": material,
                    "page_number": 1,
                    "confidence_score": 0.8
                })
            
            for pset in property_sets[:10]:
                elements.append({
                    "element_type": "property_set",
                    "element_content": pset,
                    "page_number": 1,
                    "confidence_score": 0.8
                })
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
            total_entities = sum(entity_counts.values())
            top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            stats_text = f"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {total_entities}. –¢–æ–ø —Ç–∏–ø–æ–≤: " + ", ".join([f"{entity}({count})" for entity, count in top_entities])
            
            elements.append({
                "element_type": "statistics",
                "element_content": stats_text,
                "page_number": 1,
                "confidence_score": 0.95
            })
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if len(elements.document_pages_results) <= 1:  # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                elements.append({
                    "element_type": "text",
                    "element_content": f"IFC —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {total_entities} —Å—É—â–Ω–æ—Å—Ç–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤",
                    "page_number": 1,
                    "confidence_score": 0.7
                })
            
        except Exception as e:
            logger.error(f"IFC parsing error: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É
            try:
                text_content = file_content.decode('utf-8', errors='ignore')
                elements.append({
                    "element_type": "text",
                    "element_content": f"IFC —Ñ–∞–π–ª (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è): {text_content[:500]}",
                    "page_number": 1,
                    "confidence_score": 0.3
                })
            except:
                elements.append({
                    "element_type": "error",
                    "element_content": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ IFC —Ñ–∞–π–ª–∞: {str(e)}",
                    "page_number": 1,
                    "confidence_score": 0.1
                })
        
        return elements
    
    def save_to_database(self, filename: str, original_filename: str, file_type: str,
                         file_size: int, document_hash: str, inspection_result: DocumentInspectionResult,
                         category: str = "other", document_type: str = "normative") -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîç [DEBUG] DocumentParser: save_to_database called")
        logger.info(f"üîç [DEBUG] DocumentParser: document_type: {document_type}")
        logger.info(f"üîç [DEBUG] DocumentParser: pages to save: {len(inspection_result.document_pages_results)}")
        
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                if document_type == "checkable":
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    logger.info(f"üîç [DEBUG] DocumentParser: Saving as checkable document...")
                    review_deadline = datetime.now() + timedelta(days=2)
                    logger.info(f"üîç [DEBUG] DocumentParser: Review deadline: {review_deadline}")
                    
                    cursor.execute("""
                        INSERT INTO checkable_documents 
                        (filename, original_filename, file_type, file_size, document_hash, 
                         processing_status, category, review_deadline)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (filename, original_filename, file_type, file_size, document_hash, 
                          "completed", category, review_deadline))
                    document_id = cursor.fetchone()["id"]
                    logger.info(f"üîç [DEBUG] DocumentParser: Checkable document saved with ID: {document_id}")
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                    token_count = self.calculate_document_tokens(inspection_result)
                    
                    cursor.execute("""
                        INSERT INTO uploaded_documents 
                        (filename, original_filename, file_type, file_size, document_hash, 
                         processing_status, category, token_count)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (filename, original_filename, file_type, file_size, document_hash, 
                          "completed", category, token_count))
                    document_id = cursor.fetchone()["id"]
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"üîç [DEBUG] DocumentParser: Starting to save {len(inspection_result.document_pages_results)} elements...")
                for i, page_result in enumerate(inspection_result.document_pages_results):
                    logger.debug(f"üîç [DEBUG] DocumentParser: Saving element {i+1}/{len(inspection_result.document_pages_results)}, page {page_result.page_number}")
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    metadata = {
                        "page_type": page_result.page_type,
                        "processing_method": page_result.page_text_method,
                        "format_info": {
                            "format_name": page_result.page_format,
                            "width_mm": page_result.page_width_mm,
                            "height_mm": page_result.page_height_mm,
                            "orientation": page_result.page_orientation,
                            "a4_sheets": page_result.page_a4_sheets_equivalent
                        }
                    }
                    
                    if page_result.page_ocr_confidence is not None:
                        metadata["ocr_confidence"] = page_result.page_ocr_confidence
                    if page_result.page_ocr_method is not None:
                        metadata["ocr_method"] = page_result.page_ocr_method
                    
                    if document_type == "checkable":
                        logger.debug(f"üîç [DEBUG] DocumentParser: Inserting into checkable_elements, text length: {len(page_result.page_text)}")
                        cursor.execute("""
                            INSERT INTO checkable_elements 
                            (checkable_document_id, element_type, element_content, page_number, confidence_score, element_metadata)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            document_id,
                            "text",
                            page_result.page_text,
                            page_result.page_number,
                            page_result.page_text_confidence,
                            json.dumps(metadata)
                        ))
                    else:
                        logger.debug(f"üîç [DEBUG] DocumentParser: Inserting into extracted_elements, text length: {len(page_result.page_text)}")
                        cursor.execute("""
                            INSERT INTO extracted_elements 
                            (uploaded_document_id, element_type, element_content, page_number, confidence_score, metadata)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            document_id,
                            "text",
                            page_result.page_text,
                            page_result.page_number,
                            page_result.page_text_confidence,
                            json.dumps(metadata)
                        ))
                
                self.db_conn.commit()
                logger.info(f"üîç [DEBUG] DocumentParser: Database commit successful")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if document_type == "checkable":
                    logger.info(f"üîç [DEBUG] DocumentParser: Starting automatic norm control check...")
                    try:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                        document_content = "\n\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
                        logger.debug(f"üîç [DEBUG] DocumentParser: Document content length for norm control: {len(document_content)} characters")
                        
                        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
                        asyncio.create_task(self.perform_norm_control_check(document_id, document_content))
                        logger.info(f"üîç [DEBUG] DocumentParser: Started automatic norm control check for document {document_id}")
                    except Exception as e:
                        logger.error(f"üîç [DEBUG] DocumentParser: Failed to start norm control check for document {document_id}: {e}")
                
                logger.info(f"üîç [DEBUG] DocumentParser: save_to_database completed successfully, returning document_id: {document_id}")
                return document_id
                
        except Exception as e:
            self.db_conn.rollback()
            logger.error(f"Database save error: {e}")
            raise
    
    def create_initial_document_record(self, filename: str, file_type: str, file_size: int, document_hash: str, file_path: str, category: str = "other") -> int:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        def _create_record(conn):
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    INSERT INTO uploaded_documents 
                    (filename, original_filename, file_type, file_size, document_hash, processing_status, file_path, category)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (filename, filename, file_type, file_size, document_hash, "uploaded", file_path, category))
                
                document_id = cursor.fetchone()["id"]
                logger.info(f"Created initial document record with ID: {document_id}")
                return document_id
        
        try:
            return self.execute_in_transaction(_create_record)
        except Exception as e:
            logger.error(f"Error creating initial document record: {e}")
            raise
    
    def update_document_status(self, document_id: int, status: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s
                    WHERE id = %s
                """, (status, document_id))
                logger.info(f"Updated document {document_id} status to: {status}")
        
        try:
            self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Error updating document status: {e}")
            raise
    
    def update_checkable_document_status(self, document_id: int, status: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE checkable_documents 
                    SET processing_status = %s
                    WHERE id = %s
                """, (status, document_id))
                logger.info(f"Updated checkable document {document_id} status to: {status}")
        
        try:
            self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Error updating checkable document status: {e}")
            raise
    
    def save_elements_to_database(self, document_id: int, inspection_result: DocumentInspectionResult):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        def _save_elements(conn):
            with conn.cursor() as cursor:
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                logger.info(f"Cleared old elements for document {document_id}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"Saving new document information {document_id}")
                logger.info(f"Saving new document information {inspection_result.document_pages_results}")
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                logger.info(f"üîç [DEBUG] DocumentParser: Starting to save {len(inspection_result.document_pages_results)} page results")
                for i, page_result in enumerate(inspection_result.document_pages_results):
                    logger.info(f"üîç [DEBUG] DocumentParser: Saving page {page_result.page_number} (index {i}), text length: {len(page_result.page_text or '')}")
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    metadata = {
                        "page_type": page_result.page_type,
                        "processing_method": page_result.page_text_method,
                        "format_info": {
                            "format_name": page_result.page_format,
                            "width_mm": page_result.page_width_mm,
                            "height_mm": page_result.page_height_mm,
                            "orientation": page_result.page_orientation,
                            "a4_sheets": page_result.page_a4_sheets_equivalent
                        }
                    }
                    
                    if page_result.page_ocr_confidence is not None:
                        metadata["ocr_confidence"] = page_result.page_ocr_confidence
                    if page_result.page_ocr_method is not None:
                        metadata["ocr_method"] = page_result.page_ocr_method
                    
                    cursor.execute("""
                        INSERT INTO extracted_elements 
                        (uploaded_document_id, element_type, element_content, page_number, confidence_score, metadata)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        document_id,
                        "text",
                        page_result.page_text,
                        page_result.page_number,
                        page_result.page_text_confidence,
                        json.dumps(metadata)
                    ))
                
                logger.info(f"Saved {len(inspection_result.document_pages_results)} elements for document {document_id}")
        
        try:
            self.execute_in_transaction(_save_elements)
        except Exception as e:
            logger.error(f"Error saving elements to database: {e}")
            raise
    
    def update_document_completion(self, document_id: int, elements_count: int, token_count: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        def _update_completion(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s, token_count = %s
                    WHERE id = %s
                """, ("completed", token_count, document_id))
                logger.info(f"Updated document {document_id} completion: {elements_count} elements, {token_count} tokens")
        
        try:
            self.execute_in_transaction(_update_completion)
        except Exception as e:
            logger.error(f"Error updating document completion: {e}")
            raise
    
    async def index_to_rag_service_async(self, document_id: int, document_title: str, inspection_result: DocumentInspectionResult):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ RAG-—Å–µ—Ä–≤–∏—Å"""
        try:
            logger.info(f"Starting async RAG indexing for document {document_id}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            elements = []
            for page_result in inspection_result.document_pages_results:
                elements.append({
                    "element_type": "text",
                    "element_content": page_result.page_text,
                    "page_number": page_result.page_number,
                    "confidence_score": page_result.page_text_confidence
                })
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            combined_text = ""
            for page_result in inspection_result.document_pages_results:
                if page_result.page_text:
                    combined_text += page_result.page_text + "\n\n"
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ RAG-—Å–µ—Ä–≤–∏—Å
            async with httpx.AsyncClient(timeout=300.0) as client:
                response = await client.post(
                    f"{RAG_SERVICE_URL}/index",
                    data={
                        "document_id": document_id,
                        "document_title": document_title,
                        "content": combined_text,
                        "chapter": "",
                        "section": "",
                        "page_number": 1
                    }
                )
                
                if response.status_code == 200:
                    logger.info(f"Successfully indexed document {document_id} to RAG service")
                else:
                    logger.error(f"Failed to index document {document_id} to RAG service: {response.status_code}")
                    
        except Exception as e:
            logger.error(f"Error in async RAG indexing for document {document_id}: {e}")
    
    def calculate_document_tokens(self, inspection_result: DocumentInspectionResult) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            total_tokens = 0
            encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
            
            for page_result in inspection_result.document_pages_results:
                if page_result.page_text:
                    tokens = encoding.encode(page_result.page_text)
                    total_tokens += len(tokens)
            
            logger.info(f"Calculated {total_tokens} tokens for document")
            return total_tokens
            
        except Exception as e:
            logger.error(f"Error calculating document tokens: {e}")
            return 0
            logger.error(f"Database save error: {e}")
            raise

    def save_checkable_document(self, filename: str, original_filename: str, file_type: str, 
                               file_size: int, document_hash: str, inspection_result: DocumentInspectionResult, 
                               category: str = "other") -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üîç [DEBUG] DocumentParser: save_checkable_document called")
        logger.info(f"üîç [DEBUG] DocumentParser: filename: {filename}")
        logger.info(f"üîç [DEBUG] DocumentParser: original_filename: {original_filename}")
        logger.info(f"üîç [DEBUG] DocumentParser: file_type: {file_type}")
        logger.info(f"üîç [DEBUG] DocumentParser: file_size: {file_size}")
        logger.info(f"üîç [DEBUG] DocumentParser: document_hash: {document_hash}")
        logger.info(f"üîç [DEBUG] DocumentParser: category: {category}")
        logger.info(f"üîç [DEBUG] DocumentParser: inspection_result pages: {len(inspection_result.document_pages_results)}")
        
        document_id = self.save_to_database(filename, original_filename, file_type, file_size, 
                                   document_hash, inspection_result, category, "checkable")
        
        logger.info(f"üîç [DEBUG] DocumentParser: save_checkable_document completed, document_id: {document_id}")
        return document_id

    def cleanup_expired_documents(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        def _cleanup(conn):
            with conn.cursor() as cursor:
                cursor.execute("SELECT cleanup_expired_checkable_documents()")
                result = cursor.fetchone()
                return result[0] if result else 0
        
        try:
            return self.execute_in_transaction(_cleanup)
        except Exception as e:
            logger.error(f"Cleanup error: {e}")
            return 0

    def get_checkable_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        def _get_documents(conn):
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, original_filename, file_type, file_size, upload_date, 
                           processing_status, category, review_deadline, review_status, 
                           assigned_reviewer
                    FROM checkable_documents 
                    ORDER BY upload_date DESC
                """)
                documents = cursor.fetchall()
                return [dict(doc) for doc in documents]
        
        try:
            return self.execute_in_transaction(_get_documents)
        except Exception as e:
            logger.error(f"Get checkable documents error: {e}")
            return []

    def get_checkable_document(self, document_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, original_filename, file_type, file_size, upload_date, 
                           processing_status, category, review_deadline, review_status, 
                           assigned_reviewer
                    FROM checkable_documents 
                    WHERE id = %s
                """, (document_id,))
                document = cursor.fetchone()
                return dict(document) if document else None
        except Exception as e:
            logger.error(f"Get checkable document error: {e}")
            return None

    def get_norm_control_result_by_document_id(self, document_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –ø–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, analysis_status, total_findings, critical_findings, warning_findings, 
                           info_findings, analysis_date
                    FROM norm_control_results
                    WHERE checkable_document_id = %s
                    ORDER BY analysis_date DESC
                    LIMIT 1
                """, (document_id,))
                result = cursor.fetchone()
                return dict(result) if result else None
        except Exception as e:
            logger.error(f"Get norm control result error: {e}")
            return None

    def get_page_results_by_document_id(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT 
                        f.id,
                        f.finding_type,
                        f.severity_level,
                        f.category,
                        f.title,
                        f.description,
                        f.recommendation,
                        f.confidence_score,
                        f.created_at
                    FROM findings f
                    JOIN norm_control_results ncr ON f.norm_control_result_id = ncr.id
                    WHERE ncr.checkable_document_id = %s
                    ORDER BY f.severity_level DESC, f.created_at
                """, (document_id,))
                results = cursor.fetchall()
                return [dict(result) for result in results]
        except Exception as e:
            logger.error(f"Get page results error: {e}")
            return []

    def get_review_report_by_norm_control_id(self, norm_control_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ –ø–æ ID —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT id, reviewer_name, conclusion, report_date
                    FROM review_reports
                    WHERE norm_control_result_id = %s
                    ORDER BY report_date DESC
                    LIMIT 1
                """, (norm_control_id,))
                result = cursor.fetchone()
                return dict(result) if result else None
        except Exception as e:
            logger.error(f"Get review report error: {e}")
            return None

    def get_document_info(self, pdf_reader) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ PDF –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            info = pdf_reader.metadata
            if info:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF
                title = info.get('/Title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
                author = info.get('/Author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')
                subject = info.get('/Subject', '')
                creator = info.get('/Creator', '')
                producer = info.get('/Producer', '')
                creation_date = info.get('/CreationDate', '')
                mod_date = info.get('/ModDate', '')
                
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                document_number = ""
                if title:
                    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "A9.5.MTH.04" –∏–ª–∏ "–ö–ñ-01" –∏ —Ç.–¥.
                    import re
                    number_match = re.search(r'[A-Z–ê-–Ø]{1,3}[-\d\.]+[A-Z–ê-–Ø]*\d*', title)
                    if number_match:
                        document_number = number_match.group(0)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                document_type = "pdf"
                engineering_stage = "–ü–î"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                document_mark = ""
                document_status = "IFR"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                
                # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞—Ä–∫—É –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                if title:
                    title_upper = title.upper()
                    if any(mark in title_upper for mark in ['–ö–ñ', 'KZH']):
                        document_mark = "–ö–ñ"
                    elif any(mark in title_upper for mark in ['–ö–ú', 'KM']):
                        document_mark = "–ö–ú"
                    elif any(mark in title_upper for mark in ['–ê–°', 'AS']):
                        document_mark = "–ê–°"
                    elif any(mark in title_upper for mark in ['–¢–•', 'TX']):
                        document_mark = "–¢–•"
                    elif any(mark in title_upper for mark in ['–ö–°', 'KS']):
                        document_mark = "–ö–°"
                    elif any(mark in title_upper for mark in ['–ö–ü', 'KP']):
                        document_mark = "–ö–ü"
                    elif any(mark in title_upper for mark in ['–ö–†', 'KR']):
                        document_mark = "–ö–†"
                
                return {
                    "name": title,
                    "type": document_type,
                    "engineering_stage": engineering_stage,
                    "mark": document_mark,
                    "number": document_number,
                    "date": creation_date or mod_date,
                    "author": author,
                    "reviewer": "",  # –ù–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ PDF
                    "approver": "",  # –ù–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ PDF
                    "status": document_status,
                    "size": 0  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                }
            else:
                # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                return {
                    "name": "–î–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
                    "type": "pdf",
                    "engineering_stage": "–ü–î",
                    "mark": "",
                    "number": "",
                    "date": "",
                    "author": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä",
                    "reviewer": "",
                    "approver": "",
                    "status": "IFR",
                    "size": 0
                }
        except Exception as e:
            logger.error(f"Error extracting document info: {e}")
            return {
                "name": "–î–æ–∫—É–º–µ–Ω—Ç —Å –æ—à–∏–±–∫–æ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö",
                "type": "pdf",
                "engineering_stage": "–ü–î",
                "mark": "",
                "number": "",
                "date": "",
                "author": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä",
                "reviewer": "",
                "approver": "",
                "status": "IFR",
                "size": 0
            }

    def get_document_format_statistics(self, document_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT metadata->>'format_info' as format_info
                    FROM checkable_elements
                    WHERE checkable_document_id = %s
                    ORDER BY page_number
                """, (document_id,))
                elements = cursor.fetchall()
                
                if not elements:
                    return {"error": "Document not found or no elements"}
                
                stats = {
                    "total_pages": len(elements),
                    "total_a4_sheets": 0.0,
                    "formats": {},
                    "orientations": {"portrait": 0, "landscape": 0},
                    "page_types": {"vector": 0, "scanned": 0, "unknown": 0},
                    "pages": []
                }
                
                for element in elements:
                    try:
                        format_info = json.loads(element["format_info"]) if element["format_info"] else {}
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        page_info = {
                            "page_number": format_info.get("page_number", 0),
                            "format": format_info.get("format_name", "unknown"),
                            "width_mm": format_info.get("width_mm", 0),
                            "height_mm": format_info.get("height_mm", 0),
                            "orientation": format_info.get("orientation", "unknown"),
                            "a4_sheets": format_info.get("a4_sheets", 0)
                        }
                        stats["pages"].append(page_info)
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        a4_sheets = format_info.get("a4_sheets", 0)
                        if a4_sheets is not None:
                            stats["total_a4_sheets"] += a4_sheets
                        
                        format_name = format_info.get("format_name", "unknown")
                        if format_name not in stats["formats"]:
                            stats["formats"][format_name] = 0
                        stats["formats"][format_name] += 1
                        
                        orientation = format_info.get("orientation", "unknown")
                        if orientation in stats["orientations"]:
                            stats["orientations"][orientation] += 1
                        
                    except Exception as e:
                        logger.warning(f"Error parsing format info: {e}")
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        stats["pages"].append({
                            "page_number": 0,
                            "format": "unknown",
                            "width_mm": 0,
                            "height_mm": 0,
                            "orientation": "unknown",
                            "a4_sheets": 0
                        })
                        continue
                
                # –û–∫—Ä—É–≥–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –ê4
                stats["total_a4_sheets"] = round(stats["total_a4_sheets"], 2)
                
                return stats
                
        except Exception as e:
            logger.error(f"Get document format statistics error: {e}")
            return {"error": str(e)}

    def determine_document_category(self, filename: str, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        if any(keyword in filename_lower for keyword in ['gost', '–≥–æ—Å—Ç']):
            return 'gost'
        elif any(keyword in filename_lower for keyword in ['sp', '—Å–Ω']):
            return 'sp'
        elif any(keyword in filename_lower for keyword in ['snip', '—Å–Ω–∏–ø']):
            return 'snip'
        elif any(keyword in filename_lower for keyword in ['tr', '—Ç—Ä']):
            return 'tr'
        elif any(keyword in content_lower for keyword in ['–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π', '–∫–æ—Ä–ø', 'corporate']):
            return 'corporate'
        else:
            return 'other'

    def update_review_status(self, document_id: int, status: str, reviewer: str = None, notes: str = None) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        def _update_status(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE checkable_documents 
                    SET review_status = %s, assigned_reviewer = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE id = %s
                """, (status, reviewer, document_id))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_update_status)
        except Exception as e:
            logger.error(f"Update review status error: {e}")
            return False

    async def delete_normative_document(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        def _delete_document(conn):
            with conn.cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                cursor.execute("""
                    SELECT original_filename, document_hash 
                    FROM uploaded_documents 
                    WHERE id = %s
                """, (document_id,))
                doc_info = cursor.fetchone()
                
                if not doc_info:
                    logger.warning(f"Document {document_id} not found")
                    return False, None
                
                # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                elements_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
                cursor.execute("DELETE FROM norm_control_results WHERE uploaded_document_id = %s", (document_id,))
                results_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM uploaded_documents WHERE id = %s", (document_id,))
                document_deleted = cursor.rowcount
                
                logger.info(f"Deleted normative document {document_id} ({doc_info[0]}): "
                          f"{elements_deleted} elements, {results_deleted} results, {document_deleted} document")
                
                return document_deleted > 0, doc_info[0]
        
        try:
            success, filename = self.execute_in_transaction(_delete_document)
            
            if success:
                # –£–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ RAG —Å–µ—Ä–≤–∏—Å–∞
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.delete(f"{RAG_SERVICE_URL}/index-documentes/document/{document_id}")
                        if response.status_code == 200:
                            logger.info(f"Successfully deleted indexes for document {document_id}")
                        else:
                            logger.warning(f"Failed to delete indexes for document {document_id}: {response.status_code}")
                except Exception as e:
                    logger.error(f"Error deleting indexes for document {document_id}: {e}")
            
            return success
                
        except Exception as e:
            logger.error(f"Delete normative document error: {e}")
            return False

    def delete_checkable_document(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        def _delete_document(conn):
            with conn.cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                cursor.execute("""
                    SELECT original_filename, document_hash 
                    FROM checkable_documents 
                    WHERE id = %s
                """, (document_id,))
                doc_info = cursor.fetchone()
                
                if not doc_info:
                    logger.warning(f"Checkable document {document_id} not found")
                    return False
                
                # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("DELETE FROM checkable_elements WHERE checkable_document_id = %s", (document_id,))
                elements_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º –æ—Ç—á–µ—Ç—ã –æ –ø—Ä–æ–≤–µ—Ä–∫–µ
                cursor.execute("DELETE FROM review_reports WHERE checkable_document_id = %s", (document_id,))
                reports_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
                cursor.execute("DELETE FROM norm_control_results WHERE checkable_document_id = %s", (document_id,))
                results_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM checkable_documents WHERE id = %s", (document_id,))
                document_deleted = cursor.rowcount
                
                logger.info(f"Deleted checkable document {document_id} ({doc_info[0]}): "
                          f"{elements_deleted} elements, {reports_deleted} reports, "
                          f"{results_deleted} results, {document_deleted} document")
                
                return document_deleted > 0
        
        try:
            return self.execute_in_transaction(_delete_document)
        except Exception as e:
            logger.error(f"Delete checkable document error: {e}")
            return False
    
    async def index_to_rag_service(self, document_id: int, document_title: str, elements: List[Dict[str, Any]]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG-—Å–µ—Ä–≤–∏—Å"""
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
            content = "\n\n".join([elem["element_content"] for elem in elements])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª–∞–≤—É –∏ —Ä–∞–∑–¥–µ–ª –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            chapter = ""
            section = ""
            for elem in elements:
                if elem["element_type"] in ["project_info", "building", "storey"]:
                    chapter = elem["element_content"][:100]
                    break
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ RAG-—Å–µ—Ä–≤–∏—Å
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{RAG_SERVICE_URL}/index",
                    data={
                        "document_id": document_id,
                        "document_title": document_title,
                        "content": content,
                        "chapter": chapter,
                        "section": section,
                        "page_number": 1
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"Successfully indexed document {document_id} in RAG service: {result}")
                else:
                    logger.error(f"Failed to index document {document_id} in RAG service: {response.text}")
                    
        except Exception as e:
            logger.error(f"RAG indexing error for document {document_id}: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å, –µ—Å–ª–∏ RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å

    def get_system_settings(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT setting_key, setting_value, setting_description, setting_type, is_public, updated_at
                    FROM system_settings
                    WHERE is_public = true
                    ORDER BY setting_key
                """)
                settings = cursor.fetchall()
                return [dict(setting) for setting in settings]
        except Exception as e:
            logger.error(f"Get system settings error: {e}")
            return []

    def get_system_setting(self, setting_key: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            with self.db_conn.cursor() as cursor:
                cursor.execute("""
                    SELECT setting_value
                    FROM system_settings
                    WHERE setting_key = %s AND is_public = true
                """, (setting_key,))
                result = cursor.fetchone()
                return result[0] if result else None
        except Exception as e:
            logger.error(f"Get system setting error: {e}")
            return None

    def update_system_setting(self, setting_key: str, setting_value: str) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _update_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE system_settings
                    SET setting_value = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE setting_key = %s AND is_public = true
                """, (setting_value, setting_key))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_update_setting)
        except Exception as e:
            logger.error(f"Update system setting error: {e}")
            return False

    def create_system_setting(self, setting_key: str, setting_value: str, 
                            setting_description: str, setting_type: str = "text") -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _create_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO system_settings (setting_key, setting_value, setting_description, setting_type)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (setting_key) DO UPDATE SET
                    setting_value = EXCLUDED.setting_value,
                    setting_description = EXCLUDED.setting_description,
                    setting_type = EXCLUDED.setting_type,
                    updated_at = CURRENT_TIMESTAMP
                """, (setting_key, setting_value, setting_description, setting_type))
                return True
        
        try:
            return self.execute_in_transaction(_create_setting)
        except Exception as e:
            logger.error(f"Create system setting error: {e}")
            return False

    def delete_system_setting(self, setting_key: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        def _delete_setting(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    DELETE FROM system_settings
                    WHERE setting_key = %s AND is_public = true
                """, (setting_key,))
                return cursor.rowcount > 0
        
        try:
            return self.execute_in_transaction(_delete_setting)
        except Exception as e:
            logger.error(f"Delete system setting error: {e}")
            return False

    def get_normcontrol_prompt_template(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ —Å–∏—Å—Ç–µ–º—ã –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            normcontrol_prompt = self.get_system_setting("normcontrol_prompt")
            if not normcontrol_prompt:
                normcontrol_prompt = "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            prompt_template = self.get_system_setting("normcontrol_prompt_template")
            if prompt_template:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω
                prompt_template = prompt_template.replace("{normcontrol_prompt}", normcontrol_prompt)
                return prompt_template
            
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü
            processed_prompt = normcontrol_prompt.replace("{document_content}", "{page_content}")
            processed_prompt = processed_prompt.replace("{normative_docs}", "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
            
            # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            simple_template = f"""
{processed_prompt}

=== –ü–†–û–í–ï–†–ö–ê –°–¢–†–ê–ù–ò–¶–´ {{page_number}} ===

–°–û–î–ï–†–ñ–ò–ú–û–ï –°–¢–†–ê–ù–ò–¶–´:
{{page_content}}

–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ü–†–û–í–ï–†–ö–ï:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¢–û–õ–¨–ö–û —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {{page_number}}
2. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º (–ì–û–°–¢, –°–ü, –°–ù–∏–ü)
3. –ò—â–∏—Ç–µ –æ—à–∏–±–∫–∏ –≤ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏, –Ω—É–º–µ—Ä–∞—Ü–∏–∏, —Ä–∞–∑–º–µ—Ä–∞—Ö, –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è—Ö
4. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –ø–æ–ª–Ω–æ—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π
5. –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞

–ö–†–ò–¢–ï–†–ò–ò –û–¶–ï–ù–ö–ò:
- –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï: –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–æ—Ä–º–∞–º
- –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø: –Ω–µ–ø–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏
- –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ï: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é, –∑–∞–º–µ—á–∞–Ω–∏—è –ø–æ —Å—Ç–∏–ª—é

–°–§–û–†–ú–ò–†–£–ô–¢–ï –û–¢–ß–ï–¢ –í –§–û–†–ú–ê–¢–ï JSON:

{{
  "page_number": {{page_number}},
  "overall_status": "pass|fail|uncertain",
  "confidence": 0.0-1.0,
  "total_findings": —á–∏—Å–ª–æ,
  "critical_findings": —á–∏—Å–ª–æ,
  "warning_findings": —á–∏—Å–ª–æ,
  "info_findings": —á–∏—Å–ª–æ,
  "compliance_percentage": 0-100,
  "findings": [
    {{
      "id": "—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä",
      "type": "critical|warning|info",
      "category": "–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ|—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ_—Ä–µ—à–µ–Ω–∏–µ|–Ω–æ—Ä–º–∞—Ç–∏–≤—ã|–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
      "title": "–∫—Ä–∞—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_–ø—Ä–æ–±–ª–µ–º—ã",
      "description": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ_–æ–ø–∏—Å–∞–Ω–∏–µ_–ø—Ä–æ–±–ª–µ–º—ã",
      "normative_reference": "—Å—Å—ã–ª–∫–∞_–Ω–∞_–Ω–æ—Ä–º–∞—Ç–∏–≤",
      "recommendation": "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è_–ø–æ_–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é",
      "severity": "critical|warning|info",
      "location": "–æ–ø–∏—Å–∞–Ω–∏–µ_–º–µ—Å—Ç–∞_–Ω–∞_—Å—Ç—Ä–∞–Ω–∏—Ü–µ"
    }}
  ],
  "summary": "–æ–±—â–∏–π_–≤—ã–≤–æ–¥_–ø–æ_—Å—Ç—Ä–∞–Ω–∏—Ü–µ",
  "recommendations": "–æ–±—â–∏–µ_—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏_–ø–æ_—É–ª—É—á—à–µ–Ω–∏—é"
}}

–í–ê–ñ–ù–û: 
- –í–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –£–∫–∞–∑—ã–≤–∞–π—Ç–µ —Ç–æ—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
- –î–∞–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
- –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –∑–∞–º–µ—á–∞–Ω–∏—è
"""
            
            return simple_template
            
        except Exception as e:
            logger.error(f"Get normcontrol prompt template error: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."

    def combine_page_results(self, document_id: int, page_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        try:
            logger.info(f"üîç [DEBUG] DocumentParser: Combining results from {len(page_results)} pages for document {document_id}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–∏–µ —Å—á–µ—Ç—á–∏–∫–∏
            total_findings = 0
            critical_findings = 0
            warning_findings = 0
            info_findings = 0
            all_findings = []
            successful_pages = 0
            failed_pages = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for page_result in page_results:
                page_number = page_result.get("page_number", 0)
                
                if page_result.get("status") == "success":
                    successful_pages += 1
                    result_data = page_result.get("result", {})
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–º–µ—á–∞–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_findings = result_data.get("findings", [])
                    for finding in page_findings:
                        finding["page_number"] = page_number
                        all_findings.append(finding)
                        
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ —Ç–∏–ø–∞–º
                        severity = finding.get("severity", "info")
                        if severity == "critical":
                            critical_findings += 1
                        elif severity == "warning":
                            warning_findings += 1
                        else:
                            info_findings += 1
                    
                    total_findings += len(page_findings)
                    logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number}: {len(page_findings)} findings")
                    
                else:
                    failed_pages += 1
                    logger.warning(f"üîç [DEBUG] DocumentParser: Page {page_number} failed: {page_result.get('error', 'Unknown error')}")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            combined_result = {
                "document_id": document_id,
                "total_pages": len(page_results),
                "successful_pages": successful_pages,
                "failed_pages": failed_pages,
                "total_findings": total_findings,
                "critical_findings": critical_findings,
                "warning_findings": warning_findings,
                "info_findings": info_findings,
                "findings": all_findings,
                "status": "completed" if failed_pages == 0 else "completed_with_errors"
            }
            
            logger.info(f"üîç [DEBUG] DocumentParser: Combined result: {total_findings} total findings, "
                       f"{critical_findings} critical, {warning_findings} warnings, {info_findings} info")
            
            return combined_result
            
        except Exception as e:
            logger.error(f"üîç [DEBUG] DocumentParser: Error combining page results: {e}")
            return {
                "document_id": document_id,
                "status": "error",
                "error": str(e)
            }

    def split_document_into_pages(self, document_id: int) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π PDF"""
        logger.info(f"üîç [DEBUG] DocumentParser: Starting split_document_into_pages for document {document_id}")
        try:
            pages = []
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT page_number, element_content, element_type, confidence_score
                    FROM checkable_elements
                    WHERE checkable_document_id = %s
                    ORDER BY page_number, id
                """, (document_id,))
                elements = cursor.fetchall()
            
            if not elements:
                logger.warning(f"No elements found for document {document_id}")
                return []
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            current_page = None
            current_page_content = []
            current_page_elements = []
            
            for element in elements:
                page_number = element["page_number"]
                
                if current_page is None:
                    current_page = page_number
                    current_page_content = []
                    current_page_elements = []
                
                if page_number != current_page:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    pages.append({
                        "page_number": current_page,
                        "content": "\n\n".join(current_page_content),
                        "char_count": len("\n\n".join(current_page_content)),
                        "element_count": len(current_page_elements),
                        "elements": current_page_elements
                    })
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    current_page = page_number
                    current_page_content = []
                    current_page_elements = []
                
                # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç –∫ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                current_page_content.append(element["element_content"])
                current_page_elements.append({
                    "type": element["element_type"],
                    "content": element["element_content"],
                    "confidence": element["confidence_score"]
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if current_page is not None:
                pages.append({
                    "page_number": current_page,
                    "content": "\n\n".join(current_page_content),
                    "char_count": len("\n\n".join(current_page_content)),
                    "element_count": len(current_page_elements),
                    "elements": current_page_elements
                })
            
            logger.info(f"üîç [DEBUG] DocumentParser: Split document {document_id} into {len(pages)} pages based on PDF structure")
            logger.info(f"üîç [DEBUG] DocumentParser: Page details for document {document_id}:")
            for page in pages:
                logger.info(f"üîç [DEBUG] DocumentParser: - Page {page['page_number']}: {page['char_count']} chars, {page['element_count']} elements")
                logger.info(f"üîç [DEBUG] DocumentParser: - Page {page['page_number']} content preview: {page['content'][:100]}...")
            
            logger.info(f"üîç [DEBUG] DocumentParser: split_document_into_pages completed for document {document_id}")
            
            return pages
            
        except Exception as e:
            logger.error(f"Error splitting document into pages: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []

    async def perform_norm_control_check_for_page(self, document_id: int, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üîç [DEBUG] DocumentParser: Starting norm control check for document {document_id}, page {page_data['page_number']}")
        logger.info(f"üîç [DEBUG] DocumentParser: Page parameters:")
        logger.info(f"üîç [DEBUG] DocumentParser: - Page number: {page_data['page_number']}")
        logger.info(f"üîç [DEBUG] DocumentParser: - Content length: {len(page_data['content'])} characters")
        logger.info(f"üîç [DEBUG] DocumentParser: - Element count: {page_data['element_count']}")
        logger.info(f"üîç [DEBUG] DocumentParser: - Content preview: {page_data['content'][:100]}...")
        
        try:
            page_number = page_data["page_number"]
            page_content = page_data["content"]
            
            logger.info(f"üîç [DEBUG] DocumentParser: Starting norm control check for document {document_id}, page {page_number}")
            logger.info(f"üîç [DEBUG] DocumentParser: Page content length: {len(page_content)} characters")
            
            # ===== –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–ú–ü–¢–ê –î–õ–Ø LLM –ò–ó –°–ò–°–¢–ï–ú–´ –ù–ê–°–¢–†–û–ï–ö =====
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ —Å–∏—Å—Ç–µ–º—ã –Ω–∞—Å—Ç—Ä–æ–µ–∫
            prompt_template = self.get_normcontrol_prompt_template()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞
            prompt = prompt_template.format(
                page_number=page_number,
                page_content=page_content
            )
            
            # ===== –û–¢–ü–†–ê–í–ö–ê –ó–ê–ü–†–û–°–ê –ö LLM –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –°–¢–†–ê–ù–ò–¶–´ =====
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM —á–µ—Ä–µ–∑ gateway –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            logger.info(f"Sending request to LLM for page {page_number}...")
            logger.info(f"Prompt length: {len(prompt)} characters")
            
            async with httpx.AsyncClient(verify=False, timeout=LLM_REQUEST_TIMEOUT) as client:
                response = await client.post(
                    "http://gateway:8443/v1/chat/completions",
                    headers={
                        "Authorization": "Bearer test-token",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "llama3.1:8b",
                        "messages": [
                            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.1
                    }
                )
                
                # ===== –ü–û–õ–£–ß–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê –ü–†–û–í–ï–†–ö–ò –û–¢ LLM =====
                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    
                    # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç –æ—Ç LLM
                    try:
                        import json
                        import re
                        
                        # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–µ–∂–¥—É —Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏)
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group(0)
                            check_result = json.loads(json_str)
                        else:
                            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç
                            check_result = json.loads(content)
                        
                        return {
                            "status": "success",
                            "result": check_result,
                            "raw_response": content
                        }
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON parsing error: {e}")
                        logger.error(f"Raw response: {content}")
                        return {
                            "status": "error",
                            "error": "Invalid JSON response from LLM",
                            "raw_response": content
                        }
                else:
                    logger.error(f"LLM request failed: {response.status_code} - {response.text}")
                    return {
                        "status": "error",
                        "error": f"LLM request failed: {response.status_code}"
                    }
                    
        except Exception as e:
            logger.error(f"Norm control check error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def perform_norm_control_check(self, document_id: int, document_content: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º LLM"""
        try:
            logger.info(f"Starting norm control check for document {document_id}")
            logger.info(f"Document content length: {len(document_content)} characters")
            
            # ===== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–†–û–í–ï–†–ö–ò –ù–û–†–ú–û–ö–û–ù–¢–†–û–õ–Ø –° LLM =====
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π PDF
            pages = self.split_document_into_pages(document_id)
            logger.info(f"Document split into {len(pages)} pages based on PDF structure")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–¥–µ–ª—å–Ω–æ
            page_results = []
            total_findings = 0
            total_critical_findings = 0
            total_warning_findings = 0
            total_info_findings = 0
            all_findings = []
            
            for page_data in pages:
                logger.info(f"Processing page {page_data['page_number']} of {len(pages)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                # ===== –í–´–ó–û–í –ü–†–û–í–ï–†–ö–ò –°–¢–†–ê–ù–ò–¶–´ –° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï–ú LLM =====
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
                page_result = await self.perform_norm_control_check_for_page(document_id, page_data)
                
                if page_result["status"] == "success":
                    result_data = page_result["result"]
                    page_results.append(result_data)
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    total_findings += result_data.get("total_findings", 0)
                    total_critical_findings += result_data.get("critical_findings", 0)
                    total_warning_findings += result_data.get("warning_findings", 0)
                    total_info_findings += result_data.get("info_findings", 0)
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ findings
                    page_findings = result_data.get("findings", [])
                    for finding in page_findings:
                        finding["page_number"] = page_data["page_number"]
                    all_findings.extend(page_findings)
                    
                    logger.info(f"Page {page_data['page_number']} processed successfully")
                else:
                    logger.error(f"Failed to process page {page_data['page_number']}: {page_result.get('error', 'Unknown error')}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_results.append({
                        "page_number": page_data["page_number"],
                        "overall_status": "error",
                        "confidence": 0.0,
                        "total_findings": 0,
                        "critical_findings": 0,
                        "warning_findings": 0,
                        "info_findings": 0,
                        "findings": [],
                        "summary": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_result.get('error', 'Unknown error')}",
                        "compliance_percentage": 0
                    })
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            overall_status = "pass"
            if total_critical_findings > 0:
                overall_status = "fail"
            elif total_warning_findings > 0:
                overall_status = "uncertain"
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            total_pages = len(pages)
            successful_pages = len([r for r in page_results if r.get("overall_status") == "pass"])
            compliance_percentage = (successful_pages / total_pages * 100) if total_pages > 0 else 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
            combined_result = {
                "overall_status": overall_status,
                "confidence": 0.8,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ
                "total_findings": total_findings,
                "critical_findings": total_critical_findings,
                "warning_findings": total_warning_findings,
                "info_findings": total_info_findings,
                "total_pages": total_pages,
                "successful_pages": successful_pages,
                "page_results": page_results,
                "findings": all_findings,
                "summary": f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü. –ù–∞–π–¥–µ–Ω–æ {total_findings} –Ω–∞—Ä—É—à–µ–Ω–∏–π.",
                "compliance_percentage": compliance_percentage,
                "recommendations": f"–û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {total_critical_findings} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π, {total_warning_findings} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π, {total_info_findings} –∑–∞–º–µ—á–∞–Ω–∏–π."
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–†–û–í–ï–†–ö–ò LLM =====
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            await self.save_norm_control_result(document_id, combined_result)
            
            return {
                "status": "success",
                "result": combined_result,
                "pages_processed": len(pages)
            }
                    
        except Exception as e:
            logger.error(f"Norm control check error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def save_norm_control_result(self, document_id: int, check_result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –æ—Ç LLM –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        def _save_result(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO norm_control_results 
                    (checkable_document_id, analysis_date, analysis_type, model_used, analysis_status,
                     total_findings, critical_findings, warning_findings, info_findings)
                    VALUES (%s, CURRENT_TIMESTAMP, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,
                    "norm_control",
                    "llama3.1:8b",
                    check_result.get("overall_status", "uncertain"),
                    check_result.get("total_findings", 0),
                    check_result.get("critical_findings", 0),
                    check_result.get("warning_findings", 0),
                    check_result.get("info_findings", 0)
                ))
                
                result_id = cursor.fetchone()[0]
                logger.info(f"Saved norm control result {result_id} for document {document_id}")
                return result_id
        
        try:
            result_id = self.execute_in_transaction(_save_result)
            
            # ===== –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–ï–¢–ê –û –ü–†–û–í–ï–†–ö–ï LLM =====
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM
            await self.create_review_report(document_id, result_id, check_result)
            
            return result_id
                
        except Exception as e:
            logger.error(f"Save norm control result error: {e}")
            raise

    async def create_review_report(self, document_id: int, result_id: int, check_result: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM"""
        def _create_report(conn):
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO review_reports 
                    (checkable_document_id, norm_control_result_id, report_date, review_type,
                     overall_status, reviewer_name, conclusion)
                    VALUES (%s, %s, CURRENT_TIMESTAMP, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,
                    result_id,
                    "automatic",
                    check_result.get("overall_status", "uncertain"),
                    "AI System",
                    f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {check_result.get('summary', '')}"
                ))
                
                report_id = cursor.fetchone()[0]
                logger.info(f"Created review report {report_id} for document {document_id}")
                return report_id
        
        try:
            return self.execute_in_transaction(_create_report)
        except Exception as e:
            logger.error(f"Create review report error: {e}")
            raise

# –ú–æ–¥–µ–ª–∏ Pydantic
class StatusUpdateRequest(BaseModel):
    status: str
    reviewer: str = None
    notes: str = None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
parser = DocumentParser()

@app.post("/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    category: str = Form("other")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"üîç [DEBUG] DocumentParser: Upload started for file: {file.filename}")
    print(f"üîç [DEBUG] DocumentParser: File size: {file.size} bytes")
    print(f"üîç [DEBUG] DocumentParser: Content type: {file.content_type}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if file.size and file.size > MAX_NORMATIVE_DOCUMENT_SIZE:
        print(f"üîç [DEBUG] DocumentParser: File too large: {file.size} bytes (max: {MAX_NORMATIVE_DOCUMENT_SIZE})")
        raise HTTPException(
            status_code=413, 
            detail=f"File too large. Maximum size is {MAX_NORMATIVE_DOCUMENT_SIZE // (1024*1024)}MB. Your file is {file.size // (1024*1024)}MB"
        )
    
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        print(f"üîç [DEBUG] DocumentParser: Reading file content...")
        file_content = await file.read()
        file_size = len(file_content)
        print(f"üîç [DEBUG] DocumentParser: File content read, size: {file_size} bytes")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        print(f"üîç [DEBUG] DocumentParser: Detecting file type...")
        file_type = parser.detect_file_type(file_content)
        print(f"üîç [DEBUG] DocumentParser: Detected file type: {file_type}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞
        print(f"üîç [DEBUG] DocumentParser: Calculating file hash...")
        document_hash = parser.calculate_file_hash(file_content)
        print(f"üîç [DEBUG] DocumentParser: File hash: {document_hash}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        print(f"üîç [DEBUG] DocumentParser: Checking for duplicates...")
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("SELECT id FROM uploaded_documents WHERE document_hash = %s", (document_hash,))
            existing_doc = cursor.fetchone()
            if existing_doc:
                print(f"üîç [DEBUG] DocumentParser: Document already exists with ID: {existing_doc['id']}")
                raise HTTPException(status_code=400, detail="Document already exists")
        print(f"üîç [DEBUG] DocumentParser: No duplicates found")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
        print(f"üîç [DEBUG] DocumentParser: Saving file to disk...")
        file_path = f"/app/uploads/{document_hash}_{file.filename}"
        with open(file_path, "wb") as f:
            f.write(file_content)
        print(f"üîç [DEBUG] DocumentParser: File saved to: {file_path}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å—Ç–∞—Ç—É—Å–æ–º "processing"
        print(f"üîç [DEBUG] DocumentParser: Creating initial database record...")
        document_id = parser.create_initial_document_record(
            file.filename,
            file_type,
            file_size,
            document_hash,
            file_path,
            category
        )
        print(f"üîç [DEBUG] DocumentParser: Created document record with ID: {document_id}")
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print(f"üîç [DEBUG] DocumentParser: Starting async document processing...")
        print(f"üîç [DEBUG] DocumentParser: Adding background task for document {document_id}")
        background_tasks.add_task(
            process_document_async,
            document_id=document_id,
            file_path=file_path,
            file_type=file_type,
            filename=file.filename
        )
        print(f"üîç [DEBUG] DocumentParser: Background task added successfully")
        
        result = {
            "document_id": document_id,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "status": "processing",
            "message": "Document uploaded successfully. Processing started in background."
        }
        print(f"üîç [DEBUG] DocumentParser: Upload initiated successfully: {result}")
        return result
        
    except HTTPException as e:
        print(f"üîç [DEBUG] DocumentParser: HTTPException in upload: {e.status_code} - {e.detail}")
        raise e
    except Exception as e:
        print(f"üîç [DEBUG] DocumentParser: Unexpected error in upload: {type(e).__name__}: {str(e)}")
        import traceback
        print(f"üîç [DEBUG] DocumentParser: Traceback: {traceback.format_exc()}")
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_document_async(document_id: int, file_path: str, file_type: str, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"üîç [DEBUG] DocumentParser: Starting async processing for document {document_id}")
    print(f"üîç [DEBUG] DocumentParser: File type: {file_type}, filename: {filename}")
    print(f"üîç [DEBUG] DocumentParser: File path: {file_path}")
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        print(f"üîç [DEBUG] DocumentParser: Updating status to 'processing' for document {document_id}")
        parser.update_document_status(document_id, "processing")
        print(f"üîç [DEBUG] DocumentParser: Status updated successfully")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞
        print(f"üîç [DEBUG] DocumentParser: Reading file from disk: {file_path}")
        with open(file_path, "rb") as f:
            file_content = f.read()
        print(f"üîç [DEBUG] DocumentParser: File content size: {len(file_content)} bytes")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print(f"üîç [DEBUG] DocumentParser: Parsing document {document_id}...")
        if file_type == "pdf":
            elements = parser.parse_pdf(file_content)
        else:
            raise ValueError(f"Unsupported file type: {file_type}")
        
        print(f"üîç [DEBUG] DocumentParser: Parsing completed for document {document_id}, elements count: {len(elements.document_pages_results)}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        print(f"üîç [DEBUG] DocumentParser: Saving elements to database for document {document_id}...")
        parser.save_elements_to_database(document_id, elements)
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        print(f"üîç [DEBUG] DocumentParser: Calculating tokens for document {document_id}...")
        total_tokens = parser.calculate_document_tokens(elements)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        print(f"üîç [DEBUG] DocumentParser: Updating document record for {document_id}...")
        parser.update_document_completion(document_id, len(elements.document_pages_results), total_tokens)
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ RAG-—Å–µ—Ä–≤–∏—Å
        print(f"üîç [DEBUG] DocumentParser: Starting RAG indexing for document {document_id}...")
        asyncio.create_task(
            parser.index_to_rag_service_async(
                document_id=document_id,
                document_title=filename,
                inspection_result=elements
            )
        )
        
        print(f"üîç [DEBUG] DocumentParser: Async processing completed for document {document_id}")
        
    except Exception as e:
        print(f"üîç [DEBUG] DocumentParser: Error in async processing for document {document_id}: {str(e)}")
        import traceback
        print(f"üîç [DEBUG] DocumentParser: Async processing traceback: {traceback.format_exc()}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "error"
        try:
            parser.update_document_status(document_id, "error")
        except Exception as update_error:
            print(f"üîç [DEBUG] DocumentParser: Failed to update error status: {update_error}")
        
        logger.error(f"Async processing error for document {document_id}: {e}")

async def process_checkable_document_async(document_id: int, document_content: str, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
    logger.info(f"üîç [DEBUG] DocumentParser: Starting async processing for checkable document {document_id}")
    logger.info(f"üîç [DEBUG] DocumentParser: Async processing parameters:")
    logger.info(f"üîç [DEBUG] DocumentParser: - Document ID: {document_id}")
    logger.info(f"üîç [DEBUG] DocumentParser: - Document content size: {len(document_content)} characters")
    logger.info(f"üîç [DEBUG] DocumentParser: - Filename: {filename}")
    log_memory_usage("start of async processing")
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        logger.info(f"üîç [DEBUG] DocumentParser: Updating document {document_id} status to 'processing'")
        parser.update_checkable_document_status(document_id, "processing")
        logger.info(f"üîç [DEBUG] DocumentParser: Updated document {document_id} status to 'processing'")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        logger.info(f"üîç [DEBUG] DocumentParser: Splitting document {document_id} into pages")
        pages = parser.split_document_into_pages(document_id)
        logger.info(f"üîç [DEBUG] DocumentParser: Document {document_id} split into {len(pages)} pages")
        
        if not pages:
            logger.warning(f"üîç [DEBUG] DocumentParser: No pages found for document {document_id}, using fallback")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            result = await parser.perform_norm_control_check(document_id, document_content)
        else:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            logger.info(f"üîç [DEBUG] DocumentParser: Starting page-by-page norm control check for document {document_id}")
            all_page_results = []
            
            for i, page_data in enumerate(pages):
                page_number = page_data["page_number"]
                logger.info(f"üîç [DEBUG] DocumentParser: Processing page {page_number}/{len(pages)} for document {document_id}")
                
                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_result = await parser.perform_norm_control_check_for_page(document_id, page_data)
                    page_result["page_number"] = page_number
                    all_page_results.append(page_result)
                    
                    logger.info(f"üîç [DEBUG] DocumentParser: Page {page_number} processed successfully")
                    
                except Exception as page_error:
                    logger.error(f"üîç [DEBUG] DocumentParser: Error processing page {page_number}: {page_error}")
                    all_page_results.append({
                        "page_number": page_number,
                        "status": "error",
                        "error": str(page_error)
                    })
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            logger.info(f"üîç [DEBUG] DocumentParser: Combining results from {len(all_page_results)} pages")
            result = parser.combine_page_results(document_id, all_page_results)
        
        logger.info(f"üîç [DEBUG] DocumentParser: Norm control check completed for document {document_id}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "completed"
        parser.update_checkable_document_status(document_id, "completed")
        logger.info(f"üîç [DEBUG] DocumentParser: Updated document {document_id} status to 'completed'")
        
        logger.info(f"üîç [DEBUG] DocumentParser: Async processing completed successfully for document {document_id}")
        
    except Exception as e:
        logger.error(f"üîç [DEBUG] DocumentParser: Error in async processing for checkable document {document_id}: {str(e)}")
        import traceback
        logger.error(f"üîç [DEBUG] DocumentParser: Async processing traceback: {traceback.format_exc()}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "error"
        try:
            parser.update_checkable_document_status(document_id, "error")
        except Exception as update_error:
            logger.error(f"üîç [DEBUG] DocumentParser: Failed to update error status: {update_error}")

@app.get("/documents")
async def list_documents():
    """–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, file_type, file_size, upload_date, processing_status, token_count, category
                FROM uploaded_documents
                ORDER BY upload_date DESC
            """)
            documents = cursor.fetchall()
            
        return {"documents": [dict(doc) for doc in documents]}
        
    except Exception as e:
        logger.error(f"List documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/elements")
async def get_document_elements(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, element_type, element_content, page_number, confidence_score, created_at
                FROM extracted_elements
                WHERE uploaded_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
        return {"elements": [dict(elem) for elem in elements]}
        
    except Exception as e:
        logger.error(f"Get elements error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents/{document_id}/process")
async def process_document_manual(document_id: int, background_tasks: BackgroundTasks):
    """–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        print(f"üîç [DEBUG] DocumentParser: Manual processing requested for document {document_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, file_type, file_size, document_hash, processing_status
                FROM uploaded_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
            
        if document["processing_status"] == "completed":
            return {"message": "Document already processed", "status": "completed"}
            
        print(f"üîç [DEBUG] DocumentParser: Document info: {document}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT file_path FROM uploaded_documents WHERE id = %s
            """, (document_id,))
            result = cursor.fetchone()
            file_path = result.get("file_path") if result else None
            
        if not file_path:
            raise HTTPException(status_code=404, detail="File not found on disk")
            
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        background_tasks.add_task(
            process_document_async,
            document_id=document_id,
            file_path=file_path,
            file_type=document["file_type"],
            filename=document["original_filename"]
        )
        
        return {"message": "Processing started", "document_id": document_id}
        
    except Exception as e:
        logger.error(f"Manual processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}")
async def delete_normative_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = await parser.delete_normative_document(document_id)
        
        if success:
            return {"status": "success", "message": f"Document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete normative document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/checkable-documents/{document_id}")
async def delete_checkable_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = parser.delete_checkable_document(document_id)
        
        if success:
            return {"status": "success", "message": f"Checkable document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete checkable document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/chat")
async def upload_chat_file(
    file: UploadFile = File(...)
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —á–∞—Ç–µ"""
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file_content = await file.read()
        file_size = len(file_content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 10MB –¥–ª—è —á–∞—Ç–∞)
        if file_size > 10 * 1024 * 1024:
            raise HTTPException(status_code=413, detail="File too large. Maximum size is 10MB")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        file_type = parser.detect_file_type(file_content)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞
        elements = parser.parse_file(file_content, file_type)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        text_content = "\n\n".join([elem.get("element_content", "") for elem in elements if elem.get("element_content")])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —á–∞—Ç–∞ (—É—á–∏—Ç—ã–≤–∞—è –ª–∏–º–∏—Ç Ollama ~4000 —Ç–æ–∫–µ–Ω–æ–≤)
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –Ω–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        # 4000 —Ç–æ–∫–µ–Ω–æ–≤ ‚âà 12000 —Å–∏–º–≤–æ–ª–æ–≤ (3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
        max_chars = 10000  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è Ollama
        if len(text_content) > max_chars:
            text_content = text_content[:max_chars] + "\n\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞. –ú–∞–∫—Å–∏–º—É–º 10000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —á–∞—Ç–∞]"
        
        return {
            "success": True,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "content": text_content,
            "elements_count": len(elements.document_pages_results)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat file upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/checkable")
async def upload_checkable_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üîç [DEBUG] DocumentParser: upload_checkable_document started for file: {file.filename}")
    logger.info(f"üîç [DEBUG] DocumentParser: Content-Type: {file.content_type}")
    logger.info(f"üîç [DEBUG] DocumentParser: File size from UploadFile: {file.size}")
    logger.info(f"üîç [DEBUG] DocumentParser: Processing mode: ASYNC")
    logger.info(f"üîç [DEBUG] DocumentParser: Memory limit: {MAX_CHECKABLE_DOCUMENT_SIZE // (1024*1024)}MB")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if file.size and file.size > MAX_CHECKABLE_DOCUMENT_SIZE:
        logger.warning(f"üîç [DEBUG] DocumentParser: File too large: {file.size} bytes (max: {MAX_CHECKABLE_DOCUMENT_SIZE})")
        raise HTTPException(
            status_code=413, 
            detail=f"File too large. Maximum size is {MAX_CHECKABLE_DOCUMENT_SIZE // (1024*1024)}MB. Your file is {file.size // (1024*1024)}MB"
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    log_memory_usage("before file processing")
    available_memory = get_available_memory()
    file_size_mb = file.size / (1024 * 1024) if file.size else 0
    
    if available_memory < file_size_mb * 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3x –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.warning(f"üîç [DEBUG] DocumentParser: Insufficient memory for file {file_size_mb:.1f}MB. Available: {available_memory:.1f}MB")
        raise HTTPException(
            status_code=507,  # Insufficient Storage
            detail=f"Insufficient memory for processing. File size: {file_size_mb:.1f}MB, Available memory: {available_memory:.1f}MB"
        )
    
    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Reading file content...")
        file_content = await file.read()
        file_size = len(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: File content read successfully, size: {file_size} bytes")
        logger.info(f"üîç [DEBUG] DocumentParser: Content preview (first 100 bytes): {file_content[:100]}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Detecting file type...")
        file_type = parser.detect_file_type(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: Detected file type: {file_type}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Calculating file hash...")
        document_hash = parser.calculate_file_hash(file_content)
        logger.info(f"üîç [DEBUG] DocumentParser: Calculated hash: {document_hash}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        logger.info(f"üîç [DEBUG] DocumentParser: Checking for duplicate document...")
        with parser.db_conn.cursor() as cursor:
            cursor.execute("SELECT id FROM checkable_documents WHERE document_hash = %s", (document_hash,))
            existing_doc = cursor.fetchone()
            if existing_doc:
                logger.warning(f"üîç [DEBUG] DocumentParser: Document already exists with hash: {document_hash}, existing ID: {existing_doc[0]}")
                raise HTTPException(status_code=400, detail="Document already exists")
            else:
                logger.info(f"üîç [DEBUG] DocumentParser: No duplicate found, proceeding with upload")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Starting document parsing for type: {file_type}")
        
        if file_type == "pdf":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing PDF document...")
            logger.info(f"üîç [DEBUG] DocumentParser: PDF size: {file_size / (1024*1024):.1f}MB")
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
            import asyncio
            pdf_timeout = 1200 if file_size > 10 * 1024 * 1024 else 600  # 20 –º–∏–Ω –¥–ª—è —Ñ–∞–π–ª–æ–≤ > 10MB, 10 –º–∏–Ω –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                inspection_result = await asyncio.wait_for(
                    asyncio.to_thread(parser.parse_pdf, file_content),
                    timeout=pdf_timeout
                )
                logger.info(f"üîç [DEBUG] DocumentParser: PDF parsing completed successfully")
                logger.info(f"üîç [DEBUG] DocumentParser: Parsed {len(inspection_result.document_pages_results)} pages")
                logger.info(f"üîç [DEBUG] DocumentParser: Document stats - Total: {inspection_result.document_pages}, Vector: {inspection_result.document_pages_vector}, Scanned: {inspection_result.document_pages_scanned}")
                
            except asyncio.TimeoutError:
                logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing timeout after {pdf_timeout} seconds")
                raise HTTPException(
                    status_code=408,  # Request Timeout
                    detail=f"PDF parsing timeout after {pdf_timeout} seconds. File is too large or complex."
                )
            except Exception as pdf_error:
                logger.error(f"üîç [DEBUG] DocumentParser: PDF parsing error: {pdf_error}")
                raise HTTPException(
                    status_code=500,
                    detail=f"PDF parsing failed: {str(pdf_error)}"
                )

        elif file_type == "docx":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing DOCX document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è docx
            elements = parser.parse_docx(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: DOCX parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "docx_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed DOCX element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: DOCX processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "dwg":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing DWG document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è dwg
            elements = parser.parse_dwg(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: DWG parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "dwg_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed DWG element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: DWG processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "ifc":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing IFC document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è ifc
            elements = parser.parse_ifc(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: IFC parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "ifc_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed IFC element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: IFC processing completed, created {len(inspection_result.document_pages_results)} page results")
        elif file_type == "txt":
            logger.info(f"üîç [DEBUG] DocumentParser: Parsing TXT document...")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è txt
            elements = parser.parse_text(file_content)
            logger.info(f"üîç [DEBUG] DocumentParser: TXT parsing completed, found {len(elements)} elements")
            inspection_result = DocumentInspectionResult()
            inspection_result.document_pages_results = []
            for i, element in enumerate(elements):
                page_result = DocumentPageInspectionResult()
                page_result.page_number = element.get("page_number", 1)
                page_result.page_text = element.get("element_content", "")
                page_result.page_text_confidence = element.get("confidence_score", 1.0)
                page_result.page_text_method = "txt_parsing"
                page_result.page_text_length = len(element.get("element_content", ""))
                page_result.page_type = "vector"
                inspection_result.document_pages_results.append(page_result)
                logger.debug(f"üîç [DEBUG] DocumentParser: Processed TXT element {i+1}, page {page_result.page_number}, text length: {page_result.page_text_length}")
            logger.info(f"üîç [DEBUG] DocumentParser: TXT processing completed, created {len(inspection_result.document_pages_results)} page results")
        else:
            logger.error(f"üîç [DEBUG] DocumentParser: Unsupported file type: {file_type}")
            raise HTTPException(status_code=400, detail=f"Unsupported file type: {file_type}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        logger.info(f"üîç [DEBUG] DocumentParser: Determining document category...")
        content_text = "\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
        logger.debug(f"üîç [DEBUG] DocumentParser: Content text length: {len(content_text)} characters")
        category = parser.determine_document_category(file.filename, content_text)
        logger.info(f"üîç [DEBUG] DocumentParser: Determined category: {category}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        logger.info(f"üîç [DEBUG] DocumentParser: Saving checkable document to database...")
        document_id = parser.save_checkable_document(
            file.filename,
            file.filename,
            file_type,
            file_size,
            document_hash,
            inspection_result,
            category
        )
        logger.info(f"üîç [DEBUG] DocumentParser: Document saved successfully with ID: {document_id}")
        
        # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
        logger.info(f"üîç [DEBUG] DocumentParser: Starting async norm control check...")
        document_content = "\n\n".join([page_result.page_text for page_result in inspection_result.document_pages_results])
        background_tasks.add_task(
            process_checkable_document_async,
            document_id=document_id,
            document_content=document_content,
            filename=file.filename
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞
        response_data = {
            "document_id": document_id,
            "filename": file.filename,
            "file_type": file_type,
            "file_size": file_size,
            "pages_count": len(inspection_result.document_pages_results),
            "category": category,
            "status": "processing",  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å "completed" –Ω–∞ "processing"
            "review_deadline": (datetime.now() + timedelta(days=2)).isoformat(),
            "document_stats": {
                "total_pages": inspection_result.document_pages,
                "vector_pages": inspection_result.document_pages_vector,
                "scanned_pages": inspection_result.document_pages_scanned,
                "unknown_pages": inspection_result.document_pages_unknown,
                "a4_sheets_equivalent": inspection_result.document_pages_total_a4_sheets_equivalent
            },
            "message": "Document uploaded successfully. Norm control check started in background."
        }
        
        logger.info(f"üîç [DEBUG] DocumentParser: Upload completed successfully")
        logger.info(f"üîç [DEBUG] DocumentParser: Response data: {response_data}")
        
        return response_data
        
    except HTTPException as http_ex:
        logger.warning(f"üîç [DEBUG] DocumentParser: HTTPException raised: {http_ex.status_code} - {http_ex.detail}")
        raise
    except Exception as e:
        logger.error(f"üîç [DEBUG] DocumentParser: Upload checkable document error: {e}")
        import traceback
        logger.error(f"üîç [DEBUG] DocumentParser: Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents")
async def list_checkable_documents():
    """–°–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        documents = parser.get_checkable_documents()
        return {"documents": documents}
        
    except Exception as e:
        logger.error(f"List checkable documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/elements")
async def get_checkable_document_elements(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, element_type, element_content, page_number, confidence_score, created_at
                FROM checkable_elements
                WHERE checkable_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
        return {"elements": [dict(elem) for elem in elements]}
        
    except Exception as e:
        logger.error(f"Get checkable elements error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/checkable-documents/{document_id}/status")
async def update_checkable_document_status(
    document_id: int,
    request: StatusUpdateRequest
):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = parser.update_review_status(document_id, request.status, request.reviewer, request.notes)
        if success:
            return {"status": "success", "message": f"Document {document_id} status updated"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except Exception as e:
        logger.error(f"Update status error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/cleanup-expired")
async def cleanup_expired_documents():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        deleted_count = parser.cleanup_expired_documents()
        return {
            "status": "success",
            "deleted_count": deleted_count,
            "message": f"Deleted {deleted_count} expired documents"
        }
        
    except Exception as e:
        logger.error(f"Cleanup error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex-documents")
async def reindex_documents():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, token_count
                FROM uploaded_documents
                WHERE processing_status = 'completed'
                ORDER BY upload_date DESC
            """)
            documents = cursor.fetchall()
        
        total_documents = len(documents)
        total_tokens = sum(doc['token_count'] or 0 for doc in documents)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        updated_count = 0
        new_total_tokens = 0
        
        for doc in documents:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –±–ª–æ–∫–µ
                with parser.db_conn.cursor(cursor_factory=RealDictCursor) as element_cursor:
                    element_cursor.execute("""
                        SELECT element_content
                        FROM extracted_elements
                        WHERE uploaded_document_id = %s
                    """, (doc['id'],))
                    elements = element_cursor.fetchall()
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
                total_doc_tokens = 0
                for element in elements:
                    if element['element_content']:
                        total_doc_tokens += parser.count_tokens(element['element_content'])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                with parser.db_conn.cursor() as update_cursor:
                    update_cursor.execute("""
                        UPDATE uploaded_documents
                        SET token_count = %s
                        WHERE id = %s
                    """, (total_doc_tokens, doc['id']))
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ RAG —Å–µ—Ä–≤–∏—Å
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                    with parser.db_conn.cursor(cursor_factory=RealDictCursor) as rag_cursor:
                        rag_cursor.execute("""
                            SELECT element_type, element_content, page_number
                            FROM extracted_elements
                            WHERE uploaded_document_id = %s
                            ORDER BY page_number, id
                        """, (doc['id'],))
                        rag_elements = rag_cursor.fetchall()
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ RAG —Å–µ—Ä–≤–∏—Å
                    await parser.index_to_rag_service(
                        document_id=doc['id'],
                        document_title=doc['original_filename'],
                        elements=[dict(elem) for elem in rag_elements]
                    )
                    logger.info(f"Successfully indexed document {doc['id']} in RAG service")
                except Exception as rag_error:
                    logger.error(f"Failed to index document {doc['id']} in RAG service: {rag_error}")
                
                new_total_tokens += total_doc_tokens
                updated_count += 1
                
            except Exception as e:
                logger.error(f"Error updating tokens for document {doc['id']}: {e}")
                parser.db_conn.rollback()
                continue
        
        parser.db_conn.commit()
        
        return {
            "status": "success",
            "total_documents": total_documents,
            "updated_documents": updated_count,
            "old_total_tokens": total_tokens,
            "new_total_tokens": new_total_tokens,
            "message": f"Reindexed {updated_count} documents with {new_total_tokens} total tokens"
        }
        
    except Exception as e:
        logger.error(f"Reindex error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/tokens")
async def get_document_tokens(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT id, original_filename, token_count, file_size, upload_date
                FROM uploaded_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
            if not document:
                raise HTTPException(status_code=404, detail="Document not found")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT element_type, element_content, page_number
                FROM extracted_elements
                WHERE uploaded_document_id = %s
                ORDER BY page_number, id
            """, (document_id,))
            elements = cursor.fetchall()
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ —Ç–∏–ø–∞–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            token_stats = {
                "total_tokens": document['token_count'] or 0,
                "elements_count": len(elements),
                "by_type": {},
                "by_page": {}
            }
            
            for element in elements:
                element_type = element['element_type']
                page_number = element['page_number'] or 1
                content = element['element_content'] or ""
                
                if content:
                    tokens = parser.count_tokens(content)
                    
                    # –ü–æ —Ç–∏–ø–∞–º
                    if element_type not in token_stats["by_type"]:
                        token_stats["by_type"][element_type] = {"count": 0, "tokens": 0}
                    token_stats["by_type"][element_type]["count"] += 1
                    token_stats["by_type"][element_type]["tokens"] += tokens
                    
                    # –ü–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                    if page_number not in token_stats["by_page"]:
                        token_stats["by_page"][page_number] = {"count": 0, "tokens": 0}
                    token_stats["by_page"][page_number]["count"] += 1
                    token_stats["by_page"][page_number]["tokens"] += tokens
            
            return {
                "document": dict(document),
                "token_statistics": token_stats
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document tokens error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# –ú–æ–¥–µ–ª–∏ Pydantic –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
class SettingUpdateRequest(BaseModel):
    setting_value: str

class SettingCreateRequest(BaseModel):
    setting_key: str
    setting_value: str
    setting_description: str
    setting_type: str = "text"

@app.get("/settings/prompt-templates")
async def get_prompt_templates():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    try:
        logger.info("Getting prompt templates...")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        normcontrol_prompt = parser.get_system_setting("normcontrol_prompt")
        logger.info(f"Normcontrol prompt: {normcontrol_prompt[:100] if normcontrol_prompt else 'None'}...")
        
        # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        prompt_template = parser.get_system_setting("normcontrol_prompt_template")
        logger.info(f"Prompt template: {prompt_template[:100] if prompt_template else 'None'}...")
        
        templates = {
            "normcontrol_prompt": normcontrol_prompt,
            "normcontrol_prompt_template": prompt_template,
            "has_custom_template": prompt_template is not None
        }
        
        logger.info("Successfully retrieved prompt templates")
        return {"status": "success", "templates": templates}
    except Exception as e:
        logger.error(f"Get prompt templates error: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/settings/prompt-templates")
async def update_prompt_template(request: Dict[str, Any]):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞"""
    try:
        template_key = request.get("template_key")
        template_value = request.get("template_value")
        template_description = request.get("template_description", "")
        
        if not template_key or not template_value:
            raise HTTPException(status_code=400, detail="Missing template_key or template_value")
        
        success = parser.create_system_setting(
            template_key, 
            template_value, 
            template_description, 
            "prompt_template"
        )
        
        if success:
            return {"status": "success", "message": f"Template {template_key} updated successfully"}
        else:
            raise HTTPException(status_code=500, detail="Failed to update template")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update prompt template error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/settings")
async def get_settings():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        settings = parser.get_system_settings()
        return {"settings": settings}
    except Exception as e:
        logger.error(f"Get settings error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/settings/{setting_key}")
async def get_setting(setting_key: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        setting_value = parser.get_system_setting(setting_key)
        if setting_value is None:
            raise HTTPException(status_code=404, detail="Setting not found")
        return {"setting_key": setting_key, "setting_value": setting_value}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/settings/{setting_key}")
async def update_setting(setting_key: str, request: SettingUpdateRequest):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.update_system_setting(setting_key, request.setting_value)
        if success:
            return {"status": "success", "message": f"Setting {setting_key} updated successfully"}
        else:
            raise HTTPException(status_code=404, detail="Setting not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/settings")
async def create_setting(request: SettingCreateRequest):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.create_system_setting(
            request.setting_key,
            request.setting_value,
            request.setting_description,
            request.setting_type
        )
        if success:
            return {"status": "success", "message": f"Setting {request.setting_key} created successfully"}
        else:
            raise HTTPException(status_code=500, detail="Failed to create setting")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Create setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/settings/{setting_key}")
async def delete_setting(setting_key: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        success = parser.delete_system_setting(setting_key)
        if success:
            return {"status": "success", "message": f"Setting {setting_key} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail="Setting not found or cannot be deleted")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete setting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/report")
async def get_checkable_document_report(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            cursor.execute("""
                SELECT id, original_filename, file_type, upload_date, review_deadline, review_status
                FROM checkable_documents
                WHERE id = %s
            """, (document_id,))
            document = cursor.fetchone()
            
            if not document:
                raise HTTPException(status_code=404, detail="Document not found")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            cursor.execute("""
                SELECT id, analysis_date, analysis_status,
                       total_findings, critical_findings, warning_findings, info_findings
                FROM norm_control_results
                WHERE checkable_document_id = %s
                ORDER BY analysis_date DESC
                LIMIT 1
            """, (document_id,))
            norm_result = cursor.fetchone()
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç—ã –æ –ø—Ä–æ–≤–µ—Ä–∫–µ
            cursor.execute("""
                SELECT id, report_date, overall_status, reviewer_name, conclusion
                FROM review_reports
                WHERE checkable_document_id = %s
                ORDER BY report_date DESC
            """, (document_id,))
            reports = cursor.fetchall()
            
            return {
                "document": dict(document),
                "norm_control_result": dict(norm_result) if norm_result else None,
                "review_reports": [dict(report) for report in reports]
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document report error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/checkable-documents/{document_id}/check")
async def trigger_norm_control_check(document_id: int):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT element_content
                FROM checkable_elements
                WHERE checkable_document_id = %s
                ORDER BY element_order, page_number
            """, (document_id,))
            elements = cursor.fetchall()
        
        if not elements:
            raise HTTPException(status_code=404, detail="Document content not found")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        document_content = "\n\n".join([elem["element_content"] for elem in elements])
        
        # ===== –ó–ê–ü–£–°–ö –ü–†–û–í–ï–†–ö–ò –ù–û–†–ú–û–ö–û–ù–¢–†–û–õ–Ø –° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï–ú LLM =====
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
        result = await parser.perform_norm_control_check(document_id, document_content)
        
        return {
            "status": "success",
            "message": "Norm control check completed",
            "result": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Trigger norm control check error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/checkable-documents/{document_id}/format-statistics")
async def get_document_format_statistics(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        stats = parser.get_document_format_statistics(document_id)
        return {"status": "success", "statistics": stats}
    except Exception as e:
        logger.error(f"Get document format statistics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/stats")
async def get_documents_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        with parser.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                    COUNT(CASE WHEN processing_status = 'pending' THEN 1 END) as pending_documents,
                    COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents,
                    COUNT(DISTINCT category) as unique_categories,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
            """)
            doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            cursor.execute("""
                SELECT 
                    category,
                    COUNT(*) as count,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
                WHERE category IS NOT NULL AND category != ''
                GROUP BY category
                ORDER BY count DESC
            """)
            categories = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_elements,
                    COUNT(DISTINCT uploaded_document_id) as documents_with_elements
                FROM extracted_elements
            """)
            elements_stats = cursor.fetchone()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            total_docs = doc_stats["total_documents"] or 0
            completed_docs = doc_stats["completed_documents"] or 0
            indexing_progress = (completed_docs / total_docs * 100) if total_docs > 0 else 0
            
        return {
            "status": "success",
            "statistics": {
                "total_documents": total_docs,
                "indexed_documents": completed_docs,
                "indexing_progress_percent": round(indexing_progress, 1),
                "categories_count": doc_stats["unique_categories"] or 0,
                "total_tokens": doc_stats["total_tokens"] or 0,
                "total_elements": elements_stats["total_elements"] or 0,
                "documents_with_elements": elements_stats["documents_with_elements"] or 0,
                "categories": [dict(cat) for cat in categories]
            }
        }
        
    except Exception as e:
        logger.error(f"Get documents stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        db_conn = parser.get_db_connection()
        with db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                    COUNT(CASE WHEN processing_status = 'pending' THEN 1 END) as pending_documents,
                    COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents,
                    COUNT(CASE WHEN file_type = 'pdf' THEN 1 END) as pdf_documents,
                    COUNT(CASE WHEN file_type = 'docx' THEN 1 END) as docx_documents,
                    COUNT(CASE WHEN file_type = 'dwg' THEN 1 END) as dwg_documents,
                    COUNT(CASE WHEN file_type = 'txt' THEN 1 END) as txt_documents,
                    SUM(file_size) as total_size_bytes,
                    AVG(file_size) as avg_file_size_bytes,
                    SUM(token_count) as total_tokens
                FROM uploaded_documents
            """)
            doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_checkable_documents,
                    COUNT(CASE WHEN review_status = 'pending' THEN 1 END) as pending_reviews,
                    COUNT(CASE WHEN review_status = 'completed' THEN 1 END) as completed_reviews,
                    COUNT(CASE WHEN review_status = 'in_progress' THEN 1 END) as in_progress_reviews,
                    COUNT(CASE WHEN review_status = 'overdue' THEN 1 END) as overdue_reviews
                FROM checkable_documents
            """)
            checkable_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_elements,
                    COUNT(CASE WHEN element_type = 'text' THEN 1 END) as text_elements,
                    COUNT(CASE WHEN element_type = 'table' THEN 1 END) as table_elements,
                    COUNT(CASE WHEN element_type = 'figure' THEN 1 END) as figure_elements,
                    COUNT(CASE WHEN element_type = 'stamp' THEN 1 END) as stamp_elements
                FROM extracted_elements
            """)
            elements_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_norm_control_results,
                    COUNT(CASE WHEN analysis_status = 'completed' THEN 1 END) as completed_checks,
                    COUNT(CASE WHEN analysis_status = 'pending' THEN 1 END) as pending_checks,
                    COUNT(CASE WHEN analysis_status = 'error' THEN 1 END) as error_checks,
                    SUM(total_findings) as total_findings,
                    SUM(critical_findings) as critical_findings,
                    SUM(warning_findings) as warning_findings,
                    SUM(info_findings) as info_findings
                FROM norm_control_results
            """)
            norm_control_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç—á–µ—Ç–∞–º
            cursor.execute("""
                SELECT COUNT(*) as total_review_reports
                FROM review_reports
            """)
            reports_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞)
            cursor.execute("""
                SELECT 
                    COUNT(*) as documents_last_24h
                FROM uploaded_documents 
                WHERE upload_date >= NOW() - INTERVAL '24 hours'
            """)
            time_stats = cursor.fetchone()
            
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "metrics": {
                "documents": {
                    "total": doc_stats["total_documents"] or 0,
                    "completed": doc_stats["completed_documents"] or 0,
                    "pending": doc_stats["pending_documents"] or 0,
                    "error": doc_stats["error_documents"] or 0,
                    "by_type": {
                        "pdf": doc_stats["pdf_documents"] or 0,
                        "docx": doc_stats["docx_documents"] or 0,
                        "dwg": doc_stats["dwg_documents"] or 0,
                        "txt": doc_stats["txt_documents"] or 0
                    },
                    "total_size_bytes": doc_stats["total_size_bytes"] or 0,
                    "avg_file_size_bytes": float(doc_stats["avg_file_size_bytes"] or 0),
                    "total_tokens": doc_stats["total_tokens"] or 0
                },
                "checkable_documents": {
                    "total": checkable_stats["total_checkable_documents"] or 0,
                    "pending_reviews": checkable_stats["pending_reviews"] or 0,
                    "completed_reviews": checkable_stats["completed_reviews"] or 0,
                    "in_progress_reviews": checkable_stats["in_progress_reviews"] or 0,
                    "overdue_reviews": checkable_stats["overdue_reviews"] or 0
                },
                "elements": {
                    "total": elements_stats["total_elements"] or 0,
                    "text": elements_stats["text_elements"] or 0,
                    "table": elements_stats["table_elements"] or 0,
                    "figure": elements_stats["figure_elements"] or 0,
                    "stamp": elements_stats["stamp_elements"] or 0
                },
                "norm_control": {
                    "total_results": norm_control_stats["total_norm_control_results"] or 0,
                    "completed_checks": norm_control_stats["completed_checks"] or 0,
                    "pending_checks": norm_control_stats["pending_checks"] or 0,
                    "error_checks": norm_control_stats["error_checks"] or 0,
                    "total_findings": norm_control_stats["total_findings"] or 0,
                    "critical_findings": norm_control_stats["critical_findings"] or 0,
                    "warning_findings": norm_control_stats["warning_findings"] or 0,
                    "info_findings": norm_control_stats["info_findings"] or 0
                },
                "reports": {
                    "total": reports_stats["total_review_reports"] or 0
                },
                "performance": {
                    "documents_last_24h": time_stats["documents_last_24h"] or 0
                }
            }
        }
        
    except Exception as e:
        logger.error(f"Get metrics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def safe_text(text: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ PDF"""
    if text is None:
        return ""
    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏
    replacements = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya',
        '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
        '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
        '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
        '–§': 'F', '–•': 'H', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
        '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
    }
    
    result = ""
    for char in str(text):
        result += replacements.get(char, char)
    return result

def generate_docx_report_from_template(document: Dict, norm_control_result: Dict, page_results: List[Dict], review_report: Dict) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–∞ DOCX"""
    try:
        from docx import Document
        from docx.shared import Inches
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.oxml.shared import OxmlElement, qn
        from docx.oxml.ns import nsdecls
        from docx.oxml import parse_xml
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —à–∞–±–ª–æ–Ω
        template_path = "/app/report_format/–û–¢–ß–ï–¢_file_name.docx"
        doc = Document(template_path)
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        for paragraph in doc.paragraphs:
            # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã
            if "{{DOCUMENT_NAME}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{DOCUMENT_NAME}}", document['original_filename'])
            if "{{DOCUMENT_TYPE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{DOCUMENT_TYPE}}", document['file_type'].upper())
            if "{{FILE_SIZE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{FILE_SIZE}}", f"{document['file_size'] / 1024:.1f} KB")
            if "{{UPLOAD_DATE}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{UPLOAD_DATE}}", document['upload_date'].strftime("%d.%m.%Y %H:%M"))
            if "{{CATEGORY}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{CATEGORY}}", document['category'])
            if "{{STATUS}}" in paragraph.text:
                paragraph.text = paragraph.text.replace("{{STATUS}}", document['processing_status'])
            
            # –ó–∞–º–µ–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
            if norm_control_result:
                if "{{TOTAL_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{TOTAL_FINDINGS}}", str(norm_control_result['total_findings'] or 0))
                if "{{CRITICAL_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{CRITICAL_FINDINGS}}", str(norm_control_result['critical_findings'] or 0))
                if "{{WARNING_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{WARNING_FINDINGS}}", str(norm_control_result['warning_findings'] or 0))
                if "{{INFO_FINDINGS}}" in paragraph.text:
                    paragraph.text = paragraph.text.replace("{{INFO_FINDINGS}}", str(norm_control_result['info_findings'] or 0))
                if "{{ANALYSIS_DATE}}" in paragraph.text:
                    analysis_date = norm_control_result['analysis_date'].strftime("%d.%m.%Y %H:%M") if norm_control_result['analysis_date'] else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
                    paragraph.text = paragraph.text.replace("{{ANALYSIS_DATE}}", analysis_date)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        if page_results:
            doc.add_paragraph("–î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú", style='Heading 2')
            
            for page_result in page_results:
                page_num = page_result.get('page_number', 'N/A')
                status = page_result.get('status', 'N/A')
                findings_count = len(page_result.get('findings', []))
                
                p = doc.add_paragraph()
                p.add_run(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: ").bold = True
                p.add_run(f"–°—Ç–∞—Ç—É—Å: {status}, –ù–∞–π–¥–µ–Ω–æ –∑–∞–º–µ—á–∞–Ω–∏–π: {findings_count}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–º–µ—á–∞–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                findings = page_result.get('findings', [])
                for finding in findings:
                    finding_p = doc.add_paragraph()
                    finding_p.add_run(f"‚Ä¢ {finding.get('type', 'Unknown')}: ").bold = True
                    finding_p.add_run(finding.get('description', 'No description'))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return buffer.getvalue()
        
    except Exception as e:
        logger.error(f"Error generating DOCX report: {e}")
        # Fallback –∫ PDF –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        return generate_pdf_report(document, norm_control_result, page_results, review_report)

def extract_project_info_from_filename(filename: str) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    import re
    
    project_info = {
        'project_name': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
        'engineering_stage': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'document_mark': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'revision': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞',
        'page_count': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'
    }
    
    try:
        # –ü—Ä–∏–º–µ—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        # –ï110-0038-–£–ö–ö_24.848-–†–î-01-02.12.032-–ê–†_0_0_RU_IFC.pdf
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ö–ñ, –ö–ú, –ê–°, –¢–• –∏ —Ç.–¥.)
        mark_pattern = r'[–ê-–Ø]{2}'
        mark_match = re.search(mark_pattern, filename)
        if mark_match:
            project_info['document_mark'] = mark_match.group(0)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞
        project_pattern = r'(\d{4}-\d{4})'
        project_match = re.search(project_pattern, filename)
        if project_match:
            project_info['project_name'] = f"–ü—Ä–æ–µ–∫—Ç {project_match.group(1)}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–≤–∏–∑–∏—é
        revision_pattern = r'_(\d+)_(\d+)_'
        revision_match = re.search(revision_pattern, filename)
        if revision_match:
            project_info['revision'] = f"{revision_match.group(1)}.{revision_match.group(2)}"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–¥–∏—é –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—Ä–∫–µ
        if project_info['document_mark'] in ['–ö–ñ', '–ö–ú']:
            project_info['engineering_stage'] = '–ö–ñ/–ö–ú'
        elif project_info['document_mark'] in ['–ê–°', '–¢–•']:
            project_info['engineering_stage'] = '–ê–°/–¢–•'
        elif project_info['document_mark'] in ['–ö–°', '–ö–ü']:
            project_info['engineering_stage'] = '–ö–°/–ö–ü'
        
    except Exception as e:
        logger.error(f"Error extracting project info: {e}")
    
    return project_info

def group_findings_by_pages(page_results: List[Dict]) -> Dict[int, List[Dict]]:
    """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–º–µ—á–∞–Ω–∏–π –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
    page_findings = {}
    
    for finding in page_results:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∑–∞–º–µ—á–∞–Ω–∏—è
        page_num = finding.get('page_number', 1)
        if page_num not in page_findings:
            page_findings[page_num] = []
        page_findings[page_num].append(finding)
    
    return page_findings

def get_severity_text(severity_level: int) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–º–µ—á–∞–Ω–∏—è"""
    severity_map = {
        3: "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ",
        2: "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", 
        1: "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ"
    }
    return severity_map.get(severity_level, "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")

def generate_conclusion(norm_control_result: Dict, page_summary: Dict[int, List[Dict]]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    if not norm_control_result:
        return "–ê–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
    
    total_findings = norm_control_result.get('total_findings', 0)
    critical_findings = norm_control_result.get('critical_findings', 0)
    warning_findings = norm_control_result.get('warning_findings', 0)
    
    total_pages = len(page_summary)
    pages_with_critical = sum(1 for findings in page_summary.values() 
                             if any(f.get('severity_level') == 3 for f in findings))
    pages_with_warnings = sum(1 for findings in page_summary.values() 
                             if any(f.get('severity_level') in [1, 2] for f in findings))
    
    conclusion_parts = []
    
    if critical_findings > 0:
        conclusion_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {critical_findings} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –Ω–∞ {pages_with_critical} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –ø–µ—Ä–µ–¥ –ø—Ä–∏–Ω—è—Ç–∏–µ–º.")
    elif warning_findings > 0:
        conclusion_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {warning_findings} –∑–∞–º–µ—á–∞–Ω–∏–π –Ω–∞ {pages_with_warnings} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–Ω—è—Ç —Å —É—á–µ—Ç–æ–º —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–º–µ—á–∞–Ω–∏–π.")
    else:
        conclusion_parts.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
        conclusion_parts.append("–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø—Ä–∏–Ω—è—Ç–∏—é.")
    
    conclusion_parts.append(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    conclusion_parts.append(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–µ—á–∞–Ω–∏–π: {total_findings}")
    
    return " ".join(conclusion_parts)

def generate_pdf_report(document: Dict, norm_control_result: Dict, page_results: List[Dict], review_report: Dict) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    try:
        # –°–æ–∑–¥–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è PDF
        buffer = io.BytesIO()
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        story = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à—Ä–∏—Ñ—Ç
        font_name = 'Helvetica'
        
        # –°—Ç–∏–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontName=font_name,
            fontSize=18,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName=font_name,
            fontSize=14,
            spaceAfter=20,
            textColor=colors.darkblue
        )
        subheading_style = ParagraphStyle(
            'CustomSubHeading',
            parent=styles['Heading3'],
            fontName=font_name,
            fontSize=12,
            spaceAfter=15,
            textColor=colors.darkblue
        )
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10
        )
        small_style = ParagraphStyle(
            'CustomSmall',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=8
        )
        
        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
        filename = document.get('original_filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª')
        story.append(Paragraph(safe_text("–û–¢–ß–ï–¢ –û–ë –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ü–†–û–í–ï–†–ö–ï"), title_style))
        story.append(Paragraph(safe_text(f'"{filename}"'), title_style))
        story.append(Spacer(1, 30))
        
        # 2. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        story.append(Paragraph(safe_text("1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ï–ö–¢–ï –ò –î–û–ö–£–ú–ï–ù–¢–ï"), heading_style))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        project_info = extract_project_info_from_filename(filename)
        
        project_data = [
            [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"), safe_text(project_info.get('project_name', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'))],
            [safe_text("–°—Ç–∞–¥–∏—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"), safe_text(project_info.get('engineering_stage', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ú–∞—Ä–∫–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('document_mark', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–†–µ–≤–∏–∑–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"), safe_text(project_info.get('revision', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞'))],
            [safe_text("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"), safe_text(str(project_info.get('page_count', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')))],
            [safe_text("–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"), safe_text(filename)],
            [safe_text("–¢–∏–ø —Ñ–∞–π–ª–∞"), safe_text(document.get('file_type', '').upper())],
            [safe_text("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞"), safe_text(f"{document.get('file_size', 0) / 1024:.1f} KB")],
            [safe_text("–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏"), safe_text(document.get('upload_date', '').strftime("%d.%m.%Y %H:%M") if document.get('upload_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")],
            [safe_text("–î–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏"), safe_text(norm_control_result.get('analysis_date', '').strftime("%d.%m.%Y %H:%M") if norm_control_result and norm_control_result.get('analysis_date') else "–ù–µ —É–∫–∞–∑–∞–Ω–∞")]
        ]
        
        project_table = Table(project_data, colWidths=[2.5*inch, 3.5*inch])
        project_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, 0), font_name),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(project_table)
        story.append(Spacer(1, 20))
        
        # 3. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        story.append(Paragraph(safe_text("2. –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú"), heading_style))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_summary = group_findings_by_pages(page_results)
        
        summary_headers = [
            safe_text("‚Ññ —Å—Ç—Ä."), 
            safe_text("–ü—Ä–æ–≤–µ—Ä–µ–Ω–∞"), 
            safe_text("–ö—Ä–∏—Ç–∏—á."), 
            safe_text("–ó–∞–º–µ—á–∞–Ω–∏—è"), 
            safe_text("–í—ã–≤–æ–¥"), 
            safe_text("–°—Ç–∞—Ç—É—Å")
        ]
        
        summary_data = [summary_headers]
        total_critical = 0
        total_warnings = 0
        total_pages = len(page_summary)
        pages_approved = 0
        pages_rejected = 0
        
        for page_num, page_findings in sorted(page_summary.items()):
            critical_count = sum(1 for f in page_findings if f.get('severity_level') == 3)
            warning_count = sum(1 for f in page_findings if f.get('severity_level') in [1, 2])
            total_critical += critical_count
            total_warnings += warning_count
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if critical_count > 0:
                status = safe_text("–û—Ç–∫–ª–æ–Ω–µ–Ω–∞")
                page_status = safe_text("–ù–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
                pages_rejected += 1
            elif warning_count > 0:
                status = safe_text("–£—Å–ª–æ–≤–Ω–æ –ø—Ä–∏–Ω—è—Ç–∞")
                page_status = safe_text("–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
                pages_approved += 1
            else:
                status = safe_text("–ü—Ä–∏–Ω—è—Ç–∞")
                page_status = safe_text("–û–¥–æ–±—Ä–µ–Ω–∞")
                pages_approved += 1
            
            summary_data.append([
                safe_text(str(page_num)),
                safe_text("–î–∞"),
                safe_text(str(critical_count)),
                safe_text(str(warning_count)),
                safe_text(status),
                safe_text(page_status)
            ])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
        summary_data.append([
            safe_text("–ò–¢–û–ì–û"),
            safe_text(str(total_pages)),
            safe_text(str(total_critical)),
            safe_text(str(total_warnings)),
            safe_text(f"–ü—Ä–∏–Ω—è—Ç–æ: {pages_approved}, –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {pages_rejected}"),
            safe_text("" if total_critical == 0 else "–¢—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
        ])
        
        summary_table = Table(summary_data, colWidths=[0.8*inch, 1*inch, 0.8*inch, 1*inch, 1.5*inch, 1.5*inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), font_name),
            ('FONTSIZE', (0, 0), (-1, 0), 9),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -2), colors.beige),
            ('BACKGROUND', (0, -1), (-1, -1), colors.lightblue),
            ('FONTNAME', (0, -1), (-1, -1), font_name),
            ('FONTSIZE', (0, -1), (-1, -1), 9),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(summary_table)
        story.append(Spacer(1, 20))
        
        # 4. –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        story.append(Paragraph(safe_text("3. –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú"), heading_style))
        
        for page_num, page_findings in sorted(page_summary.items()):
            if page_findings:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∑–∞–º–µ—á–∞–Ω–∏—è–º–∏
                story.append(Paragraph(safe_text(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}"), subheading_style))
                
                for finding in page_findings:
                    severity_text = get_severity_text(finding.get('severity_level', 1))
                    clause_id = finding.get('clause_id', '–ù–µ —É–∫–∞–∑–∞–Ω')
                    location = finding.get('location', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
                    
                    finding_text = f"""
                    <b>–¢–∏–ø:</b> {finding.get('finding_type', '–ù–µ —É–∫–∞–∑–∞–Ω')} | 
                    <b>–í–∞–∂–Ω–æ—Å—Ç—å:</b> {severity_text} | 
                    <b>Clause ID:</b> {clause_id} | 
                    <b>–ú–µ—Å—Ç–æ:</b> {location}
                    <br/>
                    <b>–ù–∞–∑–≤–∞–Ω–∏–µ:</b> {finding.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {finding.get('description', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    <br/>
                    <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:</b> {finding.get('recommendation', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
                    """
                    
                    story.append(Paragraph(safe_text(finding_text), normal_style))
                    story.append(Spacer(1, 10))
        
        # 5. –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        story.append(Paragraph(safe_text("4. –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò"), heading_style))
        
        if norm_control_result:
            results_data = [
                [safe_text("–ü–∞—Ä–∞–º–µ—Ç—Ä"), safe_text("–ó–Ω–∞—á–µ–Ω–∏–µ")],
                [safe_text("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–µ—á–∞–Ω–∏–π"), safe_text(str(norm_control_result.get('total_findings', 0)))],
                [safe_text("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è"), safe_text(str(norm_control_result.get('critical_findings', 0)))],
                [safe_text("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"), safe_text(str(norm_control_result.get('warning_findings', 0)))],
                [safe_text("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è"), safe_text(str(norm_control_result.get('info_findings', 0)))],
                [safe_text("–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞"), safe_text(norm_control_result.get('analysis_status', '–ù–µ —É–∫–∞–∑–∞–Ω'))]
            ]
            
            results_table = Table(results_data, colWidths=[2.5*inch, 3.5*inch])
            results_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), font_name),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(results_table)
            story.append(Spacer(1, 20))
        
        # 6. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        story.append(Paragraph(safe_text("5. –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï"), heading_style))
        
        conclusion = generate_conclusion(norm_control_result, page_summary)
        story.append(Paragraph(safe_text(conclusion), normal_style))
        
        # 7. –ü–æ–¥–ø–∏—Å—å –∏ –¥–∞—Ç–∞
        story.append(Spacer(1, 30))
        story.append(Paragraph(safe_text(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}"), small_style))
        story.append(Paragraph(safe_text("–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è"), small_style))
        
        # –°—Ç—Ä–æ–∏–º PDF
        doc.build(story)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        pdf_content = buffer.getvalue()
        buffer.close()
        
        return pdf_content
        
    except Exception as e:
        logger.error(f"PDF generation error: {e}")
        raise

@app.get("/checkable-documents/{document_id}/download-report")
async def download_report_pdf(document_id: int):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document = parser.get_checkable_document(document_id)
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è
        norm_control_result = parser.get_norm_control_result_by_document_id(document_id)
        if not norm_control_result:
            raise HTTPException(status_code=404, detail="Norm control results not found")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_results = parser.get_page_results_by_document_id(document_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞
        review_report = parser.get_review_report_by_norm_control_id(norm_control_result['id'])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç
        report_content = generate_pdf_report(document, norm_control_result, page_results, review_report)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º PDF —Ñ–∞–π–ª
        from fastapi.responses import Response
        media_type = "application/pdf"
        filename = f"norm_control_report_{document_id}.pdf"
        
        return Response(
            content=report_content,
            media_type=media_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating PDF report for document {document_id}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ PostgreSQL —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º
        db_conn = parser.get_db_connection()
        with db_conn.cursor() as cursor:
            cursor.execute("SELECT 1")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        parser.qdrant_client.get_collections()
        
        return {"status": "healthy", "timestamp": datetime.now().isoformat()}
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "error": str(e)}
        )

# –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
@app.get("/test-prompt-templates")
async def test_prompt_templates():
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω–æ–≤"""
    return {"status": "success", "message": "Test endpoint works"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
