import logging
import requests
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
import re
import io

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class ChatDocumentService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —á–∞—Ç–µ —Å –ò–ò"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"  # Qdrant –≤ Docker
        self.RAG_SERVICE_URL = "http://ai-nk-rag-service-1:8003"  # RAG —Å–µ—Ä–≤–∏—Å
        self.DOCUMENT_PARSER_URL = "http://ai-nk-document-parser-1:8001"  # Document Parser
        self.CHAT_COLLECTION = "chat_documents"
        self.VECTOR_SIZE = 1024  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ BGE-M3
        self.MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
        self.SUPPORTED_FORMATS = [
            'application/pdf',
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'application/msword',
            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'application/vnd.ms-excel',
            'text/plain',
            'text/markdown'
        ]
        
        logger.info("üöÄ [CHAT_DOCUMENT_SERVICE] Chat Document Service initialized")
    
    def validate_file(self, file_content: bytes, file_type: str, file_name: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = len(file_content)
            if file_size > self.MAX_FILE_SIZE:
                return {
                    "valid": False,
                    "error": f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {self.MAX_FILE_SIZE // (1024*1024)}MB"
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
            if file_type not in self.SUPPORTED_FORMATS:
                return {
                    "valid": False,
                    "error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_type}"
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            file_extension = os.path.splitext(file_name)[1].lower()
            allowed_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt', '.md']
            if file_extension not in allowed_extensions:
                return {
                    "valid": False,
                    "error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_extension}"
                }
            
            return {
                "valid": True,
                "file_size": file_size,
                "file_type": file_type,
                "file_extension": file_extension
            }
            
        except Exception as e:
            logger.error(f"‚ùå [FILE_VALIDATION] Error validating file: {e}")
            return {
                "valid": False,
                "error": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}"
            }
    
    def process_document(self, file_content: bytes, file_type: str, file_name: str, user_message: str = "") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —á–∞—Ç–∞"""
        try:
            logger.info(f"üìÑ [DOCUMENT_PROCESSING] Processing document: {file_name}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
            validation = self.validate_file(file_content, file_type, file_name)
            if not validation["valid"]:
                return {
                    "success": False,
                    "error": validation["error"]
                }
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_hash = hashlib.sha256(file_content).hexdigest()
            document_id = f"chat_{file_hash[:16]}"
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ Document Parser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            parser_response = self._extract_text_from_document(file_content, file_type, file_name)
            if not parser_response["success"]:
                return {
                    "success": False,
                    "error": f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {parser_response['error']}"
                }
            
            extracted_text = parser_response["text"]
            chunks = parser_response.get("chunks", [])
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ Qdrant
            indexing_result = self._index_document_in_qdrant(
                document_id, file_name, extracted_text, chunks, user_message
            )
            
            if not indexing_result["success"]:
                return {
                    "success": False,
                    "error": f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {indexing_result['error']}"
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò
            ai_context = self._prepare_ai_context(file_name, extracted_text, user_message)
            
            return {
                "success": True,
                "document_id": document_id,
                "file_name": file_name,
                "file_size": validation["file_size"],
                "extracted_text": extracted_text,
                "chunks_count": len(chunks),
                "ai_context": ai_context,
                "indexed": True
            }
            
        except Exception as e:
            logger.error(f"‚ùå [DOCUMENT_PROCESSING] Error processing document: {e}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"
            }
    
    def _extract_text_from_document(self, file_content: bytes, file_type: str, file_name: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Document Parser –∏–ª–∏ –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É"""
        try:
            # –î–ª—è TXT —Ñ–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
            if file_type == 'text/plain' or file_name.lower().endswith('.txt'):
                logger.info(f"üîç [TEXT_EXTRACTION] Processing TXT file directly: {file_name}")
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                try:
                    text = file_content.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        text = file_content.decode('cp1251')  # –ü–æ–ø—ã—Ç–∫–∞ —Å Windows-1251
                    except UnicodeDecodeError:
                        text = file_content.decode('latin-1')  # Fallback
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                chunk_size = 500
                chunks = []
                for i in range(0, len(text), chunk_size):
                    chunk_text = text[i:i + chunk_size]
                    chunks.append({
                        'content': chunk_text,
                        'page': 1,
                        'section': f'chunk_{i//chunk_size + 1}'
                    })
                
                logger.info(f"‚úÖ [TEXT_EXTRACTION] TXT file processed directly: {len(chunks)} chunks")
                return {
                    "success": True,
                    "text": text,
                    "chunks": chunks,
                    "pages": 1
                }
            
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º Document Parser
            logger.info(f"üîç [TEXT_EXTRACTION] Using Document Parser for: {file_name}")
            files = {'file': (file_name, io.BytesIO(file_content), file_type)}
            data = {'extract_text': 'true', 'chunk_size': '500'}
            
            response = requests.post(
                f"{self.DOCUMENT_PARSER_URL}/upload/checkable",
                files=files,
                data=data,
                timeout=300  # 5 –º–∏–Ω—É—Ç
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "text": result.get("extracted_text", ""),
                    "chunks": result.get("chunks", []),
                    "pages": result.get("pages", 1)
                }
            else:
                return {
                    "success": False,
                    "error": f"Document Parser error: {response.status_code} - {response.text}"
                }
                
        except Exception as e:
            logger.error(f"‚ùå [TEXT_EXTRACTION] Error extracting text: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _index_document_in_qdrant(self, document_id: str, file_name: str, text: str, chunks: List[Dict], user_message: str) -> Dict[str, Any]:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é chat_documents"""
        try:
            logger.info(f"üîç [QDRANT_INDEXING] Starting indexing for document {document_id}")
            logger.info(f"üîç [QDRANT_INDEXING] Document: {file_name}, chunks: {len(chunks)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            self._ensure_chat_collection_exists()
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤
            logger.info(f"üîç [QDRANT_INDEXING] Creating embeddings for {len(chunks)} chunks...")
            embeddings = self._create_chunk_embeddings(chunks)
            logger.info(f"üîç [QDRANT_INDEXING] Created {len(embeddings)} embeddings")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
            points = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                if len(embedding) != self.VECTOR_SIZE:
                    logger.warning(f"‚ö†Ô∏è [QDRANT_INDEXING] Chunk {i} has wrong embedding size: {len(embedding)}, expected {self.VECTOR_SIZE}")
                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                    if len(embedding) > self.VECTOR_SIZE:
                        embedding = embedding[:self.VECTOR_SIZE]
                    else:
                        embedding.extend([0.0] * (self.VECTOR_SIZE - len(embedding)))
                
                point = {
                    'ids': [hash(f"{document_id}_{i}") % (2**63 - 1)],
                    'vectors': [embedding],
                    'payloads': [{
                        'document_id': document_id,
                        'file_name': file_name,
                        'chunk_index': i,
                        'content': chunk.get('content', ''),
                        'page': chunk.get('page', 1),
                        'section': chunk.get('section', ''),
                        'user_message': user_message,
                        'timestamp': datetime.now().isoformat(),
                        'chunk_type': 'chat_document'
                    }]
                }
                points.append(point)
                logger.debug(f"üîç [QDRANT_INDEXING] Prepared point {i}: id={point['ids'][0]}, content_length={len(point['payloads'][0]['content'])}")
            
            logger.info(f"üîç [QDRANT_INDEXING] Prepared {len(points)} points for Qdrant")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
            logger.info(f"üîç [QDRANT_INDEXING] Sending points to Qdrant...")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å —Ñ–æ—Ä–º–∞—Ç–æ–º
            for i, point in enumerate(points):
                try:
                    response = requests.post(
                        f"{self.QDRANT_URL}/collections/{self.CHAT_COLLECTION}/points",
                        json=point,
                        timeout=30
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"‚ùå [QDRANT_INDEXING] Failed to add point {i}: {response.status_code} - {response.text}")
                        return {
                            "success": False,
                            "error": f"Qdrant indexing error for point {i}: {response.status_code} - {response.text}"
                        }
                    else:
                        logger.debug(f"‚úÖ [QDRANT_INDEXING] Point {i} added successfully")
                        
                except Exception as e:
                    logger.error(f"‚ùå [QDRANT_INDEXING] Error adding point {i}: {e}")
                    return {
                        "success": False,
                        "error": f"Error adding point {i}: {str(e)}"
                    }
            
            logger.info(f"‚úÖ [QDRANT_INDEXING] Indexed {len(points)} chunks for document {document_id}")
            return {"success": True, "chunks_indexed": len(points)}
                
        except Exception as e:
            logger.error(f"‚ùå [QDRANT_INDEXING] Error indexing in Qdrant: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e)
            }
    
    def _ensure_chat_collection_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ chat_documents –≤ Qdrant –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            response = requests.get(f"{self.QDRANT_URL}/collections/{self.CHAT_COLLECTION}")
            
            if response.status_code == 404:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
                collection_config = {
                    "vectors": {
                        "size": self.VECTOR_SIZE,
                        "distance": "Cosine"
                    },
                    "on_disk_payload": True
                }
                
                create_response = requests.put(
                    f"{self.QDRANT_URL}/collections/{self.CHAT_COLLECTION}",
                    json=collection_config
                )
                
                if create_response.status_code == 200:
                    logger.info(f"‚úÖ [QDRANT_COLLECTION] Created collection {self.CHAT_COLLECTION}")
                else:
                    logger.error(f"‚ùå [QDRANT_COLLECTION] Failed to create collection: {create_response.status_code}")
            
        except Exception as e:
            logger.error(f"‚ùå [QDRANT_COLLECTION] Error ensuring collection exists: {e}")
    
    def _create_chunk_embeddings(self, chunks: List[Dict]) -> List[List[float]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ RAG —Å–µ—Ä–≤–∏—Å"""
        try:
            logger.info(f"üîç [EMBEDDING] Starting embedding creation for {len(chunks)} chunks")
            embeddings = []
            
            for i, chunk in enumerate(chunks):
                content = chunk.get('content', '')
                logger.debug(f"üîç [EMBEDDING] Processing chunk {i}: content_length={len(content)}")
                
                if not content.strip():
                    # –ü—É—Å—Ç–æ–π —á–∞–Ω–∫ - —Å–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥
                    logger.warning(f"‚ö†Ô∏è [EMBEDDING] Chunk {i} is empty, creating zero embedding")
                    embeddings.append([0.0] * self.VECTOR_SIZE)
                    continue
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                logger.debug(f"üîç [EMBEDDING] Sending embedding request for chunk {i}")
                response = requests.post(
                    f"{self.RAG_SERVICE_URL}/api/embeddings",
                    json={'text': content},
                    timeout=30
                )
                
                logger.debug(f"üîç [EMBEDDING] Chunk {i} embedding response status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    embedding = result.get('embedding', [])
                    logger.debug(f"üîç [EMBEDDING] Chunk {i} embedding size: {len(embedding)}")
                    
                    if embedding:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–∞
                        if all(isinstance(x, (int, float)) for x in embedding):
                            embeddings.append(embedding)
                            logger.debug(f"üîç [EMBEDDING] Chunk {i} embedding added successfully")
                        else:
                            logger.warning(f"‚ö†Ô∏è [EMBEDDING] Chunk {i} embedding contains non-numeric values, using zero embedding")
                            embeddings.append([0.0] * self.VECTOR_SIZE)
                    else:
                        logger.warning(f"‚ö†Ô∏è [EMBEDDING] Chunk {i} empty embedding, using zero embedding")
                        embeddings.append([0.0] * self.VECTOR_SIZE)
                else:
                    logger.warning(f"‚ö†Ô∏è [EMBEDDING] Failed to create embedding for chunk {i}: {response.status_code} - {response.text}")
                    embeddings.append([0.0] * self.VECTOR_SIZE)
            
            logger.info(f"‚úÖ [EMBEDDING] Created {len(embeddings)} embeddings successfully")
            return embeddings
            
        except Exception as e:
            logger.error(f"‚ùå [EMBEDDING] Error creating embeddings: {e}", exc_info=True)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            logger.warning(f"‚ö†Ô∏è [EMBEDDING] Returning zero embeddings due to error")
            return [[0.0] * self.VECTOR_SIZE] * len(chunks)
    
    def _prepare_ai_context(self, file_name: str, text: str, user_message: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ò–ò"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            max_context_length = 8000  # ~4000 —Ç–æ–∫–µ–Ω–æ–≤
            
            if len(text) > max_context_length:
                # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞
                half_length = max_context_length // 2
                text = text[:half_length] + "\n\n... [—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤] ...\n\n" + text[-half_length:]
            
            context = f"""üìÑ –§–∞–π–ª: {file_name}

üìã –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞:
{text}

üí¨ –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message or '–û–±—Ä–∞–±–æ—Ç–∞–π —ç—Ç–æ—Ç —Ñ–∞–π–ª'}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ï—Å–ª–∏ –≤ —Ñ–∞–π–ª–µ –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Å–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –æ–ø–∏—à–∏ –∏—Ö –ø–æ–¥—Ä–æ–±–Ω–æ."""
            
            return context
            
        except Exception as e:
            logger.error(f"‚ùå [AI_CONTEXT] Error preparing AI context: {e}")
            return f"üìÑ –§–∞–π–ª: {file_name}\n\nüí¨ –ó–∞–ø—Ä–æ—Å: {user_message or '–û–±—Ä–∞–±–æ—Ç–∞–π —ç—Ç–æ—Ç —Ñ–∞–π–ª'}"
    
    def search_chat_documents(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —á–∞—Ç–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            response = requests.post(
                f"{self.RAG_SERVICE_URL}/api/embeddings",
                json={'text': query},
                timeout=30
            )
            
            if response.status_code != 200:
                return []
            
            result = response.json()
            query_embedding = result.get('embedding', [])
            
            if not query_embedding:
                return []
            
            # –ü–æ–∏—Å–∫ –≤ Qdrant
            search_response = requests.post(
                f"{self.QDRANT_URL}/collections/{self.CHAT_COLLECTION}/points/search",
                json={
                    'vector': query_embedding,
                    'limit': limit,
                    'with_payload': True
                },
                timeout=30
            )
            
            if search_response.status_code == 200:
                search_result = search_response.json()
                return search_result.get('result', [])
            else:
                return []
                
        except Exception as e:
            logger.error(f"‚ùå [CHAT_SEARCH] Error searching chat documents: {e}")
            return []
    
    def get_chat_documents_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–∞—Ç–∞"""
        try:
            response = requests.get(f"{self.QDRANT_URL}/collections/{self.CHAT_COLLECTION}")
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "total_documents": result['result']['points_count'],
                    "indexed_vectors": result['result']['indexed_vectors_count'],
                    "collection_status": result['result']['status'],
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "total_documents": 0,
                    "indexed_vectors": 0,
                    "collection_status": "unknown",
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå [CHAT_STATS] Error getting chat documents stats: {e}")
            return {
                "total_documents": 0,
                "indexed_vectors": 0,
                "collection_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
