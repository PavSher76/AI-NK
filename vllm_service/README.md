# VLLM + Ollama Integration Service

## Описание

VLLM + Ollama Integration Service - это специализированный сервис, который обеспечивает интеграцию между системой AI-NK и локально установленным Ollama. Сервис предоставляет единый API для работы с языковыми моделями, включая проверку статуса, генерацию ответов и мониторинг.

## Архитектура

```
Frontend → Gateway → VLLM Service → Ollama → Local Models
```

### Компоненты

- **VLLM Service** (порт 8005): Основной сервис интеграции
- **Ollama** (порт 11434): Локальный сервис для запуска моделей
- **Gateway**: Маршрутизация запросов к VLLM сервису

## Поддерживаемые модели

### Текущие модели
1. **GPT-OSS 20B** - основная модель для генерации ответов
   - Размер: ~13.8GB
   - Формат: GGUF
   - Квантизация: MXFP4

2. **BGE-M3** - модель для эмбеддингов и поиска
   - Размер: ~1.2GB
   - Формат: GGUF
   - Квантизация: F16

3. **GPT-OSS:latest** - последняя версия GPT-OSS

## API Эндпоинты

### 1. Корневой эндпоинт
```
GET /
```
Возвращает информацию о сервисе и доступных эндпоинтах.

### 2. Проверка здоровья
```
GET /health
```
Проверяет состояние сервиса и подключение к Ollama.

### 3. Список моделей
```
GET /models
```
Возвращает список доступных моделей Ollama с детальной информацией.

### 4. Генерация ответов
```
POST /chat
```
Генерирует ответы через выбранную модель.

**Параметры запроса:**
```json
{
  "message": "Текст сообщения",
  "model": "gpt-oss:20b",
  "history": [
    {"role": "user", "content": "Предыдущее сообщение"},
    {"role": "assistant", "content": "Предыдущий ответ"}
  ],
  "max_tokens": 2048
}
```

**Ответ:**
```json
{
  "status": "success",
  "response": "Сгенерированный ответ",
  "model": "gpt-oss:20b",
  "prompt_tokens": 82,
  "response_tokens": 108,
  "total_tokens": 190,
  "generation_time_ms": 14237.4,
  "timestamp": "2025-09-02T06:41:14.857259"
}
```

### 5. Статистика
```
GET /stats
```
Возвращает статистику сервиса и конфигурацию.

## Конфигурация

### Переменные окружения

| Переменная | Описание | Значение по умолчанию |
|------------|----------|----------------------|
| `OLLAMA_BASE_URL` | URL Ollama сервиса | `http://localhost:11434` |
| `OLLAMA_MAX_TOKENS` | Максимальное количество токенов | `2048` |
| `OLLAMA_TEMPERATURE` | Температура генерации | `0.7` |
| `OLLAMA_TOP_P` | Top-p параметр | `0.9` |
| `OLLAMA_TIMEOUT` | Таймаут запросов (секунды) | `120` |
| `OLLAMA_CACHE_DURATION` | Длительность кэша статуса | `30` |
| `LOG_LEVEL` | Уровень логирования | `INFO` |
| `HOST` | Хост для привязки | `0.0.0.0` |
| `PORT` | Порт сервиса | `8005` |
| `RELOAD` | Автоперезагрузка | `false` |

## Запуск

### 1. Локальный запуск
```bash
cd vllm_service
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python main.py
```

### 2. Docker запуск
```bash
# Сборка образа
docker-compose build vllm

# Запуск контейнера
docker-compose up -d vllm

# Просмотр логов
docker-compose logs -f vllm
```

### 3. Автоматический запуск
```bash
./scripts/start_vllm.sh
```

## Мониторинг

### Health Checks
Сервис автоматически проверяет:
- Доступность Ollama
- Наличие ключевых моделей
- Время ответа API

### Логирование
- **Уровни**: DEBUG, INFO, WARNING, ERROR
- **Формат**: Структурированные логи с эмодзи
- **Категории**: HEALTH, MODELS, CHAT, STATS

### Метрики
- Время генерации ответов
- Количество токенов
- Статус подключений
- Доступность моделей

## Интеграция с Gateway

Gateway автоматически маршрутизирует запросы к VLLM сервису:

```python
"vllm": "http://vllm:8005"  # Внутренний адрес контейнера
```

### Публичные эндпоинты
- `/api/chat` - чат с ИИ
- `/api/chat/tags` - список моделей
- `/api/ntd-consultation/chat` - консультации НТД

## Устранение неполадок

### 1. Ollama недоступен
```bash
# Проверка статуса Ollama
curl http://localhost:11434/api/tags

# Запуск Ollama
ollama serve
```

### 2. VLLM сервис не отвечает
```bash
# Проверка статуса контейнера
docker ps | grep vllm

# Просмотр логов
docker logs ai-nk-vllm-1

# Перезапуск
docker-compose restart vllm
```

### 3. Проблемы с сетью
```bash
# Проверка внутри контейнера
docker exec -it ai-nk-vllm-1 curl http://host.docker.internal:11434/api/tags

# Проверка health check
curl http://localhost:8005/health
```

## Производительность

### Оптимизации
- Кэширование статуса Ollama (30 секунд)
- Асинхронная обработка запросов
- Настраиваемые таймауты
- Логирование производительности

### Ограничения
- Таймаут генерации: 120 секунд
- Максимум токенов: 2048
- Поддержка истории разговора
- Fallback при ошибках

## Разработка

### Структура проекта
```
vllm_service/
├── main.py                    # FastAPI приложение
├── vllm_ollama_service.py    # Основная логика
├── requirements.txt           # Зависимости
├── Dockerfile                # Docker образ
├── .dockerignore            # Исключения для Docker
└── README.md                # Документация
```

### Добавление новых функций
1. Обновить `vllm_ollama_service.py`
2. Добавить новые эндпоинты в `main.py`
3. Обновить документацию
4. Протестировать через Gateway

## Лицензия

Проект AI-NK - внутренняя разработка.

## Поддержка

Для получения поддержки обратитесь к команде разработки AI-NK.
