from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
import logging
import os
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
from logging_config import setup_general_logging, get_log_stats

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—â–µ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
setup_general_logging()
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–æ–≤
from vllm_ollama_service import OllamaService
from chat_document_service import ChatDocumentService

# –ú–æ–¥–µ–ª–∏ Pydantic
class ChatRequest(BaseModel):
    message: str
    model: str = "llama3.1:8b"
    history: Optional[List[Dict[str, str]]] = None
    max_tokens: Optional[int] = None

class ChatResponse(BaseModel):
    status: str
    response: str
    model: str
    timestamp: str
    tokens_used: Optional[int] = None
    generation_time_ms: Optional[float] = None

class DocumentUploadResponse(BaseModel):
    status: str
    message: str
    document_id: Optional[str] = None
    file_name: Optional[str] = None
    chunks_count: Optional[int] = None
    error: Optional[str] = None

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="VLLM + Ollama Integration Service",
    description="–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM —Å –ª–æ–∫–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º Ollama",
    version="1.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
ollama_service = OllamaService()
chat_document_service = ChatDocumentService()

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
# ============================================================================

@app.get("/")
async def root_endpoint():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "VLLM + Ollama Integration Service",
        "version": "1.0.0",
        "description": "–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM —Å –ª–æ–∫–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º Ollama",
        "endpoints": {
            "health": "/health",
            "models": "/models",
            "chat": "/chat",
            "stats": "/stats",
            "logs_stats": "/logs/stats",
            "upload_document": "/upload_document",
            "chat_documents_stats": "/chat_documents_stats"
        },
        "configuration": {
            "ollama_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            "max_tokens": os.getenv("OLLAMA_MAX_TOKENS", "2048"),
            "temperature": os.getenv("OLLAMA_TEMPERATURE", "0.7"),
            "timeout": os.getenv("OLLAMA_TIMEOUT", "120"),
            "max_file_size": "100MB"
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_endpoint():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        health_status = ollama_service.health_check()
        return health_status
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/models")
async def models_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        models = ollama_service.get_ollama_models()
        return models
    except Exception as e:
        logger.error(f"‚ùå [MODELS] Error getting models: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama"""
    try:
        logger.info(f"üí¨ [CHAT] Processing chat request with model: {request.model}")
        
        response = ollama_service.generate_response_with_ollama(
            message=request.message,
            model_name=request.model,
            history=request.history,
            max_tokens=request.max_tokens
        )
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå [CHAT] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload_document")
async def upload_document_endpoint(
    file: UploadFile = File(...),
    message: str = Form("")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —á–∞—Ç–∞"""
    try:
        logger.info(f"üìÑ [UPLOAD_DOCUMENT] Processing document: {file.filename}")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        file_content = await file.read()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        result = chat_document_service.process_document(
            file_content=file_content,
            file_type=file.content_type or "application/octet-stream",
            file_name=file.filename or "unknown",
            user_message=message
        )
        
        if result["success"]:
            return DocumentUploadResponse(
                status="success",
                message=f"–î–æ–∫—É–º–µ–Ω—Ç {file.filename} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω",
                document_id=result["document_id"],
                file_name=result["file_name"],
                chunks_count=result["chunks_count"]
            )
        else:
            return DocumentUploadResponse(
                status="error",
                message="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                error=result["error"]
            )
            
    except Exception as e:
        logger.error(f"‚ùå [UPLOAD_DOCUMENT] Error processing document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat_with_document")
async def chat_with_document_endpoint(
    file: UploadFile = File(...),
    message: str = Form(""),
    model: str = Form("llama3.1:8b")
):
    """–ß–∞—Ç —Å –ò–ò —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
    try:
        logger.info(f"üí¨üìÑ [CHAT_WITH_DOCUMENT] Processing document: {file.filename}")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        file_content = await file.read()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        result = chat_document_service.process_document(
            file_content=file_content,
            file_type=file.content_type or "application/octet-stream",
            file_name=file.filename or "unknown",
            user_message=message
        )
        
        if not result["success"]:
            logger.error(f"‚ùå [CHAT_WITH_DOCUMENT] Document processing failed: {result['error']}")
            raise HTTPException(status_code=400, detail=result["error"])
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
        ai_response = ollama_service.generate_response_with_ollama(
            message=result["ai_context"],
            model_name=model,
            history=None,
            max_tokens=None
        )
        
        return {
            "status": "success",
            "document_processed": True,
            "document_id": result["document_id"],
            "file_name": result["file_name"],
            "chunks_count": result["chunks_count"],
            "ai_response": ai_response
        }
        
    except HTTPException as e:
        logger.error(f"‚ùå [CHAT_WITH_DOCUMENT] HTTP Error: {e.detail}")
        raise e
    except Exception as e:
        logger.error(f"‚ùå [CHAT_WITH_DOCUMENT] Unexpected error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/chat_documents_stats")
async def chat_documents_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–∞—Ç–∞"""
    try:
        stats = chat_document_service.get_chat_documents_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå [CHAT_DOCUMENTS_STATS] Error getting stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        stats = ollama_service.get_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå [STATS] Error getting stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/logs/stats")
async def logs_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ª–æ–≥–æ–≤"""
    try:
        log_stats = get_log_stats()
        return {
            "status": "success",
            "log_statistics": log_stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [LOGS_STATS] Error getting log stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞
# ============================================================================

if __name__ == "__main__":
    logger.info("üöÄ [VLLM_OLLAMA_SERVICE] Starting VLLM + Ollama Integration Service...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
    try:
        import requests
        ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        response = requests.get(f"{ollama_url}/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            bge_m3_available = any("bge-m3" in model.get("name", "") for model in models)
            gpt_oss_available = any("gpt-oss" in model.get("name", "") for model in models)
            
            if bge_m3_available:
                logger.info("‚úÖ [VLLM_OLLAMA_SERVICE] BGE-M3 model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [VLLM_OLLAMA_SERVICE] BGE-M3 model not found in Ollama")
                
            if gpt_oss_available:
                logger.info("‚úÖ [VLLM_OLLAMA_SERVICE] GPT-OSS model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [VLLM_OLLAMA_SERVICE] GPT-OSS model not found in Ollama")
                
            logger.info(f"üìä [VLLM_OLLAMA_SERVICE] Total models available: {len(models)}")
        else:
            logger.error(f"‚ùå [VLLM_OLLAMA_SERVICE] Cannot connect to Ollama at {ollama_url}")
    except Exception as e:
        logger.error(f"‚ùå [VLLM_OLLAMA_SERVICE] Error checking Ollama: {e}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8005"))
    reload = os.getenv("RELOAD", "false").lower() == "true"
    log_level = os.getenv("LOG_LEVEL", "INFO")
    
    logger.info(f"üîß [VLLM_OLLAMA_SERVICE] Configuration: host={host}, port={port}, reload={reload}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level=log_level.lower()
    )
