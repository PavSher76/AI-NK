from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
import logging
import os
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_level = os.getenv("LOG_LEVEL", "INFO")
logging.basicConfig(
    level=getattr(logging, log_level.upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–∞
from vllm_ollama_service import OllamaService

# –ú–æ–¥–µ–ª–∏ Pydantic
class ChatRequest(BaseModel):
    message: str
    model: str = "gpt-oss:20b"
    history: Optional[List[Dict[str, str]]] = None
    max_tokens: Optional[int] = None

class ChatResponse(BaseModel):
    status: str
    response: str
    model: str
    timestamp: str
    tokens_used: Optional[int] = None
    generation_time_ms: Optional[float] = None

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="VLLM + Ollama Integration Service",
    description="–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM —Å –ª–æ–∫–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º Ollama",
    version="1.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
ollama_service = OllamaService()

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
# ============================================================================

@app.get("/")
async def root_endpoint():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "VLLM + Ollama Integration Service",
        "version": "1.0.0",
        "description": "–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM —Å –ª–æ–∫–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º Ollama",
        "endpoints": {
            "health": "/health",
            "models": "/models",
            "chat": "/chat",
            "stats": "/stats"
        },
        "configuration": {
            "ollama_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            "max_tokens": os.getenv("OLLAMA_MAX_TOKENS", "2048"),
            "temperature": os.getenv("OLLAMA_TEMPERATURE", "0.7"),
            "timeout": os.getenv("OLLAMA_TIMEOUT", "120")
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_endpoint():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        health_status = ollama_service.health_check()
        return health_status
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/models")
async def models_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        models = ollama_service.get_ollama_models()
        return models
    except Exception as e:
        logger.error(f"‚ùå [MODELS] Error getting models: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama"""
    try:
        logger.info(f"üí¨ [CHAT] Processing chat request with model: {request.model}")
        
        response = ollama_service.generate_response_with_ollama(
            message=request.message,
            model_name=request.model,
            history=request.history,
            max_tokens=request.max_tokens
        )
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå [CHAT] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        stats = ollama_service.get_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå [STATS] Error getting stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞
# ============================================================================

if __name__ == "__main__":
    logger.info("üöÄ [VLLM_OLLAMA_SERVICE] Starting VLLM + Ollama Integration Service...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
    try:
        import requests
        ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        response = requests.get(f"{ollama_url}/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            bge_m3_available = any("bge-m3" in model.get("name", "") for model in models)
            gpt_oss_available = any("gpt-oss" in model.get("name", "") for model in models)
            
            if bge_m3_available:
                logger.info("‚úÖ [VLLM_OLLAMA_SERVICE] BGE-M3 model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [VLLM_OLLAMA_SERVICE] BGE-M3 model not found in Ollama")
                
            if gpt_oss_available:
                logger.info("‚úÖ [VLLM_OLLAMA_SERVICE] GPT-OSS model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [VLLM_OLLAMA_SERVICE] GPT-OSS model not found in Ollama")
                
            logger.info(f"üìä [VLLM_OLLAMA_SERVICE] Total models available: {len(models)}")
        else:
            logger.error(f"‚ùå [VLLM_OLLAMA_SERVICE] Cannot connect to Ollama at {ollama_url}")
    except Exception as e:
        logger.error(f"‚ùå [VLLM_OLLAMA_SERVICE] Error checking Ollama: {e}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8005"))
    reload = os.getenv("RELOAD", "false").lower() == "true"
    
    logger.info(f"üîß [VLLM_OLLAMA_SERVICE] Configuration: host={host}, port={port}, reload={reload}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level=log_level.lower()
    )
