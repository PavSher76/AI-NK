import logging
import asyncio
from typing import List, Dict, Any, Optional
import httpx
from datetime import datetime
import numpy as np
import requests
# from sentence_transformers import SentenceTransformer  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
import hashlib
import json
from functools import lru_cache
import time

logger = logging.getLogger(__name__)

class NTDConsultationService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    
    def __init__(self, db_connection, qdrant_client, ollama_url: str = "http://vllm:8000", rag_service=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î
        
        Args:
            db_connection: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
            qdrant_client: –ö–ª–∏–µ–Ω—Ç Qdrant
            ollama_url: URL VLLM Adapter (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é http://vllm:8000)
            rag_service: –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.db_conn = db_connection
        self.qdrant_client = qdrant_client
        self.ollama_url = ollama_url
        self.collection_name = "normative_documents"
        self.rag_service = rag_service  # –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π RAG —Å–µ—Ä–≤–∏—Å
        self.embedding_model = None
        self.cache = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.cache_ttl = 3600  # TTL –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (1 —á–∞—Å)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.SEARCH_LIMIT = 8  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.MAX_CONTEXT_LENGTH = 800  # –£–≤–µ–ª–∏—á–µ–Ω–∞ –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.MIN_CONTEXT_LENGTH = 200  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.CONFIDENCE_THRESHOLD = 0.3  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LLM
        self.MODEL_NAME = "llama3.1:8b"
        self.TEMPERATURE = 0.7
        self.MAX_TOKENS = 2500  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        
        self._load_embedding_model()
        
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ RAG —Å–µ—Ä–≤–∏—Å—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –º–æ–¥–µ–ª—å
            if self.rag_service and hasattr(self.rag_service, 'embedding_model') and self.rag_service.embedding_model:
                self.embedding_model = self.rag_service.embedding_model
                logger.info("‚úÖ [NTD_CONSULTATION] Using embedding model from RAG service")
            else:
                # –ò–Ω–∞—á–µ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                logger.info("üîß [NTD_CONSULTATION] Using simple hash embeddings for testing...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                self.embedding_model = None
                logger.info(f"‚úÖ [NTD_CONSULTATION] Simple hash embeddings ready (1024 dimensions)")
                
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error loading embedding model: {e}")
            self.embedding_model = None
    
    def _get_cache_key(self, message: str, history: List[Dict[str, str]] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∏—Å—Ç–æ—Ä–∏–∏
        cache_data = {
            "message": message.lower().strip(),
            "history": history or []
        }
        cache_string = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(cache_string.encode('utf-8')).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if time.time() - cached_data['timestamp'] < self.cache_ttl:
                logger.info(f"üìã [NTD_CONSULTATION] Cache hit for query")
                return cached_data['response']
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à
                del self.cache[cache_key]
        return None
    
    def _save_to_cache(self, cache_key: str, response: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∫—ç—à"""
        self.cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }
        logger.info(f"üíæ [NTD_CONSULTATION] Response cached")
        
    async def get_consultation(self, message: str, history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        
        Args:
            message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
            
        Returns:
            Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        """
        start_time = time.time()
        
        try:
            logger.info(f"üîç [NTD_CONSULTATION] Processing question: {message[:100]}...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = self._get_cache_key(message, history)
            cached_response = self._get_from_cache(cache_key)
            if cached_response:
                cached_response['cached'] = True
                return cached_response
            
            # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = await self._search_relevant_documents(message)
            
            if not relevant_docs:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] No relevant documents found")
                response = {
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "processing_time": round(time.time() - start_time, 3)
                }
                self._save_to_cache(cache_key, response)
                return response
            
            # 2. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = self._build_dynamic_context(relevant_docs, message)
            
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
            response = await self._generate_response(message, context, history)
            
            # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = self._prepare_sources(relevant_docs)
            
            # 5. –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(relevant_docs, response)
            
            # 6. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            final_response = {
                "response": response,
                "sources": sources,
                "confidence": confidence,
                "documents_used": len(relevant_docs),
                "processing_time": round(time.time() - start_time, 3),
                "context_length": len(context),
                "cached": False
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            if confidence > self.CONFIDENCE_THRESHOLD:
                self._save_to_cache(cache_key, final_response)
            
            logger.info(f"‚úÖ [NTD_CONSULTATION] Response generated successfully in {final_response['processing_time']}s")
            
            return final_response
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error: {e}")
            return {
                "response": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É —Å–∏—Å—Ç–µ–º—ã.",
                "sources": [],
                "confidence": 0.0,
                "documents_used": 0,
                "processing_time": round(time.time() - start_time, 3),
                "error": str(e),
                "cached": False
            }
    
    async def _search_relevant_documents(self, query: str, limit: int = None) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if limit is None:
            limit = self.SEARCH_LIMIT
            
        try:
            # 1. –†–µ–∞–ª—å–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
            query_vector = self._get_query_vector(query)
            
            if query_vector is None:
                logger.error("‚ùå [NTD_CONSULTATION] Failed to vectorize query")
                return []
            
            # 2. –ü–æ–∏—Å–∫ –≤ Qdrant
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=limit,
                with_payload=True,
                with_vectors=False,
                score_threshold=0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            )
            
            if not search_results:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] No search results from Qdrant")
                return []
            
            # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ë–î
            documents = []
            for result in search_results:
                doc_info = await self._get_document_info(result.id)
                if doc_info:
                    documents.append({
                        "id": result.id,
                        "score": result.score,
                        "content": result.payload.get("content", ""),
                        "title": doc_info.get("title", ""),
                        "filename": doc_info.get("filename", ""),
                        "category": doc_info.get("category", ""),
                        "page": result.payload.get("page", 1),
                        "chunk_type": result.payload.get("chunk_type", "text"),
                        "semantic_context": result.payload.get("semantic_context", ""),
                        "importance_level": result.payload.get("importance_level", 1)
                    })
            
            # 4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
            documents.sort(key=lambda x: (x['score'], x['importance_level']), reverse=True)
            
            logger.info(f"üîç [NTD_CONSULTATION] Found {len(documents)} relevant documents")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Search error: {e}")
            return []
    
    def _get_query_vector(self, query: str) -> Optional[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            if self.embedding_model:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å BGE-M3
                embedding = self.embedding_model.encode(query, normalize_embeddings=True)
                logger.debug(f"üî¢ [NTD_CONSULTATION] Query vectorized: {len(embedding)} dimensions")
                return embedding.tolist()
            else:
                logger.error("‚ùå [NTD_CONSULTATION] No embedding model available")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Vectorization error: {e}")
            return None
    
    async def _get_document_info(self, doc_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ –ë–î"""
        try:
            def _get_info(conn):
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT title, filename, category, upload_date, document_type, document_number
                        FROM uploaded_documents
                        WHERE id = %s
                    """, (doc_id,))
                    result = cursor.fetchone()
                    if result:
                        return {
                            "title": result[0],
                            "filename": result[1],
                            "category": result[2],
                            "upload_date": result[3],
                            "document_type": result[4],
                            "document_number": result[5]
                        }
                    return None
            
            return self.db_conn.execute_in_transaction(_get_info)
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error getting document info: {e}")
            return None
    
    def _build_dynamic_context(self, documents: List[Dict[str, Any]], query: str) -> str:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        context_parts = []
        total_length = 0
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        query_lower = query.lower()
        is_technical = any(word in query_lower for word in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–Ω–æ—Ä–º—ã', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã', '–ø—Ä–∞–≤–∏–ª–∞'])
        is_practical = any(word in query_lower for word in ['–∫–∞–∫', '—á—Ç–æ –¥–µ–ª–∞—Ç—å', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', '–ø—Ä–∏–º–µ—Ä—ã'])
        
        for i, doc in enumerate(documents):
            if total_length >= self.MAX_CONTEXT_LENGTH:
                break
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = doc['content']
            if doc.get('semantic_context'):
                content = f"{doc['semantic_context']}\n{content}"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            max_content_length = min(600, self.MAX_CONTEXT_LENGTH - total_length - 200)
            if len(content) > max_content_length:
                content = content[:max_content_length] + "..."
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_part = f"–î–æ–∫—É–º–µ–Ω—Ç {i+1}: {doc['title']}"
            if doc.get('document_number'):
                context_part += f" ({doc['document_number']})"
            context_part += f"\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc['category']}"
            if doc.get('chunk_type') and doc['chunk_type'] != 'text':
                context_part += f"\n–¢–∏–ø: {doc['chunk_type']}"
            context_part += f"\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {doc['score']:.3f}"
            context_part += f"\n–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content}\n---\n"
            
            context_parts.append(context_part)
            total_length += len(context_part)
        
        context = "\n".join(context_parts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if len(documents) > 0:
            context = f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\n–°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {sum(d['score'] for d in documents) / len(documents):.3f}\n\n{context}"
        
        logger.debug(f"üìù [NTD_CONSULTATION] Context built: {len(context)} characters")
        return context
    
    async def _generate_response(self, question: str, context: str, history: List[Dict[str, str]] = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
            
        Returns:
            –û—Ç–≤–µ—Ç –ò–ò
        """
        try:
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._build_enhanced_prompt(question, context, history)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ VLLM Adapter
            async with httpx.AsyncClient(timeout=90.0) as client:  # –£–≤–µ–ª–∏—á–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
                response = await client.post(
                    f"{self.ollama_url}/v1/chat/completions",
                    json={
                        "model": self.MODEL_NAME,
                        "messages": [
                            {
                                "role": "system",
                                "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": self.TEMPERATURE,
                        "max_tokens": self.MAX_TOKENS,
                        "stream": False,
                        "top_p": 0.9,
                        "frequency_penalty": 0.1,
                        "presence_penalty": 0.1
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    ai_response = data.get("choices", [{}])[0].get("message", {}).get("content", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.")
                    
                    # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                    ai_response = self._post_process_response(ai_response)
                    
                    return ai_response
                else:
                    logger.error(f"‚ùå [NTD_CONSULTATION] VLLM error: {response.status_code} - {response.text}")
                    return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
                    
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Generation error: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
    
    def _build_enhanced_prompt(self, question: str, context: str, history: List[Dict[str, str]] = None) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ò–ò"""
        
        system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –¶–∏—Ç–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
4. –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ —Ç–æ—á–Ω–æ
6. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
7. –£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞
8. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞, —É–∫–∞–∂–∏ —ç—Ç–æ

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
- –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
- –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:
{history}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç:"""

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        history_text = ""
        if history:
            history_parts = []
            for msg in history[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è
                role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg["role"] == "user" else "–ò–ò"
                content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                history_parts.append(f"{role}: {content}")
            history_text = "\n".join(history_parts)
        
        return system_prompt.format(
            context=context,
            history=history_text,
            question=question
        )
    
    def _post_process_response(self, response: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ò–ò"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        response = response.strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = response.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in cleaned_lines:
                cleaned_lines.append(line)
        
        response = '\n'.join(cleaned_lines)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not response.startswith(('‚Ä¢', '-', '1.', '2.')):
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã
            paragraphs = response.split('\n\n')
            if len(paragraphs) > 1:
                response = '\n\n'.join([f"‚Ä¢ {p.strip()}" if not p.strip().startswith('‚Ä¢') else p.strip() for p in paragraphs])
        
        return response
    
    def _prepare_sources(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞"""
        sources = []
        
        for doc in documents:
            source = {
                "title": doc.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"),
                "filename": doc.get("filename", ""),
                "category": doc.get("category", ""),
                "page": doc.get("page", 1),
                "relevance_score": round(doc.get("score", 0), 3),
                "document_type": doc.get("document_type", ""),
                "document_number": doc.get("document_number", ""),
                "chunk_type": doc.get("chunk_type", "text")
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if doc.get("semantic_context"):
                source["context"] = doc["semantic_context"][:100] + "..."
            
            sources.append(source)
        
        return sources
    
    def _calculate_confidence(self, documents: List[Dict[str, Any]], response: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        if not documents:
            return 0.0
        
        # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        avg_score = sum(doc.get("score", 0) for doc in documents) / len(documents)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_bonus = min(len(documents) / 5.0, 0.2)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
        response_quality = 0.0
        if response and len(response) > 100:
            response_quality = min(len(response) / 1000.0, 0.3)
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        importance_bonus = sum(doc.get("importance_level", 1) for doc in documents) / len(documents) * 0.1
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = avg_score + doc_bonus + response_quality + importance_bonus
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1
        confidence = min(confidence, 1.0)
        
        return round(confidence, 3)
    
    async def get_consultation_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π"""
        try:
            def _get_stats(conn):
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT 
                            COUNT(*) as total_consultations,
                            COUNT(DISTINCT user_id) as unique_users,
                            AVG(confidence_score) as avg_confidence,
                            MAX(created_at) as last_consultation,
                            AVG(processing_time) as avg_processing_time
                        FROM ntd_consultations
                        WHERE created_at >= NOW() - INTERVAL '30 days'
                    """)
                    result = cursor.fetchone()
                    return {
                        "total_consultations": result[0] or 0,
                        "unique_users": result[1] or 0,
                        "avg_confidence": round(result[2] or 0, 3),
                        "last_consultation": result[3],
                        "avg_processing_time": round(result[4] or 0, 3)
                    }
            
            stats = self.db_conn.execute_in_transaction(_get_stats)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
            stats["cache_size"] = len(self.cache)
            stats["cache_hit_rate"] = self._calculate_cache_hit_rate()
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Stats error: {e}")
            return {
                "total_consultations": 0,
                "unique_users": 0,
                "avg_confidence": 0.0,
                "last_consultation": None,
                "avg_processing_time": 0.0,
                "cache_size": len(self.cache),
                "cache_hit_rate": 0.0
            }
    
    def _calculate_cache_hit_rate(self) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é —Å—á–µ—Ç—á–∏–∫–æ–≤
        return 0.0  # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    
    async def clear_cache(self) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        cache_size = len(self.cache)
        self.cache.clear()
        logger.info(f"üóëÔ∏è [NTD_CONSULTATION] Cache cleared: {cache_size} entries")
        return {
            "status": "success",
            "cleared_entries": cache_size,
            "message": f"Cache cleared successfully. Removed {cache_size} entries."
        }
    
    async def get_cache_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        current_time = time.time()
        valid_entries = 0
        expired_entries = 0
        
        for entry in self.cache.values():
            if current_time - entry['timestamp'] < self.cache_ttl:
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            "total_entries": len(self.cache),
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "cache_ttl": self.cache_ttl,
            "cache_size_mb": len(json.dumps(self.cache)) / (1024 * 1024)
        }
