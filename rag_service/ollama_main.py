from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.responses import PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
import hashlib
import time
import asyncio
import json
import logging
import os
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

# –ò–º–ø–æ—Ä—Ç Ollama RAG —Å–µ—Ä–≤–∏—Å–∞ (–Ω–æ–≤–∞—è –º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
from services.ollama_rag_service_refactored import OllamaRAGService
from services.reranker_service import BGERerankerService

# –ú–æ–¥–µ–ª–∏ Pydantic
class NTDConsultationRequest(BaseModel):
    message: str
    user_id: str

class NTDConsultationResponse(BaseModel):
    status: str
    response: str
    sources: List[Dict[str, Any]]
    confidence: float
    documents_used: int
    timestamp: str

class SearchRequest(BaseModel):
    query: str
    k: int = 8
    document_filter: Optional[str] = None
    chapter_filter: Optional[str] = None
    chunk_type_filter: Optional[str] = None

# –ú–æ–¥–µ–ª–∏ Pydantic –¥–ª—è —á–∞—Ç–∞
class ChatRequest(BaseModel):
    message: str
    model: str = "llama3.1:8b"
    history: Optional[List[Dict[str, str]]] = None
    max_tokens: Optional[int] = None
    turbo_mode: bool = False  # –¢—É—Ä–±–æ —Ä–µ–∂–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
    reasoning_depth: str = "balanced"  # –ì–ª—É–±–∏–Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: "fast", "balanced", "deep"

class ChatResponse(BaseModel):
    status: str
    response: str
    model: str
    timestamp: str
    tokens_used: Optional[int] = None
    generation_time_ms: Optional[float] = None
    turbo_mode: bool = False
    reasoning_depth: str = "balanced"
    reasoning_steps: Optional[int] = None  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è

class EmbeddingRequest(BaseModel):
    text: str

class EmbeddingResponse(BaseModel):
    status: str
    embedding: List[float]
    text_length: int
    timestamp: str

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Ollama RAG Service",
    description="RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3",
    version="2.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama RAG —Å–µ—Ä–≤–∏—Å–∞ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
ollama_rag_service = None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
async_reindex_status = {}

def get_ollama_rag_service():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama RAG —Å–µ—Ä–≤–∏—Å–∞"""
    global ollama_rag_service
    if ollama_rag_service is None:
        logger.info(f"üîÑ [RAG_SERVICE] Creating new OllamaRAGService instance")
        ollama_rag_service = OllamaRAGService()
        logger.info(f"‚úÖ [RAG_SERVICE] OllamaRAGService instance created: {id(ollama_rag_service)}")
    else:
        logger.info(f"‚ôªÔ∏è [RAG_SERVICE] Reusing existing OllamaRAGService instance: {id(ollama_rag_service)}")
    return ollama_rag_service

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
# ============================================================================

@app.get("/documents")
async def documents_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return get_ollama_rag_service().get_documents()

@app.get("/stats")
async def stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå [STATS] Error getting stats: {e}")
        return {
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/documents/stats")
async def documents_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        return {
            "tokens": stats.get('postgresql', {}).get('total_tokens', 0),
            "chunks": stats.get('postgresql', {}).get('total_chunks', 0),
            "vectors": stats.get('qdrant', {}).get('vectors_count', 0),
            "documents": stats.get('postgresql', {}).get('total_documents', 0),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [DOCUMENTS_STATS] Error getting documents stats: {e}")
        return {
            "tokens": 0,
            "chunks": 0,
            "vectors": 0,
            "documents": 0,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/documents/{document_id}/chunks")
async def document_chunks_endpoint(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    return get_ollama_rag_service().get_document_chunks(document_id)

@app.delete("/documents/{document_id}")
async def delete_document_endpoint(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = get_ollama_rag_service().delete_document(document_id)
        if success:
            return {"status": "success", "message": f"Document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOCUMENT] Error deleting document {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex")
async def reindex_documents_endpoint():
    """–ü–æ–ª–Ω–∞—è —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—á–∏—Å—Ç–∫–æ–π Qdrant"""
    try:
        logger.info("üîÑ [REINDEX] Starting full document reindexing with Qdrant cleanup...")
        
        rag_service = get_ollama_rag_service()
        
        # 1. –û—á–∏—â–∞–µ–º Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é
        logger.info("üóëÔ∏è [REINDEX] Clearing Qdrant collection...")
        try:
            success = rag_service.qdrant_service.clear_collection()
            if success:
                logger.info("‚úÖ [REINDEX] Qdrant collection cleared")
            else:
                logger.warning("‚ö†Ô∏è [REINDEX] Failed to clear Qdrant collection")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [REINDEX] Error clearing Qdrant collection: {e}")
        
        # 2. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        logger.info("üÜï [REINDEX] Ensuring Qdrant collection exists...")
        try:
            rag_service.qdrant_service._ensure_collection_exists()
            logger.info("‚úÖ [REINDEX] Qdrant collection ensured")
        except Exception as e:
            logger.error(f"‚ùå [REINDEX] Error ensuring Qdrant collection: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to ensure Qdrant collection: {e}")
        
        # 3. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = rag_service.db_manager.execute_read_query("""
            SELECT ud.id, ud.original_filename as document_title, ud.category
            FROM uploaded_documents ud
            WHERE ud.processing_status = 'completed'
            ORDER BY ud.upload_date DESC
        """)
        
        if not documents:
            return {
                "status": "success",
                "message": "No documents to reindex",
                "documents_processed": 0,
                "timestamp": datetime.now().isoformat()
            }
        
        logger.info(f"üîÑ [REINDEX] Found {len(documents)} documents to reindex")
        
        total_processed = 0
        total_chunks = 0
        
        # 4. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for document in documents:
            try:
                document_id = document['id']
                document_title = document['document_title']
                
                logger.info(f"üìù [REINDEX] Processing document {document_id}: {document_title}")
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                chunks = rag_service.get_document_chunks(document_id)
                
                if chunks:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É (—É–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
                    import re
                    document_title_clean = re.sub(r'\.(pdf|txt|doc|docx)$', '', document_title, flags=re.IGNORECASE)
                    for chunk in chunks:
                        chunk['document_title'] = document_title_clean
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                    success = rag_service.index_document_chunks(document_id, chunks)
                    
                    if success:
                        total_processed += 1
                        total_chunks += len(chunks)
                        logger.info(f"‚úÖ [REINDEX] Successfully indexed document {document_id} with {len(chunks)} chunks")
                    else:
                        logger.error(f"‚ùå [REINDEX] Failed to index document {document_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è [REINDEX] No chunks found for document {document_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [REINDEX] Error processing document {document.get('id', 'unknown')}: {e}")
                continue
        
        logger.info(f"‚úÖ [REINDEX] Full reindexing completed. Processed {total_processed} documents with {total_chunks} chunks")
        
        return {
            "status": "success",
            "message": "Full document reindexing completed",
            "documents_processed": total_processed,
            "total_chunks": total_chunks,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX] Error during full reindexing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}/indexes")
async def delete_document_indexes_endpoint(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = get_ollama_rag_service().delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Indexes for document {document_id} deleted successfully",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
            
    except Exception as e:
        logger.error(f"‚ùå [DELETE_INDEXES] Error deleting indexes for document {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex/async")
async def async_reindex_documents_endpoint():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—á–∏—Å—Ç–∫–æ–π Qdrant"""
    try:
        logger.info("üîÑ [ASYNC_REINDEX] Starting async document reindexing...")
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
        import uuid
        task_id = str(uuid.uuid4())
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
        import asyncio
        asyncio.create_task(perform_async_reindex_with_status(task_id))
        
        return {
            "status": "started",
            "message": "Async reindexing started",
            "task_id": task_id,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [ASYNC_REINDEX] Error starting async reindexing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/reindex/status/{task_id}")
async def get_reindex_status_endpoint(task_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    try:
        logger.info(f"üìä [REINDEX_STATUS] Getting status for task {task_id}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
        if task_id in async_reindex_status:
            return async_reindex_status[task_id]
        else:
            return {
                "status": "not_found",
                "message": "Task not found",
                "timestamp": datetime.now().isoformat()
            }
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX_STATUS] Error getting status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def perform_async_reindex_with_status(task_id: str):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞"""
    try:
        logger.info(f"üîÑ [ASYNC_REINDEX] Performing async reindexing for task {task_id}...")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "running"
        async_reindex_status[task_id] = {
            "status": "running",
            "message": "Reindexing in progress...",
            "progress": 0,
            "total_documents": 0,
            "reindexed_count": 0,
            "timestamp": datetime.now().isoformat()
        }
        
        rag_service = get_ollama_rag_service()
        
        # 1. –û—á–∏—â–∞–µ–º Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é
        logger.info("üóëÔ∏è [ASYNC_REINDEX] Clearing Qdrant collection...")
        try:
            success = rag_service.qdrant_service.clear_collection()
            if success:
                logger.info("‚úÖ [ASYNC_REINDEX] Qdrant collection cleared")
            else:
                logger.warning("‚ö†Ô∏è [ASYNC_REINDEX] Failed to clear Qdrant collection")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [ASYNC_REINDEX] Error clearing Qdrant collection: {e}")
        
        # 2. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        logger.info("üÜï [ASYNC_REINDEX] Ensuring Qdrant collection exists...")
        try:
            rag_service.qdrant_service._ensure_collection_exists()
            logger.info("‚úÖ [ASYNC_REINDEX] Qdrant collection ensured")
        except Exception as e:
            logger.error(f"‚ùå [ASYNC_REINDEX] Error ensuring Qdrant collection: {e}")
            async_reindex_status[task_id] = {
                "status": "error",
                "message": f"Failed to ensure Qdrant collection: {e}",
                "timestamp": datetime.now().isoformat()
            }
            return
        
        # 3. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = rag_service.db_manager.execute_read_query("""
            SELECT ud.id, ud.original_filename as document_title, ud.category
            FROM uploaded_documents ud
            WHERE ud.processing_status = 'completed'
            ORDER BY ud.upload_date DESC
        """)
        
        if not documents:
            logger.info("‚ÑπÔ∏è [ASYNC_REINDEX] No documents to reindex")
            async_reindex_status[task_id] = {
                "status": "completed",
                "message": "No documents to reindex",
                "progress": 100,
                "total_documents": 0,
                "reindexed_count": 0,
                "timestamp": datetime.now().isoformat()
            }
            return
        
        logger.info(f"üîÑ [ASYNC_REINDEX] Found {len(documents)} documents to reindex")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        async_reindex_status[task_id] = {
            "status": "running",
            "message": f"Reindexing {len(documents)} documents...",
            "progress": 0,
            "total_documents": len(documents),
            "reindexed_count": 0,
            "timestamp": datetime.now().isoformat()
        }
        
        total_processed = 0
        total_chunks = 0
        
        # 4. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for i, document in enumerate(documents):
            try:
                document_id = document['id']
                document_title = document['document_title']
                
                logger.info(f"üìù [ASYNC_REINDEX] Processing document {document_id}: {document_title} ({i+1}/{len(documents)})")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                progress = int((i / len(documents)) * 100)
                async_reindex_status[task_id] = {
                    "status": "running",
                    "message": f"Processing document {i+1} of {len(documents)}: {document_title}",
                    "progress": progress,
                    "total_documents": len(documents),
                    "reindexed_count": total_processed,
                    "current_document": document_title,
                    "timestamp": datetime.now().isoformat()
                }
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                chunks = rag_service.get_document_chunks(document_id)
                
                if chunks:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É (—É–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
                    import re
                    document_title_clean = re.sub(r'\.(pdf|txt|doc|docx)$', '', document_title, flags=re.IGNORECASE)
                    for chunk in chunks:
                        chunk['document_title'] = document_title_clean
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                    success = rag_service.index_document_chunks(document_id, chunks)
                    
                    if success:
                        total_processed += 1
                        total_chunks += len(chunks)
                        logger.info(f"‚úÖ [ASYNC_REINDEX] Successfully indexed document {document_id} with {len(chunks)} chunks")
                    else:
                        logger.error(f"‚ùå [ASYNC_REINDEX] Failed to index document {document_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è [ASYNC_REINDEX] No chunks found for document {document_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [ASYNC_REINDEX] Error processing document {document.get('id', 'unknown')}: {e}")
                continue
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        async_reindex_status[task_id] = {
            "status": "completed",
            "message": f"Reindexing completed. {total_processed} documents reindexed with {total_chunks} chunks",
            "progress": 100,
            "total_documents": len(documents),
            "reindexed_count": total_processed,
            "total_chunks": total_chunks,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ [ASYNC_REINDEX] Async reindexing completed. Processed {total_processed} documents with {total_chunks} chunks")
        
    except Exception as e:
        logger.error(f"‚ùå [ASYNC_REINDEX] Error during async reindexing: {e}")
        async_reindex_status[task_id] = {
            "status": "error",
            "message": f"Reindexing failed: {e}",
            "timestamp": datetime.now().isoformat()
        }

async def perform_async_reindex():
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    try:
        logger.info("üîÑ [ASYNC_REINDEX] Performing async reindexing...")
        
        rag_service = get_ollama_rag_service()
        
        # 1. –û—á–∏—â–∞–µ–º Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é
        logger.info("üóëÔ∏è [ASYNC_REINDEX] Clearing Qdrant collection...")
        try:
            success = rag_service.qdrant_service.clear_collection()
            if success:
                logger.info("‚úÖ [ASYNC_REINDEX] Qdrant collection cleared")
            else:
                logger.warning("‚ö†Ô∏è [ASYNC_REINDEX] Failed to clear Qdrant collection")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [ASYNC_REINDEX] Error clearing Qdrant collection: {e}")
        
        # 2. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        logger.info("üÜï [ASYNC_REINDEX] Ensuring Qdrant collection exists...")
        try:
            rag_service.qdrant_service._ensure_collection_exists()
            logger.info("‚úÖ [ASYNC_REINDEX] Qdrant collection ensured")
        except Exception as e:
            logger.error(f"‚ùå [ASYNC_REINDEX] Error ensuring Qdrant collection: {e}")
            return
        
        # 3. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = rag_service.db_manager.execute_read_query("""
            SELECT ud.id, ud.original_filename as document_title, ud.category
            FROM uploaded_documents ud
            WHERE ud.processing_status = 'completed'
            ORDER BY ud.upload_date DESC
        """)
        
        if not documents:
            logger.info("‚ÑπÔ∏è [ASYNC_REINDEX] No documents to reindex")
            return
        
        logger.info(f"üîÑ [ASYNC_REINDEX] Found {len(documents)} documents to reindex")
        
        total_processed = 0
        total_chunks = 0
        
        # 4. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for document in documents:
            try:
                document_id = document['id']
                document_title = document['document_title']
                
                logger.info(f"üìù [ASYNC_REINDEX] Processing document {document_id}: {document_title}")
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                chunks = rag_service.get_document_chunks(document_id)
                
                if chunks:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É (—É–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
                    import re
                    document_title_clean = re.sub(r'\.(pdf|txt|doc|docx)$', '', document_title, flags=re.IGNORECASE)
                    for chunk in chunks:
                        chunk['document_title'] = document_title_clean
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                    success = rag_service.index_document_chunks(document_id, chunks)
                    
                    if success:
                        total_processed += 1
                        total_chunks += len(chunks)
                        logger.info(f"‚úÖ [ASYNC_REINDEX] Successfully indexed document {document_id} with {len(chunks)} chunks")
                    else:
                        logger.error(f"‚ùå [ASYNC_REINDEX] Failed to index document {document_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è [ASYNC_REINDEX] No chunks found for document {document_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [ASYNC_REINDEX] Error processing document {document.get('id', 'unknown')}: {e}")
                continue
        
        logger.info(f"‚úÖ [ASYNC_REINDEX] Async reindexing completed. Processed {total_processed} documents with {total_chunks} chunks")
        
    except Exception as e:
        logger.error(f"‚ùå [ASYNC_REINDEX] Error during async reindexing: {e}")

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    category: str = Form("other"),
    description: str = Form("")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üì§ [UPLOAD] Uploading normative document: {file.filename}")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        if not file.filename.lower().endswith(('.pdf', '.docx', '.txt')):
            raise HTTPException(status_code=400, detail="Unsupported file type. Only PDF, DOCX, and TXT files are allowed.")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = await file.read()
        file_size = len(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 50 MB)
        max_file_size = 50 * 1024 * 1024  # 50 MB
        if file_size > max_file_size:
            raise HTTPException(
                status_code=413, 
                detail=f"File too large. Maximum size is {max_file_size // (1024*1024)} MB"
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        document_hash = hashlib.sha256(content).hexdigest()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_id = int(time.time() * 1000) % 100000000  # 8-–∑–Ω–∞—á–Ω–æ–µ —á–∏—Å–ª–æ
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        try:
            rag_service = get_ollama_rag_service()
            saved_document_id = rag_service.save_document_to_db(
                document_id=document_id,
                filename=file.filename,
                original_filename=file.filename,
                file_type=file.filename.split('.')[-1].lower(),
                file_size=file_size,
                document_hash=document_hash,
                category=category,
                document_type='normative'
            )
            
            logger.info(f"‚úÖ [UPLOAD] Document saved to database with ID: {saved_document_id}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîÑ [UPLOAD] Starting document processing for {saved_document_id}")
            asyncio.create_task(process_normative_document_async(saved_document_id, content, file.filename))
            
            return {
                "status": "success",
                "document_id": saved_document_id,
                "filename": file.filename,
                "file_size": file_size,
                "message": f"Document uploaded successfully and processing started"
            }
            
        except Exception as e:
            logger.error(f"‚ùå [UPLOAD] Database error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [UPLOAD] Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_normative_document_async(document_id: int, content: bytes, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üîÑ [PROCESS] Starting async processing for document {document_id}")
    try:
        rag_service = get_ollama_rag_service()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        rag_service.update_document_status(document_id, "processing")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        success = await rag_service.process_document_async(document_id, content, filename)
        
        if success:
            rag_service.update_document_status(document_id, "completed")
            logger.info(f"‚úÖ [PROCESS] Document {document_id} processed successfully")
        else:
            rag_service.update_document_status(document_id, "failed", "Processing failed")
            logger.error(f"‚ùå [PROCESS] Document {document_id} processing failed")
            
    except Exception as e:
        logger.error(f"‚ùå [PROCESS] Error processing document {document_id}: {e}")
        try:
            rag_service = get_ollama_rag_service()
            rag_service.update_document_status(document_id, "failed", str(e))
        except:
            pass

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
# ============================================================================

@app.post("/search")
async def search_norms(
    query: str = Form(...),
    k: int = Form(8),
    document_filter: Optional[str] = Form(None),
    chapter_filter: Optional[str] = Form(None),
    chunk_type_filter: Optional[str] = Form(None)
):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    start_time = datetime.now()
    logger.info(f"üîç [SEARCH_NORM] Performing hybrid search for query: '{query}' with k={k}")
    logger.info(f"üîç [SEARCH_NORM] Filters: document={document_filter}, chapter={chapter_filter}, chunk_type={chunk_type_filter}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º Ollama RAG —Å–µ—Ä–≤–∏—Å
        rag_service = get_ollama_rag_service()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model_logger.info(f"ü§ñ [EMBEDDING_REQUEST] Generating embeddings for query: '{query[:100]}...'")
        
        results = rag_service.hybrid_search(
            query=query,
            k=k,
            document_filter=document_filter,
            chapter_filter=chapter_filter,
            chunk_type_filter=chunk_type_filter
        )
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ [SEARCH_NORM] Hybrid search completed in {execution_time:.2f}s. Found {len(results)} results.")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        if results:
            top_result = results[0]
            model_logger.info(f"üìä [SEARCH_RESULTS] Top result: {top_result.get('document_title', 'Unknown')} - Score: {top_result.get('score', 0):.3f}")
            model_logger.debug(f"üìä [SEARCH_RESULTS] Top result content preview: {top_result.get('content', '')[:200]}...")
        
        return {
            "query": query,
            "results_count": len(results),
            "execution_time": execution_time,
            "results": results
        }
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.error(f"‚ùå [SEARCH_NORM] Search error after {execution_time:.2f}s: {type(e).__name__}: {str(e)}")
        model_logger.error(f"‚ùå [EMBEDDING_ERROR] Failed to process query: '{query[:100]}...' - {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î
# ============================================================================

@app.post("/ntd-consultation/chat")
async def ntd_consultation_chat_endpoint(request: NTDConsultationRequest):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î"""
    try:
        rag_service = get_ollama_rag_service()
        result = rag_service.get_ntd_consultation(request.message, request.user_id)
        return result
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/structured-context")
async def get_structured_context_endpoint(request: dict):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
    try:
        logger.info("üèóÔ∏è [STRUCTURED_CONTEXT] Structured context request received")
        rag_service = get_ollama_rag_service()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        message = request.get("message", "")
        k = request.get("k", 8)
        document_filter = request.get("document_filter")
        chapter_filter = request.get("chapter_filter")
        chunk_type_filter = request.get("chunk_type_filter")
        use_reranker = request.get("use_reranker", True)
        fast_mode = request.get("fast_mode", False)
        use_mmr = request.get("use_mmr", True)
        use_intent_classification = request.get("use_intent_classification", True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ OllamaRAGService
        if hasattr(rag_service, 'get_structured_context'):
            response = rag_service.get_structured_context(
                query=message,
                k=k,
                document_filter=document_filter,
                chapter_filter=chapter_filter,
                chunk_type_filter=chunk_type_filter,
                use_reranker=use_reranker,
                fast_mode=fast_mode,
                use_mmr=use_mmr,
                use_intent_classification=use_intent_classification
            )
        else:
            # Fallback –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ RAG —Å–µ—Ä–≤–∏—Å–∞
            search_results = rag_service.hybrid_search(message, k=k)
            response = {
                "query": message,
                "timestamp": datetime.now().isoformat(),
                "context": [
                    {
                        "doc": result.get('code', ''),
                        "section": result.get('section', ''),
                        "page": result.get('page', 1),
                        "snippet": result.get('content', '')[:200] + '...' if len(result.get('content', '')) > 200 else result.get('content', ''),
                        "why": "fallback",
                        "score": result.get('score', 0.0),
                        "document_title": result.get('document_title', ''),
                        "section_title": result.get('section_title', ''),
                        "chunk_type": result.get('chunk_type', ''),
                        "metadata": result.get('metadata', {})
                    }
                    for result in search_results
                ],
                "meta_summary": {
                    "query_type": "fallback",
                    "documents_found": len(search_results),
                    "sections_covered": len(set(result.get('section', '') for result in search_results)),
                    "avg_relevance": sum(result.get('score', 0) for result in search_results) / len(search_results) if search_results else 0,
                    "coverage_quality": "fallback",
                    "key_documents": list(set(result.get('code', '') for result in search_results[:3] if result.get('code'))),
                    "key_sections": list(set(result.get('section', '') for result in search_results[:3] if result.get('section')))
                },
                "total_candidates": len(search_results),
                "avg_score": sum(result.get('score', 0) for result in search_results) / len(search_results) if search_results else 0
            }
        
        logger.info(f"‚úÖ [STRUCTURED_CONTEXT] Structured context generated successfully")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [STRUCTURED_CONTEXT] Context generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/ntd-consultation/stats")
async def ntd_consultation_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        return {
            "status": "success",
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION_STATS] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/ntd-consultation/cache")
async def clear_consultation_cache_endpoint():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    return {
        "status": "success",
        "message": "Cache cleared successfully",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/ntd-consultation/cache/stats")
async def get_consultation_cache_stats_endpoint():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    return {
        "status": "success",
        "cache_stats": {
            "cache_type": "No caching implemented",
            "cache_size": 0,
            "cache_hits": 0,
            "cache_misses": 0
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """–ß–∞—Ç —Å –ò–ò —á–µ—Ä–µ–∑ Ollama —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    try:
        logger.info(f"üí¨ [CHAT] Processing chat request with model: {request.model}, turbo_mode: {request.turbo_mode}, reasoning_depth: {request.reasoning_depth}")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —Ç—É—Ä–±–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        from services.turbo_reasoning_service import TurboReasoningService
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Ç—É—Ä–±–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        turbo_service = TurboReasoningService()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ä–µ–∂–∏–º–æ–º
        result = turbo_service.generate_response(
            message=request.message,
            history=request.history,
            turbo_mode=request.turbo_mode,
            reasoning_depth=request.reasoning_depth,
            max_tokens=request.max_tokens
        )
        
        return ChatResponse(
            status="success",
            response=result["response"],
            model=result["model"],
            timestamp=datetime.now().isoformat(),
            tokens_used=result["tokens_used"],
            generation_time_ms=result["generation_time_ms"],
            turbo_mode=result["turbo_mode"],
            reasoning_depth=result["reasoning_depth"],
            reasoning_steps=result["reasoning_steps"]
        )
            
    except Exception as e:
        logger.error(f"‚ùå [CHAT] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def models_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        logger.info("üîç [MODELS] Getting available Ollama models")
        
        import requests
        
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            models = models_data.get("models", [])
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –¥–ª—è —á–∞—Ç–∞ (–∏—Å–∫–ª—é—á–∞–µ–º bge-m3)
            chat_models = [
                {
                    "name": model.get("name", ""),
                    "model": model.get("model", ""),
                    "size": model.get("size", 0),
                    "parameter_size": model.get("details", {}).get("parameter_size", ""),
                    "quantization": model.get("details", {}).get("quantization_level", ""),
                    "family": model.get("details", {}).get("family", "")
                }
                for model in models
                if "bge-m3" not in model.get("name", "").lower()
            ]
            
            return {
                "status": "success",
                "models": chat_models,
                "total_count": len(chat_models),
                "ollama_status": "healthy",
                "timestamp": datetime.now().isoformat()
            }
        else:
            logger.error(f"‚ùå [MODELS] Ollama API error: {response.status_code}")
            raise HTTPException(status_code=500, detail=f"Ollama API error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"‚ùå [MODELS] Error getting models: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/reasoning-modes")
async def reasoning_modes_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    try:
        logger.info("üß† [REASONING_MODES] Getting available reasoning modes")
        
        from services.turbo_reasoning_service import TurboReasoningService
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–µ–∂–∏–º–∞—Ö
        turbo_service = TurboReasoningService()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∂–∏–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        reasoning_modes = turbo_service.get_reasoning_modes()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–¥–æ—Ä–æ–≤—å–µ —Å–µ—Ä–≤–∏—Å–∞
        service_health = turbo_service.health_check()
        
        return {
            "status": "success",
            "reasoning_modes": reasoning_modes,
            "service_health": "healthy" if service_health else "unhealthy",
            "total_modes": len(reasoning_modes),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [REASONING_MODES] Error getting reasoning modes: {e}")
        return {
            "status": "error",
            "error": str(e),
            "service_health": "unhealthy",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/hybrid_search_stats")
async def hybrid_search_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    try:
        logger.info("üìä [HYBRID_STATS] Getting hybrid search statistics")
        
        rag_service_instance = get_rag_service()
        stats = rag_service_instance.get_hybrid_search_stats()
        
        logger.info("‚úÖ [HYBRID_STATS] Statistics retrieved successfully")
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå [HYBRID_STATS] Error getting hybrid search stats: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/chat_documents_stats")
async def chat_documents_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–∞—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏ chat_documents –∏–∑ Qdrant
        import requests
        
        qdrant_url = "http://qdrant:6333"
        collection_name = "chat_documents"
        
        try:
            response = requests.get(f"{qdrant_url}/collections/{collection_name}", timeout=10)
            if response.status_code == 200:
                result = response.json()
                return {
                    "total_documents": result['result']['points_count'],
                    "indexed_vectors": result['result']['indexed_vectors_count'],
                    "collection_status": result['result']['status'],
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "total_documents": 0,
                    "indexed_vectors": 0,
                    "collection_status": "not_found",
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            return {
                "total_documents": 0,
                "indexed_vectors": 0,
                "collection_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"‚ùå [CHAT_DOCUMENTS_STATS] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/embeddings")
async def create_embedding_endpoint(request: EmbeddingRequest):
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
    try:
        logger.info(f"üîç [EMBEDDINGS] Creating embedding for text: '{request.text[:100]}...'")
        
        rag_service = get_ollama_rag_service()
        embedding = rag_service.embedding_service.create_embedding(request.text)
        
        return EmbeddingResponse(
            status="success",
            embedding=embedding,
            text_length=len(request.text),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå [EMBEDDINGS] Error creating embedding: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/embeddings")
async def create_embedding_get_endpoint(text: str):
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (GET –∑–∞–ø—Ä–æ—Å)"""
    try:
        logger.info(f"üîç [EMBEDDINGS] Creating embedding for text: '{text[:100]}...'")
        
        rag_service = get_ollama_rag_service()
        embedding = rag_service.embedding_service.create_embedding(text)
        
        return EmbeddingResponse(
            status="success",
            embedding=embedding,
            text_length=len(text),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå [EMBEDDINGS] Error creating embedding: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/test-chunking")
async def test_chunking_endpoint():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è"""
    try:
        logger.info("üß™ [TEST_CHUNKING] Testing granular chunking functionality...")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        test_text = """
        –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1 –∏–∑ 2
        
        –°–ü 22.13330.2016 "–û—Å–Ω–æ–≤–∞–Ω–∏—è –∑–¥–∞–Ω–∏–π –∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π"
        
        –ì–ª–∞–≤–∞ 1. –û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è
        
        1.1. –ù–∞—Å—Ç–æ—è—â–∏–π —Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Å–Ω–æ–≤–∞–Ω–∏–π –∑–¥–∞–Ω–∏–π –∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π.
        
        1.2. –û—Å–Ω–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∑–¥–∞–Ω–∏–π –∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π.
        
        1.3. –ü—Ä–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω–∏–π —Å–ª–µ–¥—É–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å:
        - –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ-–≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è;
        - –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∑–¥–∞–Ω–∏–π;
        - —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.
        
        –ì–ª–∞–≤–∞ 2. –ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ-–≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑—ã—Å–∫–∞–Ω–∏—è
        
        2.1. –ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ-–≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑—ã—Å–∫–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –°–ü 47.13330.
        
        2.2. –û–±—ä–µ–º –∏–∑—ã—Å–∫–∞–Ω–∏–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ-–≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π.
        
        –°—Ç—Ä–∞–Ω–∏—Ü–∞ 2 –∏–∑ 2
        
        –ì–ª–∞–≤–∞ 3. –†–∞—Å—á–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω–∏–π
        
        3.1. –†–∞—Å—á–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω–∏–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ –ø—Ä–µ–¥–µ–ª—å–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º.
        
        3.2. –ü—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è:
        - –Ω–∞–≥—Ä—É–∑–∫–∏ –æ—Ç –∑–¥–∞–Ω–∏–π –∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π;
        - —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–µ—Å –≥—Ä—É–Ω—Ç–æ–≤;
        - –≥–∏–¥—Ä–æ–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è.
        
        3.3. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –Ω–µ –º–µ–Ω–µ–µ 1,2.
        
        –ì–ª–∞–≤–∞ 4. –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
        
        4.1. –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∏–π –¥–æ–ª–∂–Ω—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å:
        - —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –æ—Å–∞–¥–æ–∫;
        - —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –æ—Ç–∫–æ—Å–æ–≤;
        - –∑–∞—â–∏—Ç—É –æ—Ç –ø–æ–¥—Ç–æ–ø–ª–µ–Ω–∏—è.
        
        4.2. –ü—Ä–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–æ–≤ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å:
        - –≥–∏–¥—Ä–æ–∏–∑–æ–ª—è—Ü–∏—é;
        - –¥—Ä–µ–Ω–∞–∂;
        - –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—é –ø–æ–¥–ø–æ–ª–∏–π.
        
        4.3. –ú–∞—Ç–µ—Ä–∏–∞–ª—ã —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø–æ –ø—Ä–æ—á–Ω–æ—Å—Ç–∏ –∏ –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç–∏.
        """
        
        logger.info(f"üìÑ [TEST_CHUNKING] Test text length: {len(test_text)} characters")
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏
        rag_service = get_ollama_rag_service()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        try:
            from config.chunking_config import get_chunking_config
            config = get_chunking_config('default')
            logger.info(f"üîß [TEST_CHUNKING] Using config: {config}")
            
            sentences = rag_service._split_into_sentences(test_text, config)
            logger.info(f"üî§ [TEST_CHUNKING] Split into {len(sentences)} sentences")
            
            if sentences:
                logger.info(f"üìù [TEST_CHUNKING] First sentence: {sentences[0][:100]}...")
                logger.info(f"üìù [TEST_CHUNKING] Last sentence: {sentences[-1][:100]}...")
            
        except Exception as e:
            logger.error(f"‚ùå [TEST_CHUNKING] Error in sentence splitting: {e}")
            sentences = []
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        try:
            chunks = rag_service._split_page_into_chunks(test_text, 1000)
            logger.info(f"üìù [TEST_CHUNKING] Created {len(chunks)} chunks")
            
            if chunks:
                logger.info(f"üìù [TEST_CHUNKING] First chunk: {chunks[0][:100]}...")
                logger.info(f"üìù [TEST_CHUNKING] Last chunk: {chunks[-1][:100]}...")
            
        except Exception as e:
            logger.error(f"‚ùå [TEST_CHUNKING] Error in chunk creation: {e}")
            chunks = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        chunk_analysis = []
        for i, chunk in enumerate(chunks):
            try:
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                estimated_tokens = rag_service._estimate_tokens(chunk, {'tokens_per_char': 4})
                
                chunk_info = {
                    'chunk_id': i + 1,
                    'content_length': len(chunk),
                    'estimated_tokens': estimated_tokens,
                    'content_preview': chunk[:100] + '...' if len(chunk) > 100 else chunk,
                    'sentences_count': len(chunk.split('.')),
                    'has_headers': any(word in chunk.lower() for word in ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–ø—É–Ω–∫—Ç'])
                }
                chunk_analysis.append(chunk_info)
            except Exception as e:
                logger.error(f"‚ùå [TEST_CHUNKING] Error analyzing chunk {i}: {e}")
        
        logger.info(f"‚úÖ [TEST_CHUNKING] Created {len(chunks)} chunks successfully")
        
        return {
            "status": "success",
            "message": "Granular chunking test completed",
            "total_chunks": len(chunks),
            "chunks": chunk_analysis,
            "test_text_length": len(test_text),
            "sentences_count": len(sentences),
            "config_used": config if 'config' in locals() else None,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [TEST_CHUNKING] Error in chunking test: {e}")
        import traceback
        logger.error(f"‚ùå [TEST_CHUNKING] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/test-reranker")
async def test_reranker_endpoint():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞"""
    try:
        logger.info("üîÑ [TEST_RERANKER] Testing reranker functionality...")
        
        # –ü–æ–ª—É—á–∞–µ–º RAG —Å–µ—Ä–≤–∏—Å
        rag_service = get_ollama_rag_service()
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        test_query = "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Å–Ω–æ–≤–∞–Ω–∏–π –∑–¥–∞–Ω–∏–π"
        
        logger.info(f"üîç [TEST_RERANKER] Test query: '{test_query}'")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º
        try:
            results_with_reranker = rag_service.hybrid_search(
                query=test_query,
                k=8,
                use_reranker=True,
                fast_mode=False  # –û—Ç–∫–ª—é—á–∞–µ–º –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            
            logger.info(f"‚úÖ [TEST_RERANKER] Search with reranker completed, found {len(results_with_reranker)} results")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            reranked_analysis = []
            for i, result in enumerate(results_with_reranker):
                result_info = {
                    'rank': i + 1,
                    'document_title': result.get('document_title', 'Unknown'),
                    'rerank_score': result.get('rerank_score', 'N/A'),
                    'vector_score': result.get('score', 'N/A'),
                    'content_preview': result.get('content', '')[:100] + '...' if result.get('content') else 'No content'
                }
                reranked_analysis.append(result_info)
            
            return {
                "status": "success",
                "message": "Reranker test completed",
                "query": test_query,
                "total_results": len(results_with_reranker),
                "results": reranked_analysis,
                "reranker_enabled": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [TEST_RERANKER] Error during search with reranker: {e}")
            return {
                "status": "error",
                "message": f"Error during search: {str(e)}",
                "query": test_query,
                "reranker_enabled": False,
                "timestamp": datetime.now().isoformat()
            }
        
    except Exception as e:
        logger.error(f"‚ùå [TEST_RERANKER] Error in reranker test: {e}")
        import traceback
        logger.error(f"‚ùå [TEST_RERANKER] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/test-turbo-reasoning")
async def test_turbo_reasoning_endpoint():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    try:
        logger.info("üöÄ [TEST_TURBO_REASONING] Testing turbo reasoning functionality...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ä–≤–∏—Å —Ç—É—Ä–±–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        from services.turbo_reasoning_service import TurboReasoningService
        
        turbo_service = TurboReasoningService()
        
        # –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        test_message = "–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞—Ö"
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã
        test_results = {}
        
        # 1. –¢—É—Ä–±–æ —Ä–µ–∂–∏–º
        try:
            logger.info("üß™ [TEST_TURBO_REASONING] Testing turbo mode...")
            turbo_result = turbo_service.generate_response(
                message=test_message,
                turbo_mode=True
            )
            test_results["turbo"] = {
                "response_preview": turbo_result["response"][:200] + "..." if len(turbo_result["response"]) > 200 else turbo_result["response"],
                "generation_time_ms": turbo_result["generation_time_ms"],
                "tokens_used": turbo_result["tokens_used"],
                "reasoning_steps": turbo_result["reasoning_steps"]
            }
        except Exception as e:
            logger.error(f"‚ùå [TEST_TURBO_REASONING] Turbo mode error: {e}")
            test_results["turbo"] = {"error": str(e)}
        
        # 2. –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º
        try:
            logger.info("üß™ [TEST_TURBO_REASONING] Testing fast mode...")
            fast_result = turbo_service.generate_response(
                message=test_message,
                reasoning_depth="fast"
            )
            test_results["fast"] = {
                "response_preview": fast_result["response"][:200] + "..." if len(fast_result["response"]) > 200 else fast_result["response"],
                "generation_time_ms": fast_result["generation_time_ms"],
                "tokens_used": fast_result["tokens_used"],
                "reasoning_steps": fast_result["reasoning_steps"]
            }
        except Exception as e:
            logger.error(f"‚ùå [TEST_TURBO_REASONING] Fast mode error: {e}")
            test_results["fast"] = {"error": str(e)}
        
        # 3. –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        try:
            logger.info("üß™ [TEST_TURBO_REASONING] Testing balanced mode...")
            balanced_result = turbo_service.generate_response(
                message=test_message,
                reasoning_depth="balanced"
            )
            test_results["balanced"] = {
                "response_preview": balanced_result["response"][:200] + "..." if len(balanced_result["response"]) > 200 else balanced_result["response"],
                "generation_time_ms": balanced_result["generation_time_ms"],
                "tokens_used": balanced_result["tokens_used"],
                "reasoning_steps": balanced_result["reasoning_steps"]
            }
        except Exception as e:
            logger.error(f"‚ùå [TEST_TURBO_REASONING] Balanced mode error: {e}")
            test_results["balanced"] = {"error": str(e)}
        
        # 4. –ì–ª—É–±–æ–∫–∏–π —Ä–µ–∂–∏–º
        try:
            logger.info("üß™ [TEST_TURBO_REASONING] Testing deep mode...")
            deep_result = turbo_service.generate_response(
                message=test_message,
                reasoning_depth="deep"
            )
            test_results["deep"] = {
                "response_preview": deep_result["response"][:200] + "..." if len(deep_result["response"]) > 200 else deep_result["response"],
                "generation_time_ms": deep_result["generation_time_ms"],
                "tokens_used": deep_result["tokens_used"],
                "reasoning_steps": deep_result["reasoning_steps"]
            }
        except Exception as e:
            logger.error(f"‚ùå [TEST_TURBO_REASONING] Deep mode error: {e}")
            test_results["deep"] = {"error": str(e)}
        
        logger.info("‚úÖ [TEST_TURBO_REASONING] All modes tested successfully")
        
        return {
            "status": "success",
            "message": "Turbo reasoning test completed",
            "test_message": test_message,
            "results": test_results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [TEST_TURBO_REASONING] Error in turbo reasoning test: {e}")
        import traceback
        logger.error(f"‚ùå [TEST_TURBO_REASONING] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# –°–∏—Å—Ç–µ–º–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
# ============================================================================

@app.get("/health")
async def health_endpoint():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
        import requests
        ollama_response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        ollama_status = "healthy" if ollama_response.status_code == 200 else "unhealthy"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
        qdrant_response = requests.get("http://qdrant:6333/collections", timeout=5)
        qdrant_status = "healthy" if qdrant_response.status_code == 200 else "unhealthy"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
        rag_service = get_ollama_rag_service()
        try:
            rag_service.db_manager.execute_read_query("SELECT 1")
            postgres_status = "healthy"
        except:
            postgres_status = "unhealthy"
        
        return {
            "status": "healthy" if all(s == "healthy" for s in [ollama_status, qdrant_status, postgres_status]) else "degraded",
            "services": {
                "ollama": ollama_status,
                "qdrant": qdrant_status,
                "postgresql": postgres_status
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/metrics", response_class=PlainTextResponse)
async def metrics_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ Prometheus"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics = []
        metrics.append(f"# HELP rag_service_vectors_total Total number of vectors in Qdrant")
        metrics.append(f"# TYPE rag_service_vectors_total gauge")
        metrics.append(f"rag_service_vectors_total {stats.get('qdrant', {}).get('vectors_count', 0)}")
        
        metrics.append(f"# HELP rag_service_documents_total Total number of documents in PostgreSQL")
        metrics.append(f"# TYPE rag_service_documents_total gauge")
        metrics.append(f"rag_service_documents_total {stats.get('postgresql', {}).get('total_documents', 0)}")
        
        metrics.append(f"# HELP rag_service_chunks_total Total number of chunks in PostgreSQL")
        metrics.append(f"# TYPE rag_service_chunks_total gauge")
        metrics.append(f"rag_service_chunks_total {stats.get('postgresql', {}).get('total_chunks', 0)}")
        
        metrics.append(f"# HELP rag_service_tokens_total Total number of tokens in documents")
        metrics.append(f"# TYPE rag_service_tokens_total gauge")
        metrics.append(f"rag_service_tokens_total {stats.get('postgresql', {}).get('total_tokens', 0)}")
        
        return "\n".join(metrics)
        
    except Exception as e:
        logger.error(f"‚ùå [METRICS] Error getting metrics: {e}")
        return f"# Error getting metrics: {e}"

@app.post("/clear-collection")
async def clear_collection_endpoint():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant"""
    try:
        logger.info("üßπ [CLEAR_COLLECTION] Starting collection cleanup...")
        
        # –ü–æ–ª—É—á–∞–µ–º RAG —Å–µ—Ä–≤–∏—Å
        rag_service = get_ollama_rag_service()
        
        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        success = rag_service.clear_collection()
        
        if success:
            logger.info("‚úÖ [CLEAR_COLLECTION] Collection cleared successfully")
            return {
                "status": "success",
                "message": "Collection cleared successfully",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to clear collection")
            
    except Exception as e:
        logger.error(f"‚ùå [CLEAR_COLLECTION] Error clearing collection: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
# ============================================================================

# –ù–æ–≤—ã–µ endpoints –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
@app.post("/indexing/start")
async def start_indexing_service_endpoint():
    """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    from api.endpoints import start_indexing_service
    return start_indexing_service()

@app.post("/indexing/stop")
async def stop_indexing_service_endpoint():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    from api.endpoints import stop_indexing_service
    return stop_indexing_service()

@app.get("/indexing/status")
async def get_indexing_service_status_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–∞ —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    from api.endpoints import get_indexing_service_status
    return get_indexing_service_status()

@app.post("/indexing/retry-failed")
async def retry_failed_documents_endpoint(max_retries: int = 3):
    """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    from api.endpoints import retry_failed_documents
    return retry_failed_documents(max_retries)

@app.get("/indexing/pending")
async def get_pending_documents_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–∂–∏–¥–∞—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    from api.endpoints import get_pending_documents
    return get_pending_documents()

@app.get("/indexing/failed")
async def get_failed_documents_endpoint(max_retries: int = 3):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω–µ—É–¥–∞—á–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π"""
    from api.endpoints import get_failed_documents
    return get_failed_documents(max_retries)

@app.get("/database/health")
async def get_database_health_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    from api.endpoints import get_database_health
    return get_database_health()

@app.get("/")
async def root_endpoint():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "Ollama RAG Service",
        "version": "2.1.0",
        "description": "RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3",
        "features": {
            "resilient_indexing": True,
            "connection_retry": True,
            "automatic_recovery": True
        },
        "endpoints": {
            "search": "/search",
            "chat": "/chat",
            "models": "/models",
            "ntd_consultation": "/ntd-consultation/chat",
            "documents": "/documents",
            "reindex": "/reindex",
            "health": "/health",
            "metrics": "/metrics",
            "stats": "/stats",
            "indexing": {
                "start": "/indexing/start",
                "stop": "/indexing/stop",
                "status": "/indexing/status",
                "retry_failed": "/indexing/retry-failed",
                "pending": "/indexing/pending",
                "failed": "/indexing/failed"
            },
            "database": {
                "health": "/database/health"
            }
        },
        "timestamp": datetime.now().isoformat()
    }

# ============================================================================
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞
# ============================================================================

if __name__ == "__main__":
    logger.info("üöÄ [OLLAMA_RAG_SERVICE] Starting Ollama RAG Service...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
    try:
        import requests
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            bge_m3_available = any("bge-m3" in model.get("name", "") for model in models)
            if bge_m3_available:
                logger.info("‚úÖ [OLLAMA_RAG_SERVICE] BGE-M3 model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [OLLAMA_RAG_SERVICE] BGE-M3 model not found in Ollama")
        else:
            logger.error("‚ùå [OLLAMA_RAG_SERVICE] Cannot connect to Ollama")
    except Exception as e:
        logger.error(f"‚ùå [OLLAMA_RAG_SERVICE] Error checking Ollama: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
    uvicorn.run(
        "ollama_main:app",
        host="0.0.0.0",
        port=8003,
        reload=True,
        log_level="info"
    )
