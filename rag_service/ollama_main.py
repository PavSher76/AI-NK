from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
import hashlib
import time
import asyncio
import json
import logging
import os
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

# –ò–º–ø–æ—Ä—Ç Ollama RAG —Å–µ—Ä–≤–∏—Å–∞
from services.ollama_rag_service import OllamaRAGService

# –ú–æ–¥–µ–ª–∏ Pydantic
class NTDConsultationRequest(BaseModel):
    message: str
    user_id: str

class NTDConsultationResponse(BaseModel):
    status: str
    response: str
    sources: List[Dict[str, Any]]
    confidence: float
    documents_used: int
    timestamp: str

class SearchRequest(BaseModel):
    query: str
    k: int = 8
    document_filter: Optional[str] = None
    chapter_filter: Optional[str] = None
    chunk_type_filter: Optional[str] = None

# –ú–æ–¥–µ–ª–∏ Pydantic –¥–ª—è —á–∞—Ç–∞
class ChatRequest(BaseModel):
    message: str
    model: str = "gpt-oss:latest"
    history: Optional[List[Dict[str, str]]] = None
    max_tokens: Optional[int] = None

class ChatResponse(BaseModel):
    status: str
    response: str
    model: str
    timestamp: str
    tokens_used: Optional[int] = None
    generation_time_ms: Optional[float] = None

class EmbeddingRequest(BaseModel):
    text: str

class EmbeddingResponse(BaseModel):
    status: str
    embedding: List[float]
    text_length: int
    timestamp: str

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Ollama RAG Service",
    description="RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3",
    version="2.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama RAG —Å–µ—Ä–≤–∏—Å–∞ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
ollama_rag_service = None

def get_ollama_rag_service():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama RAG —Å–µ—Ä–≤–∏—Å–∞"""
    global ollama_rag_service
    if ollama_rag_service is None:
        ollama_rag_service = OllamaRAGService()
    return ollama_rag_service

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
# ============================================================================

@app.get("/documents")
async def documents_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return get_ollama_rag_service().get_documents()

@app.get("/stats")
async def stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå [STATS] Error getting stats: {e}")
        return {
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/documents/stats")
async def documents_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        return {
            "tokens": stats.get('postgresql', {}).get('total_tokens', 0),
            "chunks": stats.get('postgresql', {}).get('total_chunks', 0),
            "vectors": stats.get('qdrant', {}).get('vectors_count', 0),
            "documents": stats.get('postgresql', {}).get('total_documents', 0),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [DOCUMENTS_STATS] Error getting documents stats: {e}")
        return {
            "tokens": 0,
            "chunks": 0,
            "vectors": 0,
            "documents": 0,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/documents/{document_id}/chunks")
async def document_chunks_endpoint(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    return get_ollama_rag_service().get_document_chunks(document_id)

@app.delete("/documents/{document_id}")
async def delete_document_endpoint(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = get_ollama_rag_service().delete_document(document_id)
        if success:
            return {"status": "success", "message": f"Document {document_id} deleted successfully"}
        else:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOCUMENT] Error deleting document {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex")
async def reindex_documents_endpoint():
    """–†–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        logger.info("üîÑ [REINDEX] Starting document reindexing...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = get_ollama_rag_service().get_documents()
        
        if not documents:
            return {
                "status": "success",
                "message": "No documents to reindex",
                "reindexed_count": 0,
                "total_documents": 0
            }
        
        logger.info(f"üîÑ [REINDEX] Found {len(documents)} documents to reindex")
        
        reindexed_count = 0
        total_chunks = 0
        
        for document in documents:
            try:
                document_id = document['id']
                document_title = document['title']
                
                logger.info(f"üîÑ [REINDEX] Reindexing document {document_id}: {document_title}")
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                chunks = get_ollama_rag_service().get_document_chunks(document_id)
                
                if not chunks:
                    logger.warning(f"‚ö†Ô∏è [REINDEX] No chunks found for document {document_id}")
                    continue
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                chunks_for_indexing = []
                for chunk in chunks:
                    chunk_data = {
                        'id': chunk['chunk_id'],
                        'document_id': document_id,
                        'chunk_id': chunk['chunk_id'],
                        'content': chunk['content'],
                        'page': chunk.get('page_number', 1),
                        'section_title': chunk.get('chapter', ''),
                        'section': chunk.get('section', ''),
                        'document_title': document_title,
                        'title': document_title,
                        'code': get_ollama_rag_service().extract_document_code(document_title),
                        'category': document.get('category', ''),
                        'chunk_type': 'paragraph'
                    }
                    chunks_for_indexing.append(chunk_data)
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                success = get_ollama_rag_service().index_document_chunks(document_id, chunks_for_indexing)
                
                if success:
                    reindexed_count += 1
                    total_chunks += len(chunks)
                    logger.info(f"‚úÖ [REINDEX] Document {document_id} reindexed successfully ({len(chunks)} chunks)")
                else:
                    logger.error(f"‚ùå [REINDEX] Failed to index document {document_id}")
                
            except Exception as e:
                logger.error(f"‚ùå [REINDEX] Error reindexing document {document_id}: {e}")
                continue
        
        logger.info(f"‚úÖ [REINDEX] Reindexing completed. {reindexed_count}/{len(documents)} documents reindexed")
        
        return {
            "status": "success",
            "message": f"Reindexing completed. {reindexed_count} documents reindexed",
            "reindexed_count": reindexed_count,
            "total_documents": len(documents),
            "total_chunks": total_chunks
        }
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX] Reindexing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}/indexes")
async def delete_document_indexes_endpoint(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        success = get_ollama_rag_service().delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Indexes for document {document_id} deleted successfully",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
            
    except Exception as e:
        logger.error(f"‚ùå [DELETE_INDEXES] Error deleting indexes for document {document_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reindex")
async def reindex_documents_endpoint():
    """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        logger.info("üîÑ [REINDEX] Starting document reindexing...")
        
        rag_service = get_ollama_rag_service()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        with rag_service.db_manager.get_cursor() as cursor:
            cursor.execute("""
                SELECT ud.id, ud.original_filename as document_title, ud.category
                FROM uploaded_documents ud
                WHERE ud.processing_status = 'completed'
                ORDER BY ud.upload_date DESC
            """)
            documents = cursor.fetchall()
        
        if not documents:
            return {
                "status": "success",
                "message": "No documents to reindex",
                "documents_processed": 0,
                "timestamp": datetime.now().isoformat()
            }
        
        total_processed = 0
        total_chunks = 0
        
        for document in documents:
            try:
                document_id = document['id']
                document_title = document['document_title']
                
                logger.info(f"üìù [REINDEX] Processing document {document_id}: {document_title}")
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                chunks = rag_service.get_document_chunks(document_id)
                
                if chunks:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
                    for chunk in chunks:
                        chunk['document_title'] = document_title
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                    success = rag_service.index_document_chunks(document_id, chunks)
                    
                    if success:
                        total_processed += 1
                        total_chunks += len(chunks)
                        logger.info(f"‚úÖ [REINDEX] Successfully indexed document {document_id} with {len(chunks)} chunks")
                    else:
                        logger.error(f"‚ùå [REINDEX] Failed to index document {document_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è [REINDEX] No chunks found for document {document_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå [REINDEX] Error processing document {document.get('id', 'unknown')}: {e}")
                continue
        
        logger.info(f"‚úÖ [REINDEX] Reindexing completed. Processed {total_processed} documents with {total_chunks} chunks")
        
        return {
            "status": "success",
            "message": "Document reindexing completed",
            "documents_processed": total_processed,
            "total_chunks": total_chunks,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX] Error during reindexing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    category: str = Form("other"),
    description: str = Form("")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üì§ [UPLOAD] Uploading normative document: {file.filename}")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        if not file.filename.lower().endswith(('.pdf', '.docx', '.txt')):
            raise HTTPException(status_code=400, detail="Unsupported file type. Only PDF, DOCX, and TXT files are allowed.")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = await file.read()
        file_size = len(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 50 MB)
        max_file_size = 50 * 1024 * 1024  # 50 MB
        if file_size > max_file_size:
            raise HTTPException(
                status_code=413, 
                detail=f"File too large. Maximum size is {max_file_size // (1024*1024)} MB"
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        document_hash = hashlib.sha256(content).hexdigest()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_id = int(time.time() * 1000) % 100000000  # 8-–∑–Ω–∞—á–Ω–æ–µ —á–∏—Å–ª–æ
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        try:
            rag_service = get_ollama_rag_service()
            saved_document_id = rag_service.save_document_to_db(
                document_id=document_id,
                filename=file.filename,
                original_filename=file.filename,
                file_type=file.filename.split('.')[-1].lower(),
                file_size=file_size,
                document_hash=document_hash,
                category=category,
                document_type='normative'
            )
            
            logger.info(f"‚úÖ [UPLOAD] Document saved to database with ID: {saved_document_id}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîÑ [UPLOAD] Starting document processing for {saved_document_id}")
            asyncio.create_task(process_normative_document_async(saved_document_id, content, file.filename))
            
            return {
                "status": "success",
                "document_id": saved_document_id,
                "filename": file.filename,
                "file_size": file_size,
                "message": f"Document uploaded successfully and processing started"
            }
            
        except Exception as e:
            logger.error(f"‚ùå [UPLOAD] Database error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [UPLOAD] Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_normative_document_async(document_id: int, content: bytes, filename: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üîÑ [PROCESS] Starting async processing for document {document_id}")
    try:
        rag_service = get_ollama_rag_service()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "processing"
        rag_service.update_document_status(document_id, "processing")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        success = await rag_service.process_document_async(document_id, content, filename)
        
        if success:
            rag_service.update_document_status(document_id, "completed")
            logger.info(f"‚úÖ [PROCESS] Document {document_id} processed successfully")
        else:
            rag_service.update_document_status(document_id, "failed", "Processing failed")
            logger.error(f"‚ùå [PROCESS] Document {document_id} processing failed")
            
    except Exception as e:
        logger.error(f"‚ùå [PROCESS] Error processing document {document_id}: {e}")
        try:
            rag_service = get_ollama_rag_service()
            rag_service.update_document_status(document_id, "failed", str(e))
        except:
            pass

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
# ============================================================================

@app.post("/search")
async def search_norms(
    query: str = Form(...),
    k: int = Form(8),
    document_filter: Optional[str] = Form(None),
    chapter_filter: Optional[str] = Form(None),
    chunk_type_filter: Optional[str] = Form(None)
):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    start_time = datetime.now()
    logger.info(f"üîç [SEARCH_NORM] Performing hybrid search for query: '{query}' with k={k}")
    logger.info(f"üîç [SEARCH_NORM] Filters: document={document_filter}, chapter={chapter_filter}, chunk_type={chunk_type_filter}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º Ollama RAG —Å–µ—Ä–≤–∏—Å
        rag_service = get_ollama_rag_service()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model_logger.info(f"ü§ñ [EMBEDDING_REQUEST] Generating embeddings for query: '{query[:100]}...'")
        
        results = rag_service.hybrid_search(
            query=query,
            k=k,
            document_filter=document_filter,
            chapter_filter=chapter_filter,
            chunk_type_filter=chunk_type_filter
        )
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ [SEARCH_NORM] Hybrid search completed in {execution_time:.2f}s. Found {len(results)} results.")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        if results:
            top_result = results[0]
            model_logger.info(f"üìä [SEARCH_RESULTS] Top result: {top_result.get('document_title', 'Unknown')} - Score: {top_result.get('score', 0):.3f}")
            model_logger.debug(f"üìä [SEARCH_RESULTS] Top result content preview: {top_result.get('content', '')[:200]}...")
        
        return {
            "query": query,
            "results_count": len(results),
            "execution_time": execution_time,
            "results": results
        }
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.error(f"‚ùå [SEARCH_NORM] Search error after {execution_time:.2f}s: {type(e).__name__}: {str(e)}")
        model_logger.error(f"‚ùå [EMBEDDING_ERROR] Failed to process query: '{query[:100]}...' - {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î
# ============================================================================

@app.post("/ntd-consultation/chat")
async def ntd_consultation_chat_endpoint(request: NTDConsultationRequest):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î"""
    try:
        rag_service = get_ollama_rag_service()
        result = rag_service.get_ntd_consultation(request.message, request.user_id)
        return result
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/ntd-consultation/stats")
async def ntd_consultation_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        return {
            "status": "success",
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION_STATS] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/ntd-consultation/cache")
async def clear_consultation_cache_endpoint():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    return {
        "status": "success",
        "message": "Cache cleared successfully",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/ntd-consultation/cache/stats")
async def get_consultation_cache_stats_endpoint():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    return {
        "status": "success",
        "cache_stats": {
            "cache_type": "No caching implemented",
            "cache_size": 0,
            "cache_hits": 0,
            "cache_misses": 0
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """–ß–∞—Ç —Å –ò–ò —á–µ—Ä–µ–∑ Ollama"""
    try:
        logger.info(f"üí¨ [CHAT] Processing chat request with model: {request.model}")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —á–∞—Ç–∞
        import requests
        import time
        
        start_time = time.time()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
        payload = {
            "model": request.model,
            "prompt": request.message,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": request.max_tokens or 2048
            }
        }
        
        if request.history:
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ –ø—Ä–æ–º–ø—Ç
            context = "\n".join([f"User: {msg.get('user', '')}\nAssistant: {msg.get('assistant', '')}" for msg in request.history])
            payload["prompt"] = f"{context}\nUser: {request.message}\nAssistant:"
        
        response = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=120)
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "")
            
            generation_time = (time.time() - start_time) * 1000
            
            return ChatResponse(
                status="success",
                response=response_text,
                model=request.model,
                timestamp=datetime.now().isoformat(),
                tokens_used=result.get("eval_count", 0),
                generation_time_ms=generation_time
            )
        else:
            logger.error(f"‚ùå [CHAT] Ollama API error: {response.status_code} - {response.text}")
            raise HTTPException(status_code=500, detail=f"Ollama API error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"‚ùå [CHAT] Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def models_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        logger.info("üîç [MODELS] Getting available Ollama models")
        
        import requests
        
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            models = models_data.get("models", [])
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –¥–ª—è —á–∞—Ç–∞ (–∏—Å–∫–ª—é—á–∞–µ–º bge-m3)
            chat_models = [
                {
                    "name": model.get("name", ""),
                    "model": model.get("model", ""),
                    "size": model.get("size", 0),
                    "parameter_size": model.get("details", {}).get("parameter_size", ""),
                    "quantization": model.get("details", {}).get("quantization_level", ""),
                    "family": model.get("details", {}).get("family", "")
                }
                for model in models
                if "bge-m3" not in model.get("name", "").lower()
            ]
            
            return {
                "status": "success",
                "models": chat_models,
                "total_count": len(chat_models),
                "ollama_status": "healthy",
                "timestamp": datetime.now().isoformat()
            }
        else:
            logger.error(f"‚ùå [MODELS] Ollama API error: {response.status_code}")
            raise HTTPException(status_code=500, detail=f"Ollama API error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"‚ùå [MODELS] Error getting models: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/chat_documents_stats")
async def chat_documents_stats_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–∞—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏ chat_documents –∏–∑ Qdrant
        import requests
        
        qdrant_url = "http://qdrant:6333"
        collection_name = "chat_documents"
        
        try:
            response = requests.get(f"{qdrant_url}/collections/{collection_name}", timeout=10)
            if response.status_code == 200:
                result = response.json()
                return {
                    "total_documents": result['result']['points_count'],
                    "indexed_vectors": result['result']['indexed_vectors_count'],
                    "collection_status": result['result']['status'],
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "total_documents": 0,
                    "indexed_vectors": 0,
                    "collection_status": "not_found",
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            return {
                "total_documents": 0,
                "indexed_vectors": 0,
                "collection_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"‚ùå [CHAT_DOCUMENTS_STATS] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/embeddings")
async def create_embedding_endpoint(request: EmbeddingRequest):
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
    try:
        logger.info(f"üîç [EMBEDDINGS] Creating embedding for text: '{request.text[:100]}...'")
        
        rag_service = get_ollama_rag_service()
        embedding = rag_service.embedding_service.create_embedding(request.text)
        
        return EmbeddingResponse(
            status="success",
            embedding=embedding,
            text_length=len(request.text),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå [EMBEDDINGS] Error creating embedding: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/embeddings")
async def create_embedding_get_endpoint(text: str):
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (GET –∑–∞–ø—Ä–æ—Å)"""
    try:
        logger.info(f"üîç [EMBEDDINGS] Creating embedding for text: '{text[:100]}...'")
        
        rag_service = get_ollama_rag_service()
        embedding = rag_service.embedding_service.create_embedding(text)
        
        return EmbeddingResponse(
            status="success",
            embedding=embedding,
            text_length=len(text),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå [EMBEDDINGS] Error creating embedding: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –°–∏—Å—Ç–µ–º–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
# ============================================================================

@app.get("/health")
async def health_endpoint():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
        import requests
        ollama_response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        ollama_status = "healthy" if ollama_response.status_code == 200 else "unhealthy"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
        qdrant_response = requests.get("http://qdrant:6333/collections", timeout=5)
        qdrant_status = "healthy" if qdrant_response.status_code == 200 else "unhealthy"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
        rag_service = get_ollama_rag_service()
        try:
            with rag_service.db_manager.get_cursor() as cursor:
                cursor.execute("SELECT 1")
            postgres_status = "healthy"
        except:
            postgres_status = "unhealthy"
        
        return {
            "status": "healthy" if all(s == "healthy" for s in [ollama_status, qdrant_status, postgres_status]) else "degraded",
            "services": {
                "ollama": ollama_status,
                "qdrant": qdrant_status,
                "postgresql": postgres_status
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/metrics")
async def metrics_endpoint():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ Prometheus"""
    try:
        rag_service = get_ollama_rag_service()
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics = []
        metrics.append(f"# HELP rag_service_vectors_total Total number of vectors in Qdrant")
        metrics.append(f"# TYPE rag_service_vectors_total gauge")
        metrics.append(f"rag_service_vectors_total {stats.get('qdrant', {}).get('vectors_count', 0)}")
        
        metrics.append(f"# HELP rag_service_documents_total Total number of documents in PostgreSQL")
        metrics.append(f"# TYPE rag_service_documents_total gauge")
        metrics.append(f"rag_service_documents_total {stats.get('postgresql', {}).get('total_documents', 0)}")
        
        metrics.append(f"# HELP rag_service_chunks_total Total number of chunks in PostgreSQL")
        metrics.append(f"# TYPE rag_service_chunks_total gauge")
        metrics.append(f"rag_service_chunks_total {stats.get('postgresql', {}).get('total_chunks', 0)}")
        
        metrics.append(f"# HELP rag_service_tokens_total Total number of tokens in documents")
        metrics.append(f"# TYPE rag_service_tokens_total gauge")
        metrics.append(f"rag_service_tokens_total {stats.get('postgresql', {}).get('total_tokens', 0)}")
        
        return "\n".join(metrics)
        
    except Exception as e:
        logger.error(f"‚ùå [METRICS] Error getting metrics: {e}")
        return f"# Error getting metrics: {e}"

@app.post("/clear-collection")
async def clear_collection_endpoint():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant"""
    try:
        logger.info("üßπ [CLEAR_COLLECTION] Starting collection cleanup...")
        
        # –ü–æ–ª—É—á–∞–µ–º RAG —Å–µ—Ä–≤–∏—Å
        rag_service = get_ollama_rag_service()
        
        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        success = rag_service.clear_collection()
        
        if success:
            logger.info("‚úÖ [CLEAR_COLLECTION] Collection cleared successfully")
            return {
                "status": "success",
                "message": "Collection cleared successfully",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to clear collection")
            
    except Exception as e:
        logger.error(f"‚ùå [CLEAR_COLLECTION] Error clearing collection: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
# ============================================================================

@app.get("/")
async def root_endpoint():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "Ollama RAG Service",
        "version": "2.0.0",
        "description": "RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3",
        "endpoints": {
            "search": "/search",
            "chat": "/chat",
            "models": "/models",
            "ntd_consultation": "/ntd-consultation/chat",
            "documents": "/documents",
            "reindex": "/reindex",
            "health": "/health",
            "metrics": "/metrics",
            "stats": "/stats"
        },
        "timestamp": datetime.now().isoformat()
    }

# ============================================================================
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞
# ============================================================================

if __name__ == "__main__":
    logger.info("üöÄ [OLLAMA_RAG_SERVICE] Starting Ollama RAG Service...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
    try:
        import requests
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            bge_m3_available = any("bge-m3" in model.get("name", "") for model in models)
            if bge_m3_available:
                logger.info("‚úÖ [OLLAMA_RAG_SERVICE] BGE-M3 model is available in Ollama")
            else:
                logger.warning("‚ö†Ô∏è [OLLAMA_RAG_SERVICE] BGE-M3 model not found in Ollama")
        else:
            logger.error("‚ùå [OLLAMA_RAG_SERVICE] Cannot connect to Ollama")
    except Exception as e:
        logger.error(f"‚ùå [OLLAMA_RAG_SERVICE] Error checking Ollama: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
    uvicorn.run(
        "ollama_main:app",
        host="0.0.0.0",
        port=8003,
        reload=True,
        log_level="info"
    )
