from fastapi import HTTPException
from datetime import datetime
from typing import List, Dict, Any
from services.rag_service import RAGService
from core.config import logger

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
rag_service = None

def get_rag_service():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ RAG —Å–µ—Ä–≤–∏—Å–∞ —Å –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    global rag_service
    if rag_service is None:
        rag_service = RAGService()
    return rag_service

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
reindex_results = {}

def get_documents():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    logger.info("üìÑ [GET_DOCUMENTS] Getting documents list...")
    try:
        rag_service = get_rag_service()
        documents = rag_service.get_documents()
        logger.info(f"‚úÖ [GET_DOCUMENTS] Documents list retrieved: {len(documents)} documents")
        return {"documents": documents}
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENTS] Documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def get_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
    logger.info("üìä [GET_STATS] Getting service statistics...")
    try:
        rag_service_instance = get_rag_service()
        stats = rag_service_instance.get_stats()
        logger.info(f"‚úÖ [GET_STATS] Service statistics retrieved: {stats}")
        return stats
    except Exception as e:
        logger.error(f"‚ùå [GET_STATS] Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def get_document_chunks(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üìÑ [GET_DOCUMENT_CHUNKS] Getting chunks for document ID: {document_id}")
    try:
        rag_service_instance = get_rag_service()
        chunks_info = rag_service_instance.get_document_chunks(document_id)
        logger.info(f"‚úÖ [GET_DOCUMENT_CHUNKS] Chunks info retrieved for document {document_id}")
        return {"chunks": chunks_info}
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Chunks error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def get_documents_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
    logger.info("üìä [GET_DOCUMENTS_STATS] Getting documents statistics...")
    try:
        rag_service_instance = get_rag_service()
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = rag_service_instance.get_documents_from_uploaded('normative')
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_documents = len(documents)
        indexed_documents = len([doc for doc in documents if doc.get('processing_status') == 'completed'])
        indexing_progress_percent = (indexed_documents / total_documents * 100) if total_documents > 0 else 0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories_stats = {}
        for doc in documents:
            category = doc.get('category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            if category not in categories_stats:
                categories_stats[category] = 0
            categories_stats[category] += 1
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        categories_list = [
            {"category": cat, "count": count} 
            for cat, count in categories_stats.items()
        ]
        
        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        total_tokens = sum(doc.get('token_count', 0) for doc in documents)
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        adapted_stats = {
            "statistics": {
                "total_documents": total_documents,
                "indexed_documents": indexed_documents,
                "indexing_progress_percent": round(indexing_progress_percent, 1),
                "total_tokens": total_tokens,
                "categories": categories_list
            }
        }
        
        logger.info(f"‚úÖ [GET_DOCUMENTS_STATS] Documents statistics retrieved: {adapted_stats}")
        return adapted_stats
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENTS_STATS] Documents stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def delete_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å–æ–≤"""
    logger.info(f"üóëÔ∏è [DELETE_DOCUMENT] Deleting document ID: {document_id}")
    try:
        rag_service_instance = get_rag_service()
        success = rag_service_instance.delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Document {document_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOCUMENT] Delete document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def delete_document_indexes(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üóëÔ∏è [DELETE_DOC_INDEXES] Deleting indexes for document ID: {document_id}")
    try:
        rag_service_instance = get_rag_service()
        success = rag_service_instance.delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Indexes for document {document_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Document indexes not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOC_INDEXES] Delete indexes error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def reindex_documents():
    """–†–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    logger.info("üîÑ [REINDEX_DOCUMENTS] Starting document reindexing...")
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = []
        with rag_service.db_manager.get_cursor() as cursor:
            cursor.execute("""
                SELECT DISTINCT 
                    ud.id as document_id,
                    ud.filename as document_title,
                    ud.category,
                    ud.processing_status
                FROM uploaded_documents ud
                WHERE ud.processing_status IN ('completed', 'pending')
                ORDER BY ud.id
            """)
            
            documents = cursor.fetchall()
        
        if not documents:
            return {
                "status": "success",
                "message": "No documents to reindex",
                "reindexed_count": 0,
                "total_documents": 0
            }
        
        logger.info(f"üîÑ [REINDEX_DOCUMENTS] Found {len(documents)} documents to reindex")
        
        reindexed_count = 0
        total_tokens = 0
        
        for document in documents:
            try:
                document_id = document['document_id']
                document_title = document['document_title']
                
                logger.info(f"üîÑ [REINDEX_DOCUMENTS] Reindexing document {document_id}: {document_title}")
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º
                chunks = []
                try:
                    with rag_service.db_manager.get_cursor() as cursor:
                        cursor.execute("""
                            SELECT 
                                chunk_id,
                                content,
                                page_number,
                                chapter,
                                section
                            FROM normative_chunks 
                            WHERE document_id = %s
                            ORDER BY page_number, chunk_id
                        """, (document_id,))
                        chunks = cursor.fetchall()
                except Exception as e:
                    logger.error(f"‚ùå [REINDEX_DOCUMENTS] Error getting chunks for document {document_id}: {e}")
                    # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
                    rag_service.db_manager.reconnect()
                    with rag_service.db_manager.get_cursor() as cursor:
                        cursor.execute("""
                            SELECT 
                                chunk_id,
                                content,
                                page_number,
                                chapter,
                                section
                            FROM normative_chunks 
                            WHERE document_id = %s
                            ORDER BY page_number, chunk_id
                        """, (document_id,))
                        chunks = cursor.fetchall()
                
                if not chunks:
                    logger.warning(f"‚ö†Ô∏è [REINDEX_DOCUMENTS] No chunks found for document {document_id}")
                    continue
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                chunks_for_indexing = []
                for chunk in chunks:
                    chunk_data = {
                        'id': chunk['chunk_id'],  # ID –¥–ª—è Qdrant
                        'document_id': document_id,
                        'chunk_id': chunk['chunk_id'],
                        'content': chunk['content'],
                        'page': chunk['page_number'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'page' –≤–º–µ—Å—Ç–æ 'page_number'
                        'section_title': chunk['chapter'] or '',
                        'section': chunk['section'] or '',  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ 'section'
                        'document_title': document_title,
                        'category': document['category'],
                        'chunk_type': 'paragraph'  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
                    }
                    chunks_for_indexing.append(chunk_data)
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
                success = rag_service.index_document_chunks(document_id, chunks_for_indexing)
                
                if success:
                    reindexed_count += 1
                    total_tokens += sum(len(chunk['content'].split()) for chunk in chunks)
                    logger.info(f"‚úÖ [REINDEX_DOCUMENTS] Document {document_id} reindexed successfully ({len(chunks)} chunks)")
                else:
                    logger.error(f"‚ùå [REINDEX_DOCUMENTS] Failed to index document {document_id}")
                
            except Exception as e:
                logger.error(f"‚ùå [REINDEX_DOCUMENTS] Error reindexing document {document.get('document_id', 'unknown')}: {e}")
                continue
        
        logger.info(f"‚úÖ [REINDEX_DOCUMENTS] Reindexing completed. {reindexed_count}/{len(documents)} documents reindexed")
        
        return {
            "status": "success",
            "message": f"Reindexing completed. {reindexed_count} documents reindexed",
            "reindexed_count": reindexed_count,
            "total_documents": len(documents),
            "total_tokens": total_tokens,
            "new_total_tokens": total_tokens
        }
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX_DOCUMENTS] Reindexing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def start_async_reindex():
    """–ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    logger.info("üöÄ [ASYNC_REINDEX] Starting async reindexing...")
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        documents = []
        with rag_service.db_manager.get_cursor() as cursor:
            cursor.execute("""
                SELECT DISTINCT 
                    ud.id as document_id,
                    ud.filename as document_title,
                    ud.category,
                    ud.processing_status
                FROM uploaded_documents ud
                                        WHERE ud.processing_status IN ('completed', 'pending')
                ORDER BY ud.id
            """)
            
            documents = cursor.fetchall()
        
        if not documents:
            return {
                "status": "success",
                "message": "No documents to reindex",
                "task_id": "no_documents",
                "total_documents": 0
            }
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
        import uuid
        task_id = str(uuid.uuid4())
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É
        import asyncio
        import threading
        
        def run_reindex_task():
            try:
                reindexed_count = 0
                total_tokens = 0
                
                for i, document in enumerate(documents):
                    try:
                        document_id = document['document_id']
                        document_title = document['document_title']
                        
                        logger.info(f"üîÑ [ASYNC_REINDEX] Reindexing document {document_id}: {document_title} ({i+1}/{len(documents)})")
                        
                        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        chunks = []
                        with rag_service.db_manager.get_cursor() as cursor:
                            cursor.execute("""
                                SELECT 
                                    chunk_id,
                                    content,
                                    page_number,
                                    section_title,
                                    subsection_title
                                FROM normative_chunks 
                                WHERE document_id = %s
                                ORDER BY page_number, chunk_id
                            """, (document_id,))
                            
                            chunks = cursor.fetchall()
                        
                        if not chunks:
                            logger.warning(f"‚ö†Ô∏è [ASYNC_REINDEX] No chunks found for document {document_id}")
                            continue
                        
                        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                        chunks_for_indexing = []
                        for chunk in chunks:
                            chunk_data = {
                                'id': chunk['chunk_id'],  # ID –¥–ª—è Qdrant
                                'document_id': document_id,
                                'chunk_id': chunk['chunk_id'],
                                'content': chunk['content'],
                                'page': chunk['page_number'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'page' –≤–º–µ—Å—Ç–æ 'page_number'
                                'section_title': chunk['section_title'] or '',
                                'section': chunk['subsection_title'] or '',  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ 'section'
                                'document_title': document_title,
                                'category': document['category'],
                                'chunk_type': 'paragraph'  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
                            }
                            chunks_for_indexing.append(chunk_data)
                        
                        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
                        success = rag_service.index_document_chunks(document_id, chunks_for_indexing)
                        
                        if success:
                            reindexed_count += 1
                            total_tokens += sum(len(chunk['content'].split()) for chunk in chunks)
                            logger.info(f"‚úÖ [ASYNC_REINDEX] Document {document_id} reindexed successfully ({len(chunks)} chunks)")
                        else:
                            logger.error(f"‚ùå [ASYNC_REINDEX] Failed to index document {document_id}")
                        
                    except Exception as e:
                        logger.error(f"‚ùå [ASYNC_REINDEX] Error reindexing document {document.get('document_id', 'unknown')}: {e}")
                        continue
                
                logger.info(f"‚úÖ [ASYNC_REINDEX] Async reindexing completed. {reindexed_count}/{len(documents)} documents reindexed")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
                reindex_results[task_id] = {
                    "status": "completed",
                    "reindexed_count": reindexed_count,
                    "total_documents": len(documents),
                    "total_tokens": total_tokens,
                    "message": f"Reindexing completed. {reindexed_count} documents reindexed"
                }
                
            except Exception as e:
                logger.error(f"‚ùå [ASYNC_REINDEX] Task error: {e}")
                reindex_results[task_id] = {
                    "status": "error",
                    "error": str(e),
                    "message": "Reindexing failed"
                }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        thread = threading.Thread(target=run_reindex_task)
        thread.daemon = True
        thread.start()
        
        return {
            "status": "started",
            "message": f"Async reindexing started for {len(documents)} documents",
            "task_id": task_id,
            "total_documents": len(documents)
        }
        
    except Exception as e:
        logger.error(f"‚ùå [ASYNC_REINDEX] Start error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def get_reindex_status(task_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    logger.info(f"üìä [REINDEX_STATUS] Getting status for task {task_id}")
    try:
        global reindex_results
        if task_id not in reindex_results:
            return {
                "status": "not_found",
                "message": "Task not found"
            }
        
        result = reindex_results[task_id]
        return result
        
    except Exception as e:
        logger.error(f"‚ùå [REINDEX_STATUS] Status error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def ntd_consultation_chat(message: str, user_id: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î"""
    logger.info("üí¨ [NTD_CONSULTATION] Chat request received")
    try:
        rag_service_instance = get_rag_service()
        response = rag_service_instance.get_ntd_consultation(message, [])
        
        logger.info(f"‚úÖ [NTD_CONSULTATION] Response generated successfully")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION] Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def ntd_consultation_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    logger.info("üìä [NTD_CONSULTATION_STATS] Getting consultation statistics...")
    try:
        rag_service_instance = get_rag_service()
        stats = rag_service_instance.get_stats()
        
        logger.info(f"‚úÖ [NTD_CONSULTATION_STATS] Statistics retrieved successfully")
        return {
            "status": "success",
            "consultation_stats": {
                "total_consultations": 0,  # –ü–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
                "active_sessions": 0,
                "documents_available": stats.get("documents", {}).get("total_documents", 0)
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION_STATS] Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def clear_consultation_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    logger.info("üóëÔ∏è [NTD_CONSULTATION_CACHE] Cache clear request received")
    try:
        logger.info("‚úÖ [NTD_CONSULTATION_CACHE] Cache cleared successfully")
        return {
            "status": "success",
            "message": "Cache cleared successfully",
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION_CACHE] Cache clear error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def get_consultation_cache_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
    logger.info("üìä [NTD_CONSULTATION_CACHE_STATS] Cache stats request received")
    try:
        logger.info("‚úÖ [NTD_CONSULTATION_CACHE_STATS] Cache stats retrieved successfully")
        return {
            "status": "success",
            "cache_stats": {
                "cache_size": 0,
                "cache_hits": 0,
                "cache_misses": 0,
                "cache_entries": 0
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [NTD_CONSULTATION_CACHE_STATS] Cache stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    logger.info("üí™ [HEALTH] Performing health check...")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        rag_service.db_manager.get_cursor().execute("SELECT 1")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Qdrant —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å
        import requests
        from core.config import QDRANT_URL
        response = requests.get(f"{QDRANT_URL}/collections")
        if response.status_code != 200:
            raise Exception("Qdrant connection failed")
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "embedding_model": "BGE-M3" if rag_service.embedding_model else "simple_hash",
            "optimized_indexer": "not_available",  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω
            "services": {
                "postgresql": "connected",
                "qdrant": "connected"
            }
        }
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def get_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ Prometheus"""
    logger.info("üìä [METRICS] Getting service metrics...")
    try:
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics_lines = []
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics_lines.append("# HELP rag_service_up Service is up")
        metrics_lines.append("# TYPE rag_service_up gauge")
        metrics_lines.append("rag_service_up 1")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        from core.config import CHUNK_SIZE, CHUNK_OVERLAP, MAX_TOKENS
        metrics_lines.append(f"# HELP rag_service_chunk_size Chunk size for text processing")
        metrics_lines.append(f"# TYPE rag_service_chunk_size gauge")
        metrics_lines.append(f"rag_service_chunk_size {CHUNK_SIZE}")
        
        metrics_lines.append(f"# HELP rag_service_chunk_overlap Chunk overlap for text processing")
        metrics_lines.append(f"# TYPE rag_service_chunk_overlap gauge")
        metrics_lines.append(f"rag_service_chunk_overlap {CHUNK_OVERLAP}")
        
        metrics_lines.append(f"# HELP rag_service_max_tokens Maximum tokens for processing")
        metrics_lines.append(f"# TYPE rag_service_max_tokens gauge")
        metrics_lines.append(f"rag_service_max_tokens {MAX_TOKENS}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
        metrics_lines.append(f"# HELP rag_service_connections_status Connection status")
        metrics_lines.append(f"# TYPE rag_service_connections_status gauge")
        metrics_lines.append(f'rag_service_connections_status{{service="postgresql"}} {1 if rag_service.db_manager.connection else 0}')
        metrics_lines.append(f'rag_service_connections_status{{service="qdrant"}} {1 if rag_service.qdrant_client else 0}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if stats:
            metrics_lines.append(f"# HELP rag_service_total_chunks Total number of chunks")
            metrics_lines.append(f"# TYPE rag_service_total_chunks counter")
            metrics_lines.append(f"rag_service_total_chunks {stats.get('total_chunks', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_total_documents Total number of documents")
            metrics_lines.append(f"# TYPE rag_service_total_documents counter")
            metrics_lines.append(f"rag_service_total_documents {stats.get('total_documents', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_total_clauses Total number of clauses")
            metrics_lines.append(f"# TYPE rag_service_total_clauses counter")
            metrics_lines.append(f"rag_service_total_clauses {stats.get('total_clauses', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_vector_indexed Total vector indexed")
            metrics_lines.append(f"# TYPE rag_service_vector_indexed counter")
            metrics_lines.append(f"rag_service_vector_indexed {stats.get('vector_indexed', 0)}")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º —á–∞–Ω–∫–æ–≤
            chunk_types = stats.get('chunk_types', {})
            for chunk_type, count in chunk_types.items():
                metrics_lines.append(f'rag_service_chunks_by_type{{type="{chunk_type}"}} {count}')
        
        logger.info(f"‚úÖ [METRICS] Service metrics retrieved successfully")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        from fastapi.responses import Response
        return Response(
            content="\n".join(metrics_lines),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
        
    except Exception as e:
        logger.error(f"‚ùå [METRICS] Metrics error: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        error_metrics = [
            "# HELP rag_service_up Service is up",
            "# TYPE rag_service_up gauge",
            "rag_service_up 0"
        ]
        from fastapi.responses import Response
        return Response(
            content="\n".join(error_metrics),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
