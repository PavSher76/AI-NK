"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPT-4o-mini
"""

import logging
import requests
import os
from typing import List, Dict, Any, Optional
import time

logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
from .openai_service import OpenAIService


class TurboReasoningService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPT-4o-mini"""
    
    def __init__(self, ollama_url: str = None):
        self.ollama_url = ollama_url or OLLAMA_URL
        self.model_name = "llama3.1:8b"
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞
        self.openai_service = OpenAIService()
        
        logger.info(f"üöÄ [TURBO_REASONING] Initialized with {self.model_name} at {self.ollama_url}")
        logger.info(f"ü§ñ [TURBO_REASONING] OpenAI service initialized for turbo mode")
    
    def generate_response(self, message: str, history: Optional[List[Dict[str, str]]] = None,
                        turbo_mode: bool = False, reasoning_depth: str = "balanced",
                        max_tokens: Optional[int] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º —Ä–µ–∂–∏–º–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
            turbo_mode: –í–∫–ª—é—á–∏—Ç—å —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o-mini)
            reasoning_depth: –ì–ª—É–±–∏–Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è ("fast", "balanced", "deep")
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –í —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI GPT-4o-mini
            if turbo_mode:
                logger.info(f"üöÄ [TURBO_REASONING] Using GPT-4o-mini for turbo mode")
                return self._generate_openai_response(message, history, max_tokens)
            
            # –í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å GPT-OSS
            return self._generate_ollama_response(message, history, reasoning_depth, max_tokens)
                
        except Exception as e:
            logger.error(f"‚ùå [TURBO_REASONING] Error generating response: {e}")
            raise e
    
    def _generate_openai_response(self, message: str, history: Optional[List[Dict[str, str]]] = None,
                                 max_tokens: Optional[int] = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API (—Ç—É—Ä–±–æ —Ä–µ–∂–∏–º)"""
        try:
            result = self.openai_service.generate_response(
                message=message,
                history=history,
                model="gpt-4o-mini",
                max_tokens=max_tokens,
                turbo_mode=True
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            result.update({
                "reasoning_depth": "turbo",
                "reasoning_steps": 1,  # –¢—É—Ä–±–æ —Ä–µ–∂–∏–º - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —à–∞–≥–∏
                "model": "gpt-4o-mini"
            })
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [TURBO_REASONING] OpenAI API error: {e}")
            # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            logger.info("üîÑ [TURBO_REASONING] Falling back to local GPT-OSS model")
            return self._generate_ollama_response(message, history, "fast", max_tokens)
    
    def _generate_ollama_response(self, message: str, history: Optional[List[Dict[str, str]]] = None,
                                 reasoning_depth: str = "balanced", max_tokens: Optional[int] = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama API (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)"""
        try:
            start_time = time.time()
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            if reasoning_depth == "fast":
                temperature = 0.4
                top_p = 0.85
                top_k = 30
                repeat_penalty = 1.15
                max_tokens = max_tokens or 1536
                reasoning_prompt = self._get_fast_reasoning_prompt()
            elif reasoning_depth == "deep":
                temperature = 0.7
                top_p = 0.95
                top_k = 50
                repeat_penalty = 1.2
                max_tokens = max_tokens or 3072
                reasoning_prompt = self._get_deep_reasoning_prompt()
            else:  # balanced
                temperature = 0.6
                top_p = 0.9
                top_k = 40
                repeat_penalty = 1.15
                max_tokens = max_tokens or 2048
                reasoning_prompt = self._get_balanced_reasoning_prompt()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            context = self._build_context(message, history, reasoning_prompt)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            payload = {
                "model": self.model_name,
                "prompt": context,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repeat_penalty": repeat_penalty,
                    "num_predict": max_tokens
                }
            }
            
            logger.info(f"üöÄ [TURBO_REASONING] Generating response with Ollama, depth: {reasoning_depth}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=300 if reasoning_depth == "deep" else 120
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "")
                
                generation_time = (time.time() - start_time) * 1000
                tokens_used = result.get("eval_count", 0)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
                reasoning_steps = self._count_reasoning_steps(response_text, reasoning_depth)
                
                logger.info(f"‚úÖ [TURBO_REASONING] Generated response in {generation_time:.1f}ms, tokens: {tokens_used}")
                
                return {
                    "response": response_text,
                    "tokens_used": tokens_used,
                    "generation_time_ms": generation_time,
                    "turbo_mode": False,
                    "reasoning_depth": reasoning_depth,
                    "reasoning_steps": reasoning_steps,
                    "model": self.model_name
                }
            else:
                raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå [TURBO_REASONING] Ollama API error: {e}")
            raise e
    
    def _build_context(self, message: str, history: Optional[List[Dict[str, str]]] = None, 
                      reasoning_prompt: str = "") -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        context_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é
        if reasoning_prompt:
            context_parts.append(reasoning_prompt)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
        if history:
            for msg in history:
                user_msg = msg.get('user', '')
                assistant_msg = msg.get('assistant', '')
                if user_msg and assistant_msg:
                    context_parts.append(f"User: {user_msg}")
                    context_parts.append(f"Assistant: {assistant_msg}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        context_parts.append(f"User: {message}")
        context_parts.append("Assistant:")
        
        return "\n\n".join(context_parts)
    
    def _get_fast_reasoning_prompt(self) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. 
–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–∞–≤–∞–π –ø—Ä—è–º—ã–µ –æ—Ç–≤–µ—Ç—ã."""
    
    def _get_balanced_reasoning_prompt(self) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ.
–ò—Å–ø–æ–ª—å–∑—É–π –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ä–∞–∑–±–∏–≤–∞–π —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —á–∞—Å—Ç–∏ –∏ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–∏ –≤—ã–≤–æ–¥—ã."""
    
    def _get_deep_reasoning_prompt(self) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –≥–ª—É–±–æ–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ.

–ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π:
1. –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
2. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–∞
3. –ü—Ä–∏–º–µ—Ä—ã –∏ –∞–Ω–∞–ª–æ–≥–∏–∏
4. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã
5. –í–æ–∑–º–æ–∂–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã

–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é –º–∞—Ä–∫–µ—Ä–æ–≤ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""
    
    def _count_reasoning_steps(self, response: str, reasoning_depth: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ"""
        try:
            # –°—á–∏—Ç–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            step_indicators = [
                r'\d+\.',           # 1., 2., 3.
                r'‚Ä¢\s',             # ‚Ä¢ 
                r'-\s',             # -
                r'‚Üí\s',             # ‚Üí
                r'–®–∞–≥\s*\d+',       # –®–∞–≥ 1
                r'–≠—Ç–∞–ø\s*\d+',      # –≠—Ç–∞–ø 1
                r'–°–Ω–∞—á–∞–ª–∞',          # –°–Ω–∞—á–∞–ª–∞
                r'–ó–∞—Ç–µ–º',           # –ó–∞—Ç–µ–º
                r'–î–∞–ª–µ–µ',           # –î–∞–ª–µ–µ
                r'–ù–∞–∫–æ–Ω–µ—Ü',         # –ù–∞–∫–æ–Ω–µ—Ü
                r'–í –∏—Ç–æ–≥–µ',         # –í –∏—Ç–æ–≥–µ
                r'–í—ã–≤–æ–¥',           # –í—ã–≤–æ–¥
                r'–ó–∞–∫–ª—é—á–µ–Ω–∏–µ'       # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
            ]
            
            import re
            total_steps = 0
            
            for pattern in step_indicators:
                matches = re.findall(pattern, response, re.IGNORECASE)
                total_steps += len(matches)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –º–∞—Ä–∫–µ—Ä—ã, –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            if total_steps == 0:
                if reasoning_depth == "deep":
                    total_steps = max(3, len(response.split('\n')) // 5)
                elif reasoning_depth == "balanced":
                    total_steps = max(2, len(response.split('\n')) // 8)
                else:  # fast
                    total_steps = 1
            
            return total_steps
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [TURBO_REASONING] Error counting reasoning steps: {e}")
            return 1
    
    def get_reasoning_modes(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return {
            "fast": {
                "name": "–ë—ã—Å—Ç—Ä—ã–π",
                "description": "–ö—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã, –ø—Ä–æ—Å—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                "temperature": 0.4,
                "max_tokens": 1024,
                "estimated_time": "5-15 —Å–µ–∫—É–Ω–¥"
            },
            "balanced": {
                "name": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π",
                "description": "–ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Å –ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏",
                "temperature": 0.6,
                "max_tokens": 2048,
                "estimated_time": "15-30 —Å–µ–∫—É–Ω–¥"
            },
            "deep": {
                "name": "–ì–ª—É–±–æ–∫–∏–π",
                "description": "–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏",
                "temperature": 0.7,
                "max_tokens": 3072,
                "estimated_time": "30-60 —Å–µ–∫—É–Ω–¥"
            },
            "turbo": {
                "name": "–¢—É—Ä–±–æ",
                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã",
                "temperature": 0.3,
                "max_tokens": 1024,
                "estimated_time": "3-10 —Å–µ–∫—É–Ω–¥"
            }
        }
    
    def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
            ollama_healthy = False
            try:
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    model_names = [model.get("name") for model in models]
                    ollama_healthy = self.model_name in model_names
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [TURBO_REASONING] Ollama health check failed: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º OpenAI
            openai_healthy = self.openai_service.health_check()
            
            # –°–µ—Ä–≤–∏—Å –∑–¥–æ—Ä–æ–≤, –µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∏–∑ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
            is_healthy = ollama_healthy or openai_healthy
            
            logger.info(f"üè• [TURBO_REASONING] Health check - Ollama: {ollama_healthy}, OpenAI: {openai_healthy}, Overall: {is_healthy}")
            
            return is_healthy
            
        except Exception as e:
            logger.error(f"‚ùå [TURBO_REASONING] Health check failed: {e}")
            return False
