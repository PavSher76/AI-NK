"""
–°–µ—Ä–≤–∏—Å MMR (Maximal Marginal Relevance) –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import re
from collections import defaultdict
import math

logger = logging.getLogger(__name__)

@dataclass
class MMRResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç MMR —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: str
    score: float
    mmr_score: float
    relevance_score: float
    diversity_score: float
    document_id: str
    chunk_id: str
    code: str
    document_title: str
    section_title: str
    content: str
    chunk_type: str
    page: int
    section: str
    metadata: Dict[str, Any]
    search_type: str
    rank: int = 0

class MMRService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è MMR (Maximal Marginal Relevance) –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, lambda_param: float = 0.7, similarity_threshold: float = 0.8):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MMR —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            lambda_param: –ü–∞—Ä–∞–º–µ—Ç—Ä –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º (0.0-1.0)
                        0.0 - —Ç–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, 1.0 - —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        self.lambda_param = lambda_param
        self.similarity_threshold = similarity_threshold
        
        logger.info(f"üîÑ [MMR] Initialized with lambda={lambda_param}, threshold={similarity_threshold}")
    
    def diversify_results(self, results: List[Dict[str, Any]], k: int = 8, 
                         query: str = "", use_semantic_similarity: bool = True) -> List[MMRResult]:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ MMR –¥–ª—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            use_semantic_similarity: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ
        
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        try:
            if not results or len(results) <= k:
                logger.info(f"‚úÖ [MMR] Not enough results for diversification: {len(results)} <= {k}")
                return self._convert_to_mmr_results(results)
            
            logger.info(f"üîÑ [MMR] Diversifying {len(results)} results ‚Üí {k} with lambda={self.lambda_param}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MMR —Ñ–æ—Ä–º–∞—Ç
            mmr_results = self._convert_to_mmr_results(results)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –∑–∞–ø—Ä–æ—Å—É
            if query:
                self._compute_relevance_scores(mmr_results, query)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º MMR –∞–ª–≥–æ—Ä–∏—Ç–º
            diversified_results = self._apply_mmr_algorithm(mmr_results, k, use_semantic_similarity)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–Ω–≥–∏
            for rank, result in enumerate(diversified_results):
                result.rank = rank + 1
            
            logger.info(f"‚úÖ [MMR] Diversification completed: {len(diversified_results)} results")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            for i, result in enumerate(diversified_results[:3]):
                logger.info(f"üèÜ [MMR] Top {i+1}: mmr_score={result.mmr_score:.4f}, "
                          f"relevance={result.relevance_score:.4f}, diversity={result.diversity_score:.4f}, "
                          f"title='{result.document_title[:50]}...'")
            
            return diversified_results
            
        except Exception as e:
            logger.error(f"‚ùå [MMR] Error during diversification: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return self._convert_to_mmr_results(results[:k])
    
    def _convert_to_mmr_results(self, results: List[Dict[str, Any]]) -> List[MMRResult]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ MMR —Ñ–æ—Ä–º–∞—Ç"""
        mmr_results = []
        for result in results:
            mmr_result = MMRResult(
                id=result.get('id', ''),
                score=result.get('score', 0.0),
                mmr_score=result.get('score', 0.0),
                relevance_score=result.get('score', 0.0),
                diversity_score=0.0,
                document_id=result.get('document_id', ''),
                chunk_id=result.get('chunk_id', ''),
                code=result.get('code', ''),
                document_title=result.get('document_title', ''),
                section_title=result.get('section_title', ''),
                content=result.get('content', ''),
                chunk_type=result.get('chunk_type', ''),
                page=result.get('page', 0),
                section=result.get('section', ''),
                metadata=result.get('metadata', {}),
                search_type=result.get('search_type', 'hybrid')
            )
            mmr_results.append(mmr_result)
        return mmr_results
    
    def _compute_relevance_scores(self, results: List[MMRResult], query: str):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É"""
        try:
            query_tokens = self._tokenize(query.lower())
            query_tf = self._compute_tf(query_tokens)
            
            for result in results:
                content_tokens = self._tokenize(result.content.lower())
                content_tf = self._compute_tf(content_tokens)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self._cosine_similarity(query_tf, content_tf)
                result.relevance_score = similarity
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [MMR] Error computing relevance scores: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–∫–æ—Ä-—ã –∫–∞–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            for result in results:
                result.relevance_score = result.score
    
    def _apply_mmr_algorithm(self, results: List[MMRResult], k: int, 
                           use_semantic_similarity: bool = True) -> List[MMRResult]:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ MMR –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        
        MMR = Œª * Relevance(d) - (1-Œª) * max(Similarity(d, di)) for di in S
        """
        try:
            if not results:
                return []
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
            results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π)
            selected = [results[0]]
            remaining = results[1:]
            
            # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            while len(selected) < k and remaining:
                best_result = None
                best_mmr_score = float('-inf')
                
                for candidate in remaining:
                    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏
                    max_similarity = 0.0
                    for selected_result in selected:
                        similarity = self._compute_similarity(
                            candidate, selected_result, use_semantic_similarity
                        )
                        max_similarity = max(max_similarity, similarity)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º MMR —Å–∫–æ—Ä
                    mmr_score = (self.lambda_param * candidate.relevance_score - 
                               (1 - self.lambda_param) * max_similarity)
                    
                    candidate.mmr_score = mmr_score
                    candidate.diversity_score = max_similarity
                    
                    if mmr_score > best_mmr_score:
                        best_mmr_score = mmr_score
                        best_result = candidate
                
                if best_result:
                    selected.append(best_result)
                    remaining.remove(best_result)
                else:
                    break
            
            return selected
            
        except Exception as e:
            logger.error(f"‚ùå [MMR] Error in MMR algorithm: {e}")
            return results[:k]
    
    def _compute_similarity(self, result1: MMRResult, result2: MMRResult, 
                          use_semantic_similarity: bool = True) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        
        Args:
            result1: –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result2: –í—Ç–æ—Ä–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            use_semantic_similarity: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        
        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0)
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
            if result1.id == result2.id:
                return 1.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É –∏ —á–∞–Ω–∫—É
            if (result1.document_id == result2.document_id and 
                result1.chunk_id == result2.chunk_id):
                return 0.9
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
            if result1.document_id == result2.document_id:
                return 0.7
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–æ–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if result1.code == result2.code and result1.code:
                return 0.6
            
            if use_semantic_similarity:
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                return self._compute_content_similarity(result1.content, result2.content)
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                return self._compute_text_similarity(result1.content, result2.content)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [MMR] Error computing similarity: {e}")
            return 0.0
    
    def _compute_content_similarity(self, content1: str, content2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            tokens1 = self._tokenize(content1.lower())
            tokens2 = self._tokenize(content2.lower())
            
            if not tokens1 or not tokens2:
                return 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º TF –≤–µ–∫—Ç–æ—Ä—ã
            tf1 = self._compute_tf(tokens1)
            tf2 = self._compute_tf(tokens2)
            
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = self._cosine_similarity(tf1, tf2)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            common_tokens = set(tokens1) & set(tokens2)
            if common_tokens:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                keyword_boost = len(common_tokens) / max(len(tokens1), len(tokens2))
                similarity = min(1.0, similarity + keyword_boost * 0.2)
            
            return similarity
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [MMR] Error computing content similarity: {e}")
            return 0.0
    
    def _compute_text_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            # Jaccard —Å—Ö–æ–¥—Å—Ç–≤–æ
            words1 = set(self._tokenize(text1.lower()))
            words2 = set(self._tokenize(text2.lower()))
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [MMR] Error computing text similarity: {e}")
            return 0.0
    
    def _tokenize(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        tokens = text.split()
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '—É', '–∑–∞', '–ø—Ä–∏', '–±–µ–∑', 
            '—á–µ—Ä–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–≤–æ–∫—Ä—É–≥', '–æ–∫–æ–ª–æ', '–±–ª–∏–∑', '–¥–∞–ª–µ–∫–æ', '–∑–¥–µ—Å—å', 
            '—Ç–∞–º', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '—ç—Ç–æ', '—Ç–æ—Ç', '—ç—Ç–æ—Ç', '—Ç–∞–∫–æ–π', 
            '–∫–∞–∫–æ–π', '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—è', '–≤—Å—ë', '–∫–∞–∂–¥—ã–π', '–ª—é–±–æ–π', '–¥—Ä—É–≥–æ–π', '–∏–Ω–æ–π', '—Å–∞–º', 
            '—Å–∞–º–∞', '—Å–∞–º–æ', '—Å–∞–º–∏', '—Å–µ–±—è', '—Å–µ–±–µ', '—Å–æ–±–æ–π', '–º–æ–π', '–º–æ—è', '–º–æ—ë', '–º–æ–∏', '—Ç–≤–æ–π', 
            '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–≤–æ–∏', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–≤–∞—à', 
            '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏', '–∏–ª–∏', '–Ω–æ', '–∞', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–∂–µ', '–ª–∏', '–±—ã', 
            '–±', '—Ç–æ', '–∂–µ', '–ª–∏', '–±—ã', '–±', '—Ç–æ', '–∂–µ', '–ª–∏', '–±—ã', '–±'
        }
        return [token for token in tokens if len(token) > 2 and token not in stop_words]
    
    def _compute_tf(self, tokens: List[str]) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF (Term Frequency)"""
        if not tokens:
            return {}
        
        token_count = len(tokens)
        tf = defaultdict(float)
        
        for token in tokens:
            tf[token] += 1.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        for token in tf:
            tf[token] /= token_count
        
        return dict(tf)
    
    def _cosine_similarity(self, tf1: Dict[str, float], tf2: Dict[str, float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É TF –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            all_tokens = set(tf1.keys()) | set(tf2.keys())
            
            if not all_tokens:
                return 0.0
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
            vector1 = [tf1.get(token, 0.0) for token in all_tokens]
            vector2 = [tf2.get(token, 0.0) for token in all_tokens]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            dot_product = sum(a * b for a, b in zip(vector1, vector2))
            norm1 = math.sqrt(sum(a * a for a in vector1))
            norm2 = math.sqrt(sum(b * b for b in vector2))
            
            if norm1 == 0.0 or norm2 == 0.0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [MMR] Error computing cosine similarity: {e}")
            return 0.0
    
    def get_diversity_stats(self, results: List[MMRResult]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        try:
            if not results:
                return {"diversity_score": 0.0, "unique_documents": 0, "duplicate_ratio": 0.0}
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            unique_docs = set()
            unique_codes = set()
            total_similarity = 0.0
            similarity_pairs = 0
            
            for i, result1 in enumerate(results):
                unique_docs.add(result1.document_id)
                unique_codes.add(result1.code)
                
                for j, result2 in enumerate(results[i+1:], i+1):
                    similarity = self._compute_similarity(result1, result2, True)
                    total_similarity += similarity
                    similarity_pairs += 1
            
            avg_similarity = total_similarity / similarity_pairs if similarity_pairs > 0 else 0.0
            diversity_score = 1.0 - avg_similarity
            duplicate_ratio = avg_similarity
            
            return {
                "diversity_score": diversity_score,
                "unique_documents": len(unique_docs),
                "unique_codes": len(unique_codes),
                "duplicate_ratio": duplicate_ratio,
                "avg_similarity": avg_similarity,
                "total_results": len(results)
            }
            
        except Exception as e:
            logger.error(f"‚ùå [MMR] Error computing diversity stats: {e}")
            return {"diversity_score": 0.0, "error": str(e)}
    
    def get_mmr_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ MMR —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "lambda_param": self.lambda_param,
            "similarity_threshold": self.similarity_threshold,
            "service_type": "mmr_diversification",
            "timestamp": "2024-01-01T00:00:00Z"  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        }
