import logging
import asyncio
from typing import List, Dict, Any, Optional
import httpx
from datetime import datetime
import numpy as np
import requests
# from sentence_transformers import SentenceTransformer  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

logger = logging.getLogger(__name__)

class NTDConsultationService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    
    def __init__(self, db_connection, qdrant_client, ollama_url: str = "http://vllm:8000", rag_service=None):
        self.db_conn = db_connection
        self.qdrant_client = qdrant_client
        self.ollama_url = ollama_url
        self.collection_name = "normative_documents"
        self.rag_service = rag_service  # –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π RAG —Å–µ—Ä–≤–∏—Å
        self.embedding_model = None
        self._load_embedding_model()
        
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ RAG —Å–µ—Ä–≤–∏—Å—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –º–æ–¥–µ–ª—å
            if self.rag_service and hasattr(self.rag_service, 'embedding_model'):
                self.embedding_model = self.rag_service.embedding_model
                logger.info("‚úÖ [NTD_CONSULTATION] Using embedding model from RAG service")
            else:
                # –ò–Ω–∞—á–µ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                self.embedding_model = None
                logger.info("‚úÖ [NTD_CONSULTATION] Using simple hash embeddings for testing")
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error loading embedding model: {e}")
            self.embedding_model = None
    
    def get_consultation(self, message: str, history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        """
        try:
            logger.info(f"üîç [NTD_CONSULTATION] Processing question: {message[:100]}...")
            
            # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = self._search_relevant_documents(message)
            
            if not relevant_docs:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] No relevant documents found")
                return {
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0
                }
            
            # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = self._build_context(relevant_docs)
            
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
            response = self._generate_simple_response(message, context)
            
            # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = self._prepare_sources(relevant_docs)
            
            logger.info(f"‚úÖ [NTD_CONSULTATION] Response generated successfully")
            
            return {
                "response": response,
                "sources": sources,
                "confidence": self._calculate_confidence(relevant_docs),
                "documents_used": len(relevant_docs)
            }
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error: {e}")
            return {
                "response": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É —Å–∏—Å—Ç–µ–º—ã.",
                "sources": [],
                "confidence": 0.0,
                "error": str(e)
            }
    
    def _search_relevant_documents(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ
        """
        try:
            if not self.embedding_model:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] Embedding model not available, using fallback")
                return self._fallback_search(query, limit)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if self.rag_service and hasattr(self.rag_service, 'create_single_embedding'):
                query_embedding = self.rag_service.create_single_embedding(query)
            else:
                query_embedding = self.embedding_model.encode(query).tolist()
            
            if query_embedding is None:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] Failed to create query embedding, using fallback")
                return self._fallback_search(query, limit)
            
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=limit,
                with_payload=True,
                with_vectors=False,
                score_threshold=0.3
            )
            
            documents = []
            for result in search_results:
                doc_info = self._get_document_info(result.id)
                if doc_info:
                    documents.append({
                        "id": result.id,
                        "score": result.score,
                        "content": result.payload.get("content", ""),
                        "title": doc_info.get("title", ""),
                        "filename": doc_info.get("filename", ""),
                        "category": doc_info.get("category", ""),
                        "page": result.payload.get("page", 1),
                        "chunk_type": result.payload.get("chunk_type", "paragraph")
                    })
            
            logger.info(f"üîç [NTD_CONSULTATION] Found {len(documents)} relevant documents")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Search error: {e}")
            return self._fallback_search(query, limit)
    
    def _fallback_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        try:
            def _search_in_db(conn):
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT 
                            nc.id, nc.document_id, nc.content, nc.chunk_type, nc.page_number,
                            ud.original_filename as title, ud.category
                        FROM normative_chunks nc
                        LEFT JOIN uploaded_documents ud ON nc.document_id = ud.id
                        WHERE nc.content ILIKE %s
                        ORDER BY nc.content ILIKE %s DESC, nc.id
                        LIMIT %s
                    """, (f'%{query}%', f'%{query}%', limit))
                    
                    results = cursor.fetchall()
                    documents = []
                    for row in results:
                        documents.append({
                            "id": row[0], "score": 0.5, "content": row[2], "title": row[5] or f"–î–æ–∫—É–º–µ–Ω—Ç {row[1]}",
                            "filename": row[5] or "", "category": row[6] or "unknown", "page": row[4] or 1,
                            "chunk_type": row[3] or "paragraph"
                        })
                    return documents
            
            return self.db_conn.execute_in_transaction(_search_in_db)
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Fallback search error: {e}")
            return []
    
    def _get_document_info(self, doc_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ –ë–î"""
        try:
            def _get_info(conn):
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT ud.original_filename as title, ud.original_filename as filename,
                               ud.category, ud.upload_date
                        FROM uploaded_documents ud
                        WHERE ud.id = %s
                    """, (doc_id,))
                    result = cursor.fetchone()
                    if result:
                        return {"title": result[0], "filename": result[1], "category": result[2], "upload_date": result[3]}
                    return None
            
            return self.db_conn.execute_in_transaction(_get_info)
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error getting document info: {e}")
            return None
    
    def _build_context(self, documents: List[Dict[str, Any]]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents: return ""
        context_parts = ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:", "=" * 50]
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"\n–î–æ–∫—É–º–µ–Ω—Ç {i}: {doc['title']}")
            context_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc['category']}")
            context_parts.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {doc['page']}")
            context_parts.append(f"–¢–∏–ø: {doc['chunk_type']}")
            context_parts.append(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {doc['score']:.3f}")
            context_parts.append(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{doc['content'][:800]}...")
            context_parts.append("-" * 30)
        return "\n".join(context_parts)
    
    def _generate_simple_response(self, question: str, context: str = "") -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è)
        """
        try:
            if context:
                prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.
–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
"""
            else:
                prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Å–≤–æ–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö –æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö.
"""
            
            response = requests.post(
                f"{self.ollama_url}/v1/chat/completions",
                json={
                    "model": "llama3.1:8b",
                    "messages": [{"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."}, {"role": "user", "content": prompt}],
                    "temperature": 0.7, "max_tokens": 2000, "stream": False
                },
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                logger.error(f"‚ùå [NTD_CONSULTATION] VLLM request failed: {response.status_code}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error generating response: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _prepare_sources(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞"""
        sources = []
        for doc in documents:
            sources.append({
                "title": doc.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"), "filename": doc.get("filename", ""),
                "category": doc.get("category", ""), "page": doc.get("page", 1),
                "chunk_type": doc.get("chunk_type", "paragraph"), "relevance_score": round(doc.get("score", 0), 3),
                "content_preview": doc.get("content", "")[:200] + "..." if len(doc.get("content", "")) > 200 else doc.get("content", "")
            })
        sources.sort(key=lambda x: x["relevance_score"], reverse=True)
        return sources
    
    def _calculate_confidence(self, documents: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents: return 0.0
        avg_score = sum(doc.get("score", 0) for doc in documents) / len(documents)
        confidence = min(avg_score, 1.0)
        return round(confidence, 3)
    
    async def get_consultation_async(self, message: str, history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        """
        try:
            logger.info(f"üîç [NTD_CONSULTATION] Processing question: {message[:100]}...")
            relevant_docs = self._search_relevant_documents(message) # Still synchronous search
            if not relevant_docs:
                logger.warning("‚ö†Ô∏è [NTD_CONSULTATION] No relevant documents found")
                return {"response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...", "sources": [], "confidence": 0.0, "documents_used": 0}
            context = self._build_context(relevant_docs)
            response = await self._generate_async_response(message, context)
            sources = self._prepare_sources(relevant_docs)
            logger.info(f"‚úÖ [NTD_CONSULTATION] Response generated successfully")
            return {"response": response, "sources": sources, "confidence": self._calculate_confidence(relevant_docs), "documents_used": len(relevant_docs)}
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error: {e}")
            return {"response": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...", "sources": [], "confidence": 0.0, "error": str(e)}
    
    async def _generate_async_response(self, question: str, context: str = "") -> str:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        try:
            if context:
                prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.
–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
"""
            else:
                prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Å–≤–æ–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö –æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö.
"""
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/v1/chat/completions",
                    json={"model": "llama3.1:8b", "messages": [{"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."}, {"role": "user", "content": prompt}], "temperature": 0.7, "max_tokens": 2000, "stream": False}
                )
                if response.status_code == 200:
                    data = response.json()
                    return data["choices"][0]["message"]["content"]
                else:
                    logger.error(f"‚ùå [NTD_CONSULTATION] VLLM request failed: {response.status_code}")
                    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error generating response: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def get_ntd_consultations_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ù–¢–î"""
        try:
            return {
                "total_consultations": 0, "successful_consultations": 0, "average_response_time": 0.0,
                "popular_questions": [], "last_consultation": None,
                "embedding_model_available": self.embedding_model is not None
            }
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION_STATS] Error: {e}")
            return {"error": str(e)}
