"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API (GPT-4o-mini)
"""

import logging
import os
import requests
from typing import List, Dict, Any, Optional
import time

logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á OpenAI –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")


class OpenAIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""
    
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or OPENAI_API_KEY
        self.base_url = base_url or OPENAI_BASE_URL
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è [OPENAI] API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
        
        logger.info(f"ü§ñ [OPENAI] Initialized with base URL: {self.base_url}")
    
    def generate_response(self, message: str, history: Optional[List[Dict[str, str]]] = None,
                         model: str = "gpt-4o-mini", max_tokens: Optional[int] = None,
                         temperature: float = 0.3, turbo_mode: bool = True) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            turbo_mode: –†–µ–∂–∏–º —Ç—É—Ä–±–æ (–≤–ª–∏—è–µ—Ç –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            if not self.api_key:
                raise Exception("OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            
            start_time = time.time()
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞
            if turbo_mode:
                temperature = 0.3
                max_tokens = max_tokens or 1024
                system_prompt = self._get_turbo_system_prompt()
            else:
                temperature = 0.6
                max_tokens = max_tokens or 2048
                system_prompt = self._get_standard_system_prompt()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è API
            messages = [{"role": "system", "content": system_prompt}]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            if history:
                for msg in history:
                    if 'user' in msg and 'assistant' in msg:
                        messages.append({"role": "user", "content": msg['user']})
                        messages.append({"role": "assistant", "content": msg['assistant']})
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            messages.append({"role": "user", "content": message})
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            logger.info(f"üöÄ [OPENAI] Generating response with model: {model}, turbo_mode: {turbo_mode}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=headers,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result["choices"][0]["message"]["content"]
                
                generation_time = (time.time() - start_time) * 1000
                usage = result.get("usage", {})
                tokens_used = usage.get("total_tokens", 0)
                
                logger.info(f"‚úÖ [OPENAI] Generated response in {generation_time:.1f}ms, tokens: {tokens_used}")
                
                return {
                    "response": response_text,
                    "tokens_used": tokens_used,
                    "generation_time_ms": generation_time,
                    "turbo_mode": turbo_mode,
                    "model": model,
                    "usage": usage
                }
            else:
                error_detail = response.text
                logger.error(f"‚ùå [OPENAI] API error: {response.status_code} - {error_detail}")
                raise Exception(f"OpenAI API error: {response.status_code} - {error_detail}")
                
        except Exception as e:
            logger.error(f"‚ùå [OPENAI] Error generating response: {e}")
            raise e
    
    def _get_turbo_system_prompt(self) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç—É—Ä–±–æ —Ä–µ–∂–∏–º–∞"""
        return """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Ä–µ–∂–∏–º–µ "–¢—É—Ä–±–æ". 
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ –¥–∞–≤–∞–π –ø—Ä—è–º—ã–µ –æ—Ç–≤–µ—Ç—ã.
–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –∏ –∏–∑–±–µ–≥–∞–π –∏–∑–ª–∏—à–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π."""
    
    def _get_standard_system_prompt(self) -> str:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
        return """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. 
–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.
–†–∞–∑–±–∏–≤–∞–π —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —á–∞—Å—Ç–∏ –∏ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–∏ –≤—ã–≤–æ–¥—ã."""
    
    def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            if not self.api_key:
                return False
            
            # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=headers,
                timeout=10
            )
            
            return response.status_code == 200
            
        except Exception as e:
            logger.error(f"‚ùå [OPENAI] Health check failed: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return [
            "gpt-4o-mini",
            "gpt-4o",
            "gpt-4-turbo",
            "gpt-3.5-turbo"
        ]







