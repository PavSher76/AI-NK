import logging
import asyncio
import time
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import threading
from queue import Queue, Empty

from .database_manager import DatabaseManager
from .qdrant_service import QdrantService
from .ollama_rag_service import OllamaEmbeddingService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class IndexingStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    PENDING = "pending"
    INDEXING = "indexing"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"

@dataclass
class IndexingTask:
    """–ó–∞–¥–∞—á–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    document_id: int
    filename: str
    content: bytes
    category: str
    retry_count: int = 0
    max_retries: int = 3
    priority: int = 0  # 0 = normal, 1 = high, -1 = low
    created_at: datetime = None
    last_attempt: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.last_attempt is None:
            self.last_attempt = datetime.now()

class ResilientIndexingService:
    """–°–µ—Ä–≤–∏—Å —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
    
    def __init__(self, db_manager: DatabaseManager, qdrant_service: QdrantService, 
                 embedding_service: OllamaEmbeddingService, max_concurrent_tasks: int = 3):
        self.db_manager = db_manager
        self.qdrant_service = qdrant_service
        self.embedding_service = embedding_service
        self.max_concurrent_tasks = max_concurrent_tasks
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á
        self.task_queue = Queue()
        self.active_tasks: Dict[int, IndexingTask] = {}
        self.failed_tasks: List[IndexingTask] = []
        
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞–º–∏
        self.worker_threads: List[threading.Thread] = []
        self.shutdown_event = threading.Event()
        self.is_running = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'retries': 0,
            'start_time': None
        }
        
        logger.info(f"üöÄ [INDEXING_SERVICE] ResilientIndexingService initialized with {max_concurrent_tasks} workers")
    
    def start(self):
        """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è [INDEXING_SERVICE] Service is already running")
            return
        
        self.is_running = True
        self.shutdown_event.clear()
        self.stats['start_time'] = datetime.now()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º worker –ø–æ—Ç–æ–∫–∏
        for i in range(self.max_concurrent_tasks):
            worker = threading.Thread(
                target=self._worker_loop,
                name=f"IndexingWorker-{i+1}",
                daemon=True
            )
            worker.start()
            self.worker_threads.append(worker)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        monitor_thread = threading.Thread(
            target=self._monitor_loop,
            name="IndexingMonitor",
            daemon=True
        )
        monitor_thread.start()
        
        logger.info(f"‚úÖ [INDEXING_SERVICE] Service started with {len(self.worker_threads)} workers")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        if not self.is_running:
            logger.warning("‚ö†Ô∏è [INDEXING_SERVICE] Service is not running")
            return
        
        logger.info("üõë [INDEXING_SERVICE] Stopping service...")
        self.is_running = False
        self.shutdown_event.set()
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
        timeout = 30  # —Å–µ–∫—É–Ω–¥
        start_time = time.time()
        
        while self.active_tasks and (time.time() - start_time) < timeout:
            logger.info(f"‚è≥ [INDEXING_SERVICE] Waiting for {len(self.active_tasks)} active tasks to complete...")
            time.sleep(1)
        
        if self.active_tasks:
            logger.warning(f"‚ö†Ô∏è [INDEXING_SERVICE] {len(self.active_tasks)} tasks still active after timeout")
        
        logger.info("‚úÖ [INDEXING_SERVICE] Service stopped")
    
    def add_indexing_task(self, document_id: int, filename: str, content: bytes, 
                         category: str, priority: int = 0, max_retries: int = 3):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –æ—á–µ—Ä–µ–¥—å"""
        task = IndexingTask(
            document_id=document_id,
            filename=filename,
            content=content,
            category=category,
            priority=priority,
            max_retries=max_retries
        )
        
        self.task_queue.put(task)
        logger.info(f"üìù [INDEXING_SERVICE] Added indexing task for document {document_id} (priority: {priority})")
    
    def _worker_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª worker –ø–æ—Ç–æ–∫–∞"""
        worker_name = threading.current_thread().name
        logger.info(f"üîÑ [INDEXING_SERVICE] {worker_name} started")
        
        while not self.shutdown_event.is_set():
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                try:
                    task = self.task_queue.get(timeout=1.0)
                except Empty:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É
                self._process_indexing_task(task)
                self.task_queue.task_done()
                
            except Exception as e:
                logger.error(f"‚ùå [INDEXING_SERVICE] {worker_name} error: {e}")
                time.sleep(1)
        
        logger.info(f"‚úÖ [INDEXING_SERVICE] {worker_name} stopped")
    
    def _process_indexing_task(self, task: IndexingTask):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        document_id = task.document_id
        worker_name = threading.current_thread().name
        
        try:
            logger.info(f"üîÑ [INDEXING_SERVICE] {worker_name} processing document {document_id} (attempt {task.retry_count + 1})")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
            self.active_tasks[document_id] = task
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
            self.db_manager.update_document_status(document_id, IndexingStatus.INDEXING.value)
            self.db_manager.update_indexing_progress(document_id, 10, "Starting indexing...")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            self.db_manager.update_indexing_progress(document_id, 20, "Extracting text...")
            text_content = self._extract_text_from_document(task.content, task.filename)
            
            if not text_content:
                raise Exception("Failed to extract text from document")
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            self.db_manager.update_indexing_progress(document_id, 40, "Creating chunks...")
            chunks = self._create_chunks(text_content, document_id, task.filename)
            
            if not chunks:
                raise Exception("Failed to create chunks")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
            self.db_manager.update_indexing_progress(document_id, 60, "Creating embeddings...")
            success = self._index_chunks(chunks, document_id)
            
            if not success:
                raise Exception("Failed to index chunks")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            self.db_manager.update_indexing_progress(document_id, 90, "Updating token count...")
            token_count = len(text_content.split())
            self.db_manager.execute_write_query("""
                UPDATE uploaded_documents 
                SET token_count = %s
                WHERE id = %s
            """, (token_count, document_id))
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º —É—Å–ø–µ—à–Ω–æ
            self.db_manager.update_document_status(document_id, IndexingStatus.COMPLETED.value)
            self.db_manager.update_indexing_progress(document_id, 100, "Indexing completed successfully")
            
            self.stats['successful'] += 1
            logger.info(f"‚úÖ [INDEXING_SERVICE] {worker_name} completed document {document_id}")
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå [INDEXING_SERVICE] {worker_name} failed document {document_id}: {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É
            if task.retry_count < task.max_retries:
                task.retry_count += 1
                task.last_attempt = datetime.now()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
                retry_delay = min(2 ** task.retry_count, 60)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                logger.info(f"üîÑ [INDEXING_SERVICE] Retrying document {document_id} in {retry_delay} seconds (attempt {task.retry_count + 1})")
                
                # –ü–æ–º–µ—á–∞–µ–º –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ –≤ –ë–î
                self.db_manager.mark_document_for_retry(document_id, error_msg)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
                threading.Timer(retry_delay, lambda: self.task_queue.put(task)).start()
                
                self.stats['retries'] += 1
            else:
                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏—Å—á–µ—Ä–ø–∞–Ω–æ
                self.db_manager.update_document_status(document_id, IndexingStatus.FAILED.value, error_msg)
                self.failed_tasks.append(task)
                self.stats['failed'] += 1
                
                logger.error(f"‚ùå [INDEXING_SERVICE] Document {document_id} failed after {task.max_retries} retries")
        
        finally:
            # –£–±–∏—Ä–∞–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
            self.active_tasks.pop(document_id, None)
            self.stats['total_processed'] += 1
    
    def _extract_text_from_document(self, content: bytes, filename: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            import sys
            import os
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
            from utils import parse_document_from_bytes
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text_content = parse_document_from_bytes(content, filename)
            return text_content
            
        except Exception as e:
            logger.error(f"‚ùå [INDEXING_SERVICE] Error extracting text from {filename}: {e}")
            raise
    
    def _create_chunks(self, text_content: str, document_id: int, filename: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–µ—Ä
            import sys
            import os
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
            from utils import create_chunks
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            chunks = create_chunks(text_content, document_id, filename)
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [INDEXING_SERVICE] Error creating chunks for document {document_id}: {e}")
            raise
    
    def _index_chunks(self, chunks: List[Dict[str, Any]], document_id: int) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –≤ Qdrant"""
        try:
            points = []
            
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                content = chunk.get('content', '')
                if not content.strip():
                    continue
                
                embedding = self.embedding_service.create_embedding(content)
                
                # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
                point = {
                    'id': chunk.get('id'),
                    'vector': embedding,
                    'payload': {
                        'document_id': document_id,
                        'title': chunk.get('document_title', ''),
                        'section_title': chunk.get('section_title', ''),
                        'content': content,
                        'chunk_type': chunk.get('chunk_type', 'paragraph'),
                        'page': chunk.get('page', 1),
                        'section': chunk.get('section', ''),
                        'metadata': chunk.get('metadata', {})
                    }
                }
                points.append(point)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
            if points:
                self.qdrant_service.upsert_points(points)
                logger.info(f"‚úÖ [INDEXING_SERVICE] Indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING_SERVICE] No points to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING_SERVICE] Error indexing chunks for document {document_id}: {e}")
            raise
    
    def _monitor_loop(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        logger.info("üìä [INDEXING_SERVICE] Monitor started")
        
        while not self.shutdown_event.is_set():
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                time.sleep(30)
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self._log_statistics()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å—à–∏–µ –∑–∞–¥–∞—á–∏
                self._check_stuck_tasks()
                
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –∏–∑ –ë–î –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                self._recover_pending_tasks()
                
            except Exception as e:
                logger.error(f"‚ùå [INDEXING_SERVICE] Monitor error: {e}")
        
        logger.info("‚úÖ [INDEXING_SERVICE] Monitor stopped")
    
    def _log_statistics(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        uptime = datetime.now() - self.stats['start_time'] if self.stats['start_time'] else timedelta(0)
        
        logger.info(f"üìä [INDEXING_SERVICE] Stats - "
                   f"Uptime: {uptime}, "
                   f"Total: {self.stats['total_processed']}, "
                   f"Success: {self.stats['successful']}, "
                   f"Failed: {self.stats['failed']}, "
                   f"Retries: {self.stats['retries']}, "
                   f"Queue: {self.task_queue.qsize()}, "
                   f"Active: {len(self.active_tasks)}")
    
    def _check_stuck_tasks(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á"""
        stuck_threshold = timedelta(minutes=10)  # 10 –º–∏–Ω—É—Ç
        current_time = datetime.now()
        
        stuck_tasks = []
        for doc_id, task in self.active_tasks.items():
            if current_time - task.last_attempt > stuck_threshold:
                stuck_tasks.append(doc_id)
        
        if stuck_tasks:
            logger.warning(f"‚ö†Ô∏è [INDEXING_SERVICE] Found {len(stuck_tasks)} stuck tasks: {stuck_tasks}")
            
            # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ failed –∏ —É–±–∏—Ä–∞–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            for doc_id in stuck_tasks:
                task = self.active_tasks.pop(doc_id, None)
                if task:
                    self.db_manager.update_document_status(doc_id, IndexingStatus.FAILED.value, "Task stuck")
                    logger.warning(f"‚ö†Ô∏è [INDEXING_SERVICE] Marked stuck task {doc_id} as failed")
    
    def _recover_pending_tasks(self):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á –∏–∑ –ë–î"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –æ–∂–∏–¥–∞—é—â–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            pending_docs = self.db_manager.get_pending_documents_for_indexing()
            
            for doc in pending_docs:
                doc_id = doc['id']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ª–∏ —É–∂–µ
                if doc_id in self.active_tasks:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –≤ –æ—á–µ—Ä–µ–¥–∏
                if any(task.document_id == doc_id for task in list(self.task_queue.queue)):
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å (–±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å)
                logger.info(f"üîÑ [INDEXING_SERVICE] Recovering pending document {doc_id}")
                # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
                # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ pending
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING_SERVICE] Error recovering pending tasks: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–∞"""
        uptime = datetime.now() - self.stats['start_time'] if self.stats['start_time'] else timedelta(0)
        
        return {
            'is_running': self.is_running,
            'uptime_seconds': uptime.total_seconds(),
            'queue_size': self.task_queue.qsize(),
            'active_tasks': len(self.active_tasks),
            'failed_tasks': len(self.failed_tasks),
            'stats': self.stats.copy(),
            'worker_threads': len(self.worker_threads)
        }
