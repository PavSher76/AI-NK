import logging
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π
from .embedding_service import OllamaEmbeddingService
from .database_manager import DatabaseManager
from .document_parser import DocumentParser
from .metadata_extractor import MetadataExtractor
from .document_chunker import DocumentChunker
from .qdrant_service import QdrantService
from .reranker_service import BGERerankerService
from .bge_reranker_service import BGERankingService
from .hybrid_search_service import HybridSearchService
from .mmr_service import MMRService
from .intent_classifier_service import IntentClassifierService
from .context_builder_service import ContextBuilderService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OllamaRAGService:
    """RAG —Å–µ—Ä–≤–∏—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"  # Qdrant –≤ Docker
        self.POSTGRES_URL = "postgresql://norms_user:norms_password@norms-db:5432/norms_db"  # –ë–î –≤ Docker
        self.VECTOR_COLLECTION = "normative_documents"
        self.VECTOR_SIZE = 1024  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ BGE-M3
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π
        self.qdrant_service = QdrantService(self.QDRANT_URL, self.VECTOR_COLLECTION, self.VECTOR_SIZE)
        self.db_manager = DatabaseManager(self.POSTGRES_URL)
        self.embedding_service = OllamaEmbeddingService()
        self.document_parser = DocumentParser()
        self.metadata_extractor = MetadataExtractor()
        self.document_chunker = DocumentChunker()
        self.reranker_service = BGERerankerService()  # –°—Ç–∞—Ä—ã–π —Ä–µ—Ä–∞–Ω–∫–µ—Ä –¥–ª—è fallback
        self.bge_reranker_service = BGERankingService()  # –ù–æ–≤—ã–π BGE —Ä–µ—Ä–∞–Ω–∫–µ—Ä
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.hybrid_search_service = HybridSearchService(
            db_connection=self.db_manager.get_connection(),
            embedding_service=self.embedding_service,
            qdrant_service=self.qdrant_service,
            alpha=0.6,  # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –¥–ª—è dense –ø–æ–∏—Å–∫–∞
            use_rrf=True,
            rrf_k=60
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MMR —Å–µ—Ä–≤–∏—Å–∞
        self.mmr_service = MMRService(
            lambda_param=0.7,  # –ë–∞–ª–∞–Ω—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            similarity_threshold=0.8
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        self.intent_classifier = IntentClassifierService()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.context_builder = ContextBuilderService()
        
        logger.info("üöÄ [OLLAMA_RAG_SERVICE] Ollama RAG Service initialized with modular architecture")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã
        from .ollama_rag_service_methods import OllamaRAGServiceMethods
        self.methods = OllamaRAGServiceMethods(self)
    
    def get_structured_context(self, query: str, k: int = 8, document_filter: Optional[str] = None, 
                              chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None,
                              use_reranker: bool = True, fast_mode: bool = False, use_mmr: bool = True,
                              use_intent_classification: bool = True) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        try:
            logger.info(f"üèóÔ∏è [STRUCTURED_CONTEXT] Building structured context for query: '{query[:50]}...'")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            search_results = self.methods.hybrid_search(
                query=query,
                k=k,
                document_filter=document_filter,
                chapter_filter=chapter_filter,
                chunk_type_filter=chunk_type_filter,
                use_reranker=use_reranker,
                fast_mode=fast_mode,
                use_mmr=use_mmr,
                use_intent_classification=use_intent_classification
            )
            
            if not search_results:
                logger.warning(f"‚ö†Ô∏è [STRUCTURED_CONTEXT] No search results found for query: '{query}'")
                return {
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "context": [],
                    "meta_summary": {
                        "query_type": "no_results",
                        "documents_found": 0,
                        "sections_covered": 0,
                        "avg_relevance": 0.0,
                        "coverage_quality": "–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                        "key_documents": [],
                        "key_sections": []
                    },
                    "total_candidates": 0,
                    "avg_score": 0.0
                }
            
            # –°—Ç—Ä–æ–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            structured_context = self.context_builder.build_structured_context(search_results, query)
            
            logger.info(f"‚úÖ [STRUCTURED_CONTEXT] Structured context built with {structured_context['total_candidates']} candidates")
            return structured_context
            
        except Exception as e:
            logger.error(f"‚ùå [STRUCTURED_CONTEXT] Error building structured context: {e}")
            return {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "context": [],
                "meta_summary": {
                    "query_type": "error",
                    "documents_found": 0,
                    "sections_covered": 0,
                    "avg_relevance": 0.0,
                    "coverage_quality": "–æ—à–∏–±–∫–∞",
                    "key_documents": [],
                    "key_sections": []
                },
                "total_candidates": 0,
                "avg_score": 0.0,
                "error": str(e)
            }
    
    def extract_document_code(self, document_title: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (–ì–û–°–¢, –°–ü, –°–ù–∏–ü –∏ —Ç.–¥.)"""
        return self.document_parser.extract_document_code(document_title)
    
    def index_document_chunks(self, document_id: int, chunks: List[Dict[str, Any]]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        try:
            logger.info(f"üìù [INDEXING] Starting indexing for document {document_id} with {len(chunks)} chunks")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîç [INDEXING] Getting metadata for document_id: {document_id}")
            document_metadata = self.methods._get_document_metadata(document_id)
            logger.info(f"üîç [INDEXING] Retrieved metadata: {document_metadata}")
            
            points = []
            
            for chunk in chunks:
                try:
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                    content = chunk.get('content', '')
                    if not content.strip():
                        continue
                    
                    embedding = self.embedding_service.create_embedding(content)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤–æ–π ID –¥–ª—è Qdrant
                    qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                    if qdrant_id < 0:
                        qdrant_id = abs(qdrant_id)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    document_title = chunk.get('document_title', '')
                    code = self.extract_document_code(document_title)
                    
                    logger.info(f"üîç [INDEXING] Document title: '{document_title}', extracted code: '{code}'")
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                    chunk_metadata = self.metadata_extractor.create_chunk_metadata(chunk, document_metadata)
                    
                    # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
                    point = self.qdrant_service.create_point(
                        point_id=qdrant_id,
                        vector=embedding,
                        payload={
                            'document_id': document_id,
                            'chunk_id': chunk['chunk_id'],
                            'code': code,
                            'title': document_title,
                            'section_title': chunk.get('section_title', ''),
                            'content': content,
                            'chunk_type': chunk.get('chunk_type', ''),
                            'page': chunk.get('page', 1),
                            'section': chunk.get('section', ''),
                            'metadata': chunk_metadata
                        }
                    )
                    points.append(point)
                    
                except Exception as e:
                    logger.error(f"‚ùå [INDEXING] Error processing chunk {chunk.get('chunk_id', 'unknown')}: {e}")
                    continue
            
            if points:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
                self.qdrant_service.upsert_points_batch(points)
                logger.info(f"‚úÖ [INDEXING] Successfully indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING] No valid chunks to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING] Error indexing document {document_id}: {e}")
            return False
    
    # –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    def hybrid_search(self, *args, **kwargs):
        return self.methods.hybrid_search(*args, **kwargs)
    
    def get_ntd_consultation(self, *args, **kwargs):
        return self.methods.get_ntd_consultation(*args, **kwargs)
    
    def get_documents(self):
        return self.db_manager.get_documents()
    
    def get_documents_from_uploaded(self, document_type: str = 'all'):
        return self.db_manager.get_documents_from_uploaded(document_type)
    
    def get_document_chunks(self, document_id: int):
        return self.db_manager.get_document_chunks(document_id)
    
    def delete_document_indexes(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Qdrant"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_INDEXES] Deleting indexes for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.get_document_chunks(document_id)
            if not chunks:
                logger.warning(f"‚ö†Ô∏è [DELETE_INDEXES] No chunks found for document {document_id}")
                return True
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ Qdrant
            point_ids = []
            for chunk in chunks:
                qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                if qdrant_id < 0:
                    qdrant_id = abs(qdrant_id)
                point_ids.append(qdrant_id)
            
            # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
            if point_ids:
                # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
                self.qdrant_service.delete_points_by_document(document_id)
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted points from Qdrant for document {document_id}")
            
            # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ –∏–∑ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks WHERE document_id = %s", (document_id,))
                deleted_chunks = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted {deleted_chunks} chunks from PostgreSQL for document {document_id}")
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_INDEXES] Error deleting indexes for document {document_id}: {e}")
            return False
    
    def delete_document(self, document_id: int) -> bool:
        """–ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_DOCUMENT] Deleting document {document_id}")
            
            # 1. –£–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ Qdrant
            indexes_deleted = self.delete_document_indexes(document_id)
            
            # 2. –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            with self.db_manager.get_cursor() as cursor:
                # –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                deleted_elements = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_elements} extracted elements for document {document_id}")
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM uploaded_documents WHERE id = %s", (document_id,))
                deleted_documents = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_documents} documents for document {document_id}")
                
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            if deleted_documents == 0:
                logger.warning(f"‚ö†Ô∏è [DELETE_DOCUMENT] Document {document_id} not found")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_DOCUMENT] Error deleting document {document_id}: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–∏—Å
            qdrant_info = self.qdrant_service.get_collection_info()
            qdrant_stats = {
                'collection_name': self.VECTOR_COLLECTION,
                'vectors_count': qdrant_info.get('points_count', 0),
                'indexed_vectors': qdrant_info.get('points_count', 0),
                'status': 'ok' if qdrant_info else 'unknown'
            }
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL
            db_stats = self.db_manager.get_stats()
            
            return {
                'qdrant': qdrant_stats,
                'postgresql': db_stats,
                'embedding_model': 'bge-m3 (Ollama)',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            try:
                db_stats = self.db_manager.get_stats()
                
                return {
                    'qdrant': {
                        'collection_name': self.VECTOR_COLLECTION,
                        'vectors_count': 0,  # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Qdrant
                        'indexed_vectors': 0,
                        'status': 'error'
                    },
                    'postgresql': db_stats,
                    'embedding_model': 'bge-m3 (Ollama)',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
            except Exception as fallback_error:
                logger.error(f"‚ùå [GET_STATS] Fallback error: {fallback_error}")
                return {
                    'error': f"Primary error: {str(e)}, Fallback error: {str(fallback_error)}",
                    'timestamp': datetime.now().isoformat()
                }
    
    def save_document_to_db(self, document_id: int, filename: str, original_filename: str, 
                           file_type: str, file_size: int, document_hash: str, 
                           category: str, document_type: str) -> int:
        return self.db_manager.save_document_to_db(
            document_id, filename, original_filename, file_type, 
            file_size, document_hash, category, document_type
        )

    def update_document_status(self, document_id: int, status: str, error_message: str = None):
        return self.db_manager.update_document_status(document_id, status, error_message)

    async def process_document_async(self, document_id: int, content: bytes, filename: str) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üîÑ [PROCESS_ASYNC] Starting processing for document {document_id}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            text_content = await self.document_parser.extract_text_from_document(content, filename)
            if not text_content:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to extract text from document {document_id}")
                return False
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.document_chunker.create_chunks(text_content, document_id, filename)
            if not chunks:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to create chunks for document {document_id}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            success = await self.index_chunks_async(chunks, document_id)
            if not success:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to index chunks for document {document_id}")
                return False
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            token_count = len(text_content.split())
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET token_count = %s
                    WHERE id = %s
                """, (token_count, document_id))
                cursor.connection.commit()
            
            logger.info(f"‚úÖ [PROCESS_ASYNC] Document {document_id} processed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [PROCESS_ASYNC] Error processing document {document_id}: {e}")
            return False

    async def index_chunks_async(self, chunks: List[Dict[str, Any]], document_id: int) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîç [INDEX_CHUNKS_ASYNC] Getting metadata for document_id: {document_id}")
            document_metadata = self.methods._get_document_metadata(document_id)
            logger.info(f"üîç [INDEX_CHUNKS_ASYNC] Retrieved metadata: {document_metadata}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                for chunk in chunks:
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, document_id, document_title, chunk_type, content, page_number, chapter, section)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        chunk['chunk_id'],
                        chunk['chunk_id'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk_id –∫–∞–∫ clause_id
                        chunk['document_id'],
                        chunk['document_title'],
                        chunk['chunk_type'],
                        chunk['content'],
                        chunk.get('page', 1),  # page_number
                        chunk.get('chapter', ''),  # chapter
                        chunk.get('section', '')   # section
                    ))
                cursor.connection.commit()
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = self.embedding_service.create_embedding(chunk['content'])
                if embedding is None:
                    logger.warning(f"‚ö†Ô∏è [INDEX_CHUNKS] Failed to create embedding for chunk {chunk['chunk_id']}")
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
                point_id = hash(chunk['chunk_id']) % (2**63 - 1)
                if point_id < 0:
                    point_id = abs(point_id)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ —Å–ø–∏—Å–æ–∫
                if hasattr(embedding, 'tolist'):
                    vector = embedding.tolist()
                else:
                    vector = list(embedding)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                chunk_metadata = self.metadata_extractor.create_chunk_metadata(chunk, document_metadata)
                
                payload = {
                    'chunk_id': chunk['chunk_id'],
                    'document_id': chunk['document_id'],
                    'document_title': chunk['document_title'],
                    'content': chunk['content'],
                    'chunk_type': chunk['chunk_type'],
                    'page': chunk.get('page', 1),
                    'chapter': chunk.get('chapter', ''),
                    'section': chunk.get('section', ''),
                    'metadata': chunk_metadata
                }
                
                self.qdrant_service.upsert_point(point_id, vector, payload)
            
            logger.info(f"‚úÖ [INDEX_CHUNKS] Indexed {len(chunks)} chunks for document {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX_CHUNKS] Error indexing chunks: {e}")
            return False

    def clear_collection(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant"""
        try:
            logger.info("üßπ [CLEAR_COLLECTION] Clearing entire collection...")
            
            # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            success = self.qdrant_service.clear_collection()
            
            if success:
                logger.info("‚úÖ [CLEAR_COLLECTION] Collection cleared successfully")
                return True
            else:
                logger.error("‚ùå [CLEAR_COLLECTION] Failed to clear collection")
                return False
            
        except Exception as e:
            logger.error(f"‚ùå [CLEAR_COLLECTION] Error clearing collection: {e}")
            return False
    
    def get_hybrid_search_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            stats = self.hybrid_search_service.get_search_stats()
            bge_reranker_stats = self.bge_reranker_service.get_reranking_stats()
            mmr_stats = self.mmr_service.get_mmr_stats()
            intent_classifier_stats = self.intent_classifier.get_intent_stats()
            
            return {
                "status": "success",
                "stats": stats,
                "bge_reranker": bge_reranker_stats,
                "mmr": mmr_stats,
                "intent_classifier": intent_classifier_stats,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_STATS] Error getting hybrid search stats: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
