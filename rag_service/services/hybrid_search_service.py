"""
–°–µ—Ä–≤–∏—Å –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å BM25 + Dense –ø–æ–∏—Å–∫–æ–º, Alpha —Å–º–µ—à–∏–≤–∞–Ω–∏–µ–º –∏ Reciprocal Rank Fusion
"""

import logging
import math
import re
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
import numpy as np
from dataclasses import dataclass
import psycopg2
from psycopg2.extras import RealDictCursor

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: str
    score: float
    document_id: str
    chunk_id: str
    code: str
    document_title: str
    section_title: str
    content: str
    chunk_type: str
    page: int
    section: str
    metadata: Dict[str, Any]
    search_type: str  # 'bm25', 'dense', 'hybrid'
    rank: int = 0

class BM25Search:
    """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è BM25 –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self, k1: float = 1.2, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = {}
        self.avgdl = 0
        self.corpus_size = 0
        self.doc_freqs = defaultdict(int)
        self.freqs = defaultdict(dict)
        
    def _tokenize(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        tokens = text.split()
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '—É', '–∑–∞', '–ø—Ä–∏', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–≤–æ–∫—Ä—É–≥', '–æ–∫–æ–ª–æ', '–±–ª–∏–∑', '–¥–∞–ª–µ–∫–æ', '–∑–¥–µ—Å—å', '—Ç–∞–º', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '—ç—Ç–æ', '—Ç–æ—Ç', '—ç—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–∫–∞–∫–æ–π', '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—è', '–≤—Å—ë', '–∫–∞–∂–¥—ã–π', '–ª—é–±–æ–π', '–¥—Ä—É–≥–æ–π', '–∏–Ω–æ–π', '—Å–∞–º', '—Å–∞–º–∞', '—Å–∞–º–æ', '—Å–∞–º–∏', '—Å–µ–±—è', '—Å–µ–±–µ', '—Å–æ–±–æ–π', '–º–æ–π', '–º–æ—è', '–º–æ—ë', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–≤–æ–∏', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏'}
        return [token for token in tokens if len(token) > 2 and token not in stop_words]
    
    def fit(self, documents: List[Dict[str, Any]]):
        """–û–±—É—á–µ–Ω–∏–µ BM25 –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.info(f"üîç [BM25] Training BM25 on {len(documents)} documents")
        
        self.corpus_size = len(documents)
        self.doc_freqs = defaultdict(int)
        self.freqs = defaultdict(dict)
        self.doc_len = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        for doc in documents:
            doc_id = doc['chunk_id']
            content = doc.get('content', '')
            tokens = self._tokenize(content)
            
            self.doc_len[doc_id] = len(tokens)
            self.freqs[doc_id] = Counter(tokens)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            for token in set(tokens):
                self.doc_freqs[token] += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.avgdl = sum(self.doc_len.values()) / len(self.doc_len) if self.doc_len else 0
        
        # –í—ã—á–∏—Å–ª—è–µ–º IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
        for term, freq in self.doc_freqs.items():
            self.idf[term] = math.log((self.corpus_size - freq + 0.5) / (freq + 0.5))
        
        logger.info(f"‚úÖ [BM25] Training completed. Corpus size: {self.corpus_size}, avg doc length: {self.avgdl:.2f}")
    
    def search(self, query: str, documents: List[Dict[str, Any]], k: int = 10) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        query_tokens = self._tokenize(query)
        scores = {}
        
        for doc in documents:
            doc_id = doc['chunk_id']
            score = 0
            
            for token in query_tokens:
                if token in self.freqs[doc_id]:
                    tf = self.freqs[doc_id][token]
                    idf = self.idf.get(token, 0)
                    
                    # –§–æ—Ä–º—É–ª–∞ BM25
                    score += idf * (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * (self.doc_len[doc_id] / self.avgdl)))
            
            if score > 0:
                scores[doc_id] = score
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for rank, (doc_id, score) in enumerate(sorted_scores[:k]):
            doc = next((d for d in documents if d['chunk_id'] == doc_id), None)
            if doc:
                result = SearchResult(
                    id=doc_id,
                    score=score,
                    document_id=doc.get('document_id', ''),
                    chunk_id=doc.get('chunk_id', ''),
                    code=doc.get('code', ''),
                    document_title=doc.get('document_title', ''),
                    section_title=doc.get('section_title', ''),
                    content=doc.get('content', ''),
                    chunk_type=doc.get('chunk_type', ''),
                    page=doc.get('page', 0),
                    section=doc.get('section', ''),
                    metadata=doc.get('metadata', {}),
                    search_type='bm25',
                    rank=rank + 1
                )
                results.append(result)
        
        logger.info(f"‚úÖ [BM25] Found {len(results)} results for query: '{query}'")
        return results

class DenseSearch:
    """Dense –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self, embedding_service, qdrant_service):
        self.embedding_service = embedding_service
        self.qdrant_service = qdrant_service
    
    def search(self, query: str, k: int = 10, filters: Optional[Dict] = None) -> List[SearchResult]:
        """Dense –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º"""
        try:
            logger.info(f"üîç [DENSE] Performing dense search for query: '{query}' with k={k}")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_service.create_embedding(query)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_result = self.qdrant_service.search_similar(
                query_vector=query_embedding,
                limit=k,
                filters=filters
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for rank, point in enumerate(search_result):
                result = SearchResult(
                    id=str(point['id']),
                    score=point['score'],
                    document_id=point['payload'].get('document_id', ''),
                    chunk_id=point['payload'].get('chunk_id', ''),
                    code=point['payload'].get('code', ''),
                    document_title=point['payload'].get('title', ''),
                    section_title=point['payload'].get('section_title', ''),
                    content=point['payload'].get('content', ''),
                    chunk_type=point['payload'].get('chunk_type', ''),
                    page=point['payload'].get('page', 0),
                    section=point['payload'].get('section', ''),
                    metadata=point['payload'].get('metadata', {}),
                    search_type='dense',
                    rank=rank + 1
                )
                results.append(result)
            
            logger.info(f"‚úÖ [DENSE] Found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå [DENSE] Error during dense search: {e}")
            return []

class AlphaBlending:
    """Alpha —Å–º–µ—à–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ BM25 –∏ Dense –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self, alpha: float = 0.5):
        self.alpha = alpha  # –í–µ—Å –¥–ª—è dense –ø–æ–∏—Å–∫–∞, (1-alpha) –¥–ª—è BM25
    
    def blend_scores(self, bm25_results: List[SearchResult], dense_results: List[SearchResult]) -> List[SearchResult]:
        """–°–º–µ—à–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å alpha –≤–µ—Å–∞–º–∏"""
        logger.info(f"üîÑ [ALPHA] Blending results with alpha={self.alpha}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º scores –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        bm25_scores = [r.score for r in bm25_results]
        dense_scores = [r.score for r in dense_results]
        
        if bm25_scores:
            bm25_max = max(bm25_scores)
            bm25_min = min(bm25_scores)
            bm25_range = bm25_max - bm25_min if bm25_max != bm25_min else 1
        else:
            bm25_max = bm25_min = bm25_range = 1
        
        if dense_scores:
            dense_max = max(dense_scores)
            dense_min = min(dense_scores)
            dense_range = dense_max - dense_min if dense_max != dense_min else 1
        else:
            dense_max = dense_min = dense_range = 1
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ ID
        results_dict = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for result in bm25_results:
            normalized_score = (result.score - bm25_min) / bm25_range
            blended_score = (1 - self.alpha) * normalized_score
            results_dict[result.id] = result
            results_dict[result.id].score = blended_score
        
        # –î–æ–±–∞–≤–ª—è–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º Dense —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for result in dense_results:
            normalized_score = (result.score - dense_min) / dense_range
            dense_score = self.alpha * normalized_score
            
            if result.id in results_dict:
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º scores
                results_dict[result.id].score += dense_score
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result.score = dense_score
                results_dict[result.id] = result
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        blended_results = sorted(results_dict.values(), key=lambda x: x.score, reverse=True)
        
        logger.info(f"‚úÖ [ALPHA] Blending completed. {len(blended_results)} unique results")
        return blended_results

class ReciprocalRankFusion:
    """Reciprocal Rank Fusion –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–Ω–≥–æ–≤"""
    
    def __init__(self, k: int = 60):
        self.k = k  # –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è RRF
    
    def fuse_ranks(self, bm25_results: List[SearchResult], dense_results: List[SearchResult]) -> List[SearchResult]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–Ω–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é RRF"""
        logger.info(f"üîÑ [RRF] Fusing ranks with k={self.k}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ RRF scores
        rrf_scores = defaultdict(float)
        results_dict = {}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for rank, result in enumerate(bm25_results):
            rrf_score = 1.0 / (self.k + rank + 1)
            rrf_scores[result.id] += rrf_score
            results_dict[result.id] = result
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Dense —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for rank, result in enumerate(dense_results):
            rrf_score = 1.0 / (self.k + rank + 1)
            rrf_scores[result.id] += rrf_score
            if result.id not in results_dict:
                results_dict[result.id] = result
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scores –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        for result_id, result in results_dict.items():
            result.score = rrf_scores[result_id]
            result.search_type = 'hybrid'
        
        fused_results = sorted(results_dict.values(), key=lambda x: x.score, reverse=True)
        
        logger.info(f"‚úÖ [RRF] Rank fusion completed. {len(fused_results)} results")
        return fused_results

class HybridSearchService:
    """–ì–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self, db_connection, embedding_service, qdrant_service, 
                 alpha: float = 0.5, use_rrf: bool = True, rrf_k: int = 60):
        self.db_conn = db_connection
        self.embedding_service = embedding_service
        self.qdrant_service = qdrant_service
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.bm25_search = BM25Search()
        self.dense_search = DenseSearch(embedding_service, qdrant_service)
        self.alpha_blending = AlphaBlending(alpha)
        self.rrf = ReciprocalRankFusion(rrf_k) if use_rrf else None
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.alpha = alpha
        self.use_rrf = use_rrf
        self.rrf_k = rrf_k
        
        # –ö—ç—à –¥–ª—è BM25
        self._bm25_trained = False
        self._documents_cache = []
        
        logger.info(f"‚úÖ [HYBRID_SEARCH] Initialized with alpha={alpha}, use_rrf={use_rrf}, rrf_k={rrf_k}")
    
    def _load_documents_for_bm25(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BM25"""
        if self._bm25_trained and self._documents_cache:
            return self._documents_cache
        
        try:
            logger.info("üìö [HYBRID_SEARCH] Loading documents for BM25 training")
            
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT 
                        chunk_id,
                        document_id,
                        document_title,
                        chapter,
                        section,
                        page_number,
                        chunk_type,
                        content,
                        metadata
                    FROM normative_chunks 
                    WHERE content IS NOT NULL AND LENGTH(content) > 10
                    ORDER BY document_id, chunk_id
                """)
                
                documents = cursor.fetchall()
                self._documents_cache = [dict(doc) for doc in documents]
                
                logger.info(f"‚úÖ [HYBRID_SEARCH] Loaded {len(self._documents_cache)} documents for BM25")
                return self._documents_cache
                
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_SEARCH] Error loading documents: {e}")
            return []
    
    def _train_bm25(self):
        """–û–±—É—á–µ–Ω–∏–µ BM25 –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self._bm25_trained:
            return
        
        documents = self._load_documents_for_bm25()
        if documents:
            self.bm25_search.fit(documents)
            self._bm25_trained = True
        else:
            logger.warning("‚ö†Ô∏è [HYBRID_SEARCH] No documents available for BM25 training")
    
    def search(self, query: str, k: int = 10, 
               document_filter: Optional[str] = None,
               chapter_filter: Optional[str] = None,
               chunk_type_filter: Optional[str] = None,
               use_alpha_blending: bool = True,
               use_rrf: Optional[bool] = None) -> List[SearchResult]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å BM25 + Dense + Alpha —Å–º–µ—à–∏–≤–∞–Ω–∏–µ + RRF
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            document_filter: –§–∏–ª—å—Ç—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
            chapter_filter: –§–∏–ª—å—Ç—Ä –ø–æ –≥–ª–∞–≤–µ
            chunk_type_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
            use_alpha_blending: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å alpha —Å–º–µ—à–∏–≤–∞–Ω–∏–µ
            use_rrf: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RRF (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        """
        try:
            logger.info(f"üîç [HYBRID_SEARCH] Starting hybrid search for: '{query}' with k={k}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
            search_k = max(k * 2, 20)  # –ò—â–µ–º –±–æ–ª—å—à–µ, —á–µ–º –Ω—É–∂–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            
            # 1. Dense –ø–æ–∏—Å–∫
            dense_filters = self._build_filters(document_filter, chapter_filter, chunk_type_filter)
            dense_results = self.dense_search.search(query, k=search_k, filters=dense_filters)
            
            # 2. BM25 –ø–æ–∏—Å–∫
            self._train_bm25()
            documents = self._load_documents_for_bm25()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–ª—è BM25
            if document_filter or chapter_filter or chunk_type_filter:
                documents = self._filter_documents(documents, document_filter, chapter_filter, chunk_type_filter)
            
            bm25_results = self.bm25_search.search(query, documents, k=search_k)
            
            logger.info(f"üìä [HYBRID_SEARCH] BM25: {len(bm25_results)}, Dense: {len(dense_results)}")
            
            # 3. –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            if use_rrf is None:
                use_rrf = self.use_rrf
            
            if use_rrf:
                # Reciprocal Rank Fusion
                final_results = self.rrf.fuse_ranks(bm25_results, dense_results)
            elif use_alpha_blending:
                # Alpha —Å–º–µ—à–∏–≤–∞–Ω–∏–µ
                final_results = self.alpha_blending.blend_scores(bm25_results, dense_results)
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç dense)
                final_results = dense_results + [r for r in bm25_results if r.id not in [d.id for d in dense_results]]
                final_results = sorted(final_results, key=lambda x: x.score, reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = final_results[:k]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–Ω–≥–∏
            for rank, result in enumerate(final_results):
                result.rank = rank + 1
            
            logger.info(f"‚úÖ [HYBRID_SEARCH] Hybrid search completed. {len(final_results)} final results")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_SEARCH] Error during hybrid search: {e}")
            return []
    
    def _build_filters(self, document_filter: Optional[str], 
                      chapter_filter: Optional[str], 
                      chunk_type_filter: Optional[str]) -> Optional[Dict]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è Qdrant"""
        must_conditions = []
        
        if document_filter and document_filter != 'all':
            must_conditions.append({
                "key": "code",
                "match": {"value": document_filter}
            })
        
        if chapter_filter:
            must_conditions.append({
                "key": "section",
                "match": {"value": chapter_filter}
            })
        
        if chunk_type_filter:
            must_conditions.append({
                "key": "chunk_type",
                "match": {"value": chunk_type_filter}
            })
        
        return {"must": must_conditions} if must_conditions else None
    
    def _filter_documents(self, documents: List[Dict[str, Any]], 
                         document_filter: Optional[str],
                         chapter_filter: Optional[str], 
                         chunk_type_filter: Optional[str]) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è BM25"""
        filtered = documents
        
        if document_filter and document_filter != 'all':
            filtered = [d for d in filtered if d.get('code') == document_filter]
        
        if chapter_filter:
            filtered = [d for d in filtered if d.get('section') == chapter_filter]
        
        if chunk_type_filter:
            filtered = [d for d in filtered if d.get('chunk_type') == chunk_type_filter]
        
        return filtered
    
    def get_search_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞"""
        return {
            "bm25_trained": self._bm25_trained,
            "documents_cached": len(self._documents_cache),
            "alpha": self.alpha,
            "use_rrf": self.use_rrf,
            "rrf_k": self.rrf_k,
            "corpus_size": self.bm25_search.corpus_size if self._bm25_trained else 0,
            "avg_doc_length": self.bm25_search.avgdl if self._bm25_trained else 0
        }
