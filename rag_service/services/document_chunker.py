import logging
import re
from typing import List, Dict, Any

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class DocumentChunker:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏"""
    
    def __init__(self):
        pass
    
    def create_chunks(self, text: str, document_id: int, filename: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            logger.info(f"üìù [CREATE_CHUNKS] Creating chunks for document {document_id}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º "–°—Ç—Ä–∞–Ω–∏—Ü–∞ X –∏–∑ Y"
            page_pattern = r'–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+(\d+)\s+–∏–∑\s+(\d+)'
            page_matches = list(re.finditer(page_pattern, text))
            
            chunks = []
            chunk_id = 1
            
            if page_matches:
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –Ω–∏–º
                logger.info(f"üìÑ [CREATE_CHUNKS] Found {len(page_matches)} page markers in document")
                
                for i, match in enumerate(page_matches):
                    page_num = int(match.group(1))
                    start_pos = match.end()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞)
                    if i + 1 < len(page_matches):
                        end_pos = page_matches[i + 1].start()
                    else:
                        end_pos = len(text)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text = text[start_pos:end_pos].strip()
                    
                    if page_text:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)
                        page_structure = self._extract_page_structure(page_text, page_num)
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —á–∞–Ω–∫–∏
                        page_chunks = self._split_page_into_chunks(page_text, chunk_size=1000)
                        
                        for chunk_text in page_chunks:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                            chunk_structure = self._identify_chunk_structure(chunk_text, page_structure)
                            
                            chunks.append({
                                'chunk_id': f"{document_id}_{page_num}_{chunk_id}",
                                'document_id': document_id,
                                'document_title': filename,
                                'content': chunk_text.strip(),
                                'chunk_type': 'paragraph',
                                'page': page_num,
                                'chapter': chunk_structure.get('chapter', ''),
                                'section': chunk_structure.get('section', '')
                            })
                            chunk_id += 1
            else:
                # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                logger.info(f"üìÑ [CREATE_CHUNKS] No page markers found, treating as single page document")
                page_chunks = self._split_page_into_chunks(text, chunk_size=1000)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document_structure = self._extract_document_structure(text)
                
                for chunk_text in page_chunks:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                    chunk_structure = self._identify_chunk_structure(chunk_text, document_structure)
                    
                    chunks.append({
                        'chunk_id': f"{document_id}_1_{chunk_id}",
                        'document_id': document_id,
                        'document_title': filename,
                        'content': chunk_text.strip(),
                        'chunk_type': 'paragraph',
                        'page': 1,
                        'chapter': chunk_structure.get('chapter', ''),
                        'section': chunk_structure.get('section', '')
                    })
                    chunk_id += 1
            
            logger.info(f"‚úÖ [CREATE_CHUNKS] Created {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS] Error creating chunks: {e}")
            return []
    
    def _split_page_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–µ —á–∞–Ω–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            from config.chunking_config import get_chunking_config, validate_chunking_config
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            config = get_chunking_config('default')
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if not validate_chunking_config(config):
                logger.warning("‚ö†Ô∏è [CHUNKING] Invalid chunking config, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            
            logger.info(f"üìù [CHUNKING] Using config: target={target_tokens}, min={min_tokens}, max={max_tokens}, overlap={overlap_ratio}")
            logger.info(f"üìù [CHUNKING] Input text length: {len(text)} characters")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [CHUNKING] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [CHUNKING] No sentences found, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            chunks = []
            current_chunk = []
            current_tokens = 0
            
            logger.info(f"üìù [CHUNKING] Starting chunk creation process...")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            for i, sentence in enumerate(sentences):
                sentence_tokens = self._estimate_tokens(sentence, config)
                logger.info(f"üìù [CHUNKING] Sentence {i+1}: {sentence_tokens} tokens, length: {len(sentence)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    logger.info(f"üìù [CHUNKING] Max tokens exceeded ({current_tokens + sentence_tokens} > {max_tokens}), creating chunk")
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    logger.info(f"üìù [CHUNKING] Created chunk {len(chunks)}: {len(chunk_text)} chars, {current_tokens} tokens")
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                    logger.info(f"üìù [CHUNKING] Started new chunk with {len(overlap_sentences)} overlap sentences, {current_tokens} tokens")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
                logger.info(f"üìù [CHUNKING] Added sentence to current chunk: {current_tokens} tokens total")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_tokens >= target_tokens and current_tokens >= min_tokens:
                    logger.info(f"üìù [CHUNKING] Target size reached ({current_tokens} >= {target_tokens}), creating chunk")
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    logger.info(f"üìù [CHUNKING] Created chunk {len(chunks)}: {len(chunk_text)} chars, {current_tokens} tokens")
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                    logger.info(f"üìù [CHUNKING] Started new chunk with {len(overlap_sentences)} overlap sentences, {current_tokens} tokens")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            if current_chunk and current_tokens >= min_tokens:
                logger.info(f"üìù [CHUNKING] Adding final chunk: {current_tokens} tokens")
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
                logger.info(f"üìù [CHUNKING] Created final chunk {len(chunks)}: {len(chunk_text)} chars, {current_tokens} tokens")
            elif current_chunk:
                logger.info(f"üìù [CHUNKING] Final chunk too small ({current_tokens} < {min_tokens}), merging with previous")
                if chunks:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    last_chunk = chunks[-1]
                    merged_chunk = last_chunk + ' ' + ' '.join(current_chunk)
                    chunks[-1] = merged_chunk
                    logger.info(f"üìù [CHUNKING] Merged final chunk with previous: {len(merged_chunk)} chars")
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    logger.info(f"üìù [CHUNKING] Created single chunk: {len(chunk_text)} chars, {current_tokens} tokens")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —á–∞–Ω–∫–∏ –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ª–æ–≥–∏–∫–∏ —Å–∫–ª–µ–π–∫–∏
            if not chunks:
                logger.warning("‚ö†Ô∏è [CHUNKING] No chunks created, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É —Å–∫–ª–µ–π–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if config.get('merge_enabled', True):
                logger.info(f"üìù [CHUNKING] Applying header merging logic to {len(chunks)} chunks...")
                chunks = self._merge_chunks_with_headers(chunks, config)
                logger.info(f"üìù [CHUNKING] After merging: {len(chunks)} chunks")
            
            logger.info(f"‚úÖ [CHUNKING] Created {len(chunks)} granular chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [GRANULAR_CHUNKS] Error creating granular chunks: {e}")
            import traceback
            logger.error(f"‚ùå [GRANULAR_CHUNKS] Traceback: {traceback.format_exc()}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ä–∞–∑–±–∏–µ–Ω–∏—é
            return self._simple_split_into_chunks(text, chunk_size)

    def _split_into_sentences(self, text: str, config: dict) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            sentence_patterns = config.get('sentence_patterns', [
                r'[.!?]+(?=\s+[–ê-–Ø–Å\d])',  # –û–±—ã—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                r'[.!?]+(?=\s+\d+\.)',      # –ü–µ—Ä–µ–¥ –Ω–æ–º–µ—Ä–∞–º–∏ –ø—É–Ω–∫—Ç–æ–≤
                r'[.!?]+(?=\s+[–ê-–Ø–Å]\s)',  # –ü–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                r'[.!?]+(?=\s*$)'           # –í –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
            ])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            combined_pattern = '|'.join(sentence_patterns)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç
            sentences = re.split(combined_pattern, text)
            
            # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            min_length = config.get('min_sentence_length', 10)
            cleaned_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > min_length:
                    cleaned_sentences.append(sentence)
            
            return cleaned_sentences
            
        except Exception as e:
            logger.error(f"‚ùå [SENTENCE_SPLIT] Error splitting into sentences: {e}")
            # Fallback: –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º
            return [s.strip() for s in text.split('.') if s.strip()]
    
    def _estimate_tokens(self, text: str, config: dict) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            tokens_per_char = config.get('tokens_per_char', 4)
            return max(1, len(text) // tokens_per_char)
        except Exception as e:
            logger.error(f"‚ùå [TOKEN_ESTIMATION] Error estimating tokens: {e}")
            return len(text) // 4
    
    def _get_overlap_sentences(self, sentences: List[str], overlap_ratio: float, config: dict) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        try:
            if not sentences:
                return []
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            min_overlap = config.get('min_overlap_sentences', 1)
            overlap_count = max(min_overlap, int(len(sentences) * overlap_ratio))
            return sentences[-overlap_count:]
            
        except Exception as e:
            logger.error(f"‚ùå [OVERLAP] Error getting overlap sentences: {e}")
            return sentences[-1:] if sentences else []
    
    def _merge_chunks_with_headers(self, chunks: List[str], config: dict) -> List[str]:
        """–°–∫–ª–µ–π–∫–∞ —á–∞–Ω–∫–æ–≤ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–±—Ä—ã–≤–∞ —Ü–∏—Ç–∞—Ç"""
        try:
            if len(chunks) <= 1:
                return chunks
            
            merged_chunks = []
            current_chunk = chunks[0]
            
            for i in range(1, len(chunks)):
                next_chunk = chunks[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —á–∞–Ω–∫–∏
                should_merge = self._should_merge_chunks(current_chunk, next_chunk, config)
                
                if should_merge:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                    current_chunk = current_chunk + ' ' + next_chunk
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                    merged_chunks.append(current_chunk)
                    current_chunk = next_chunk
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            merged_chunks.append(current_chunk)
            
            logger.info(f"üìù [MERGE_HEADERS] Merged {len(chunks)} chunks into {len(merged_chunks)} chunks")
            return merged_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_HEADERS] Error merging chunks: {e}")
            return chunks
    
    def _should_merge_chunks(self, chunk1: str, chunk2: str, config: dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
            combined_tokens = self._estimate_tokens(chunk1, config) + self._estimate_tokens(chunk2, config)
            
            # –ï—Å–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            max_merged = config.get('max_merged_tokens', 1200)
            if combined_tokens > max_merged:
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            header_patterns = config.get('header_patterns', ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–ø—É–Ω–∫—Ç'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            if any(pattern in chunk1.lower() for pattern in header_patterns):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –≤—Ç–æ—Ä–æ–π —á–∞–Ω–∫ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if chunk2 and not chunk2[0].isupper():
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            unfinished_patterns = config.get('unfinished_patterns', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–≤—ã—á–∫–∏
            quotes = unfinished_patterns.get('quotes', ['"', '¬´', '¬ª'])
            if any(chunk1.count(q) % 2 != 0 for q in quotes):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–±–∫–∏
            brackets = unfinished_patterns.get('brackets', ['(', '[', '{'])
            if any(chunk1.count(b) != chunk1.count(self._get_closing_bracket(b)) for b in brackets):
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_LOGIC] Error in merge logic: {e}")
            return False
    
    def _get_closing_bracket(self, opening_bracket: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –¥–ª—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π"""
        bracket_pairs = {
            '(': ')',
            '[': ']',
            '{': '}',
            '<': '>'
        }
        return bracket_pairs.get(opening_bracket, '')

    def _simple_split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è."""
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence + ". "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    def _extract_page_structure(self, page_text: str, page_num: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)"""
        try:
            structure = {
                'page': page_num,
                'chapters': [],
                'sections': [],
                'headers': []
            }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤ –∏ —Ä–∞–∑–¥–µ–ª–æ–≤
            chapter_patterns = [
                r'^–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–ê–°–¢–¨\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–∞—Å—Ç—å\s+(\d+)\s*[\.\-]?\s*(.+)$'
            ]
            
            section_patterns = [
                r'^(\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+)\s+(.+)$'
            ]
            
            lines = page_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        structure['chapters'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure['sections'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
                if line.isupper() and len(line) > 5 and len(line) < 100:
                    structure['headers'].append(line)
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PAGE_STRUCTURE] Error extracting page structure: {e}")
            return {'page': page_num, 'chapters': [], 'sections': [], 'headers': []}
    
    def _extract_document_structure(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            structure = {
                'chapters': [],
                'sections': [],
                'headers': []
            }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            chapter_patterns = [
                r'^–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)$'
            ]
            
            section_patterns = [
                r'^(\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+\.\d+)\s+(.+)$'
            ]
            
            lines = text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        structure['chapters'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure['sections'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_STRUCTURE] Error extracting document structure: {e}")
            return {'chapters': [], 'sections': [], 'headers': []}
    
    def _identify_chunk_structure(self, chunk_text: str, structure: Dict[str, Any]) -> Dict[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫"""
        try:
            result = {'chapter': '', 'section': ''}
            
            if not structure or not chunk_text:
                return result
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞ –≤ —á–∞–Ω–∫–µ
            section_patterns = [
                r'(\d+\.\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+)\s+(.+)',
                r'(\d+)\s+(.+)'
            ]
            
            for pattern in section_patterns:
                match = re.search(pattern, chunk_text)
                if match:
                    section_number = match.group(1)
                    section_title = match.group(2).strip()
                    
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≥–ª–∞–≤—É
                    chapter_num = section_number.split('.')[0]
                    for chapter in structure.get('chapters', []):
                        if chapter['number'] == chapter_num:
                            result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter['title']}"
                            break
                    
                    result['section'] = f"{section_number}. {section_title}"
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª, –∏—â–µ–º –≥–ª–∞–≤—É
            if not result['section']:
                chapter_patterns = [
                    r'–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)'
                ]
                
                for pattern in chapter_patterns:
                    match = re.search(pattern, chunk_text, re.IGNORECASE)
                    if match:
                        chapter_num = match.group(1)
                        chapter_title = match.group(2).strip()
                        result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter_title}"
                        break
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [IDENTIFY_CHUNK_STRUCTURE] Error identifying chunk structure: {e}")
            return {'chapter': '', 'section': ''}
