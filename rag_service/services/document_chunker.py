import logging
import re
from typing import List, Dict, Any

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class DocumentChunker:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏"""
    
    def __init__(self):
        pass
    
    def create_chunks(self, text: str, document_id: int, filename: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            logger.info(f"üìù [CREATE_CHUNKS] Creating chunks for document {document_id}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º "–°—Ç—Ä–∞–Ω–∏—Ü–∞ X –∏–∑ Y"
            page_pattern = r'–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+(\d+)\s+–∏–∑\s+(\d+)'
            page_matches = list(re.finditer(page_pattern, text))
            
            chunks = []
            chunk_id = 1
            
            if page_matches:
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –Ω–∏–º
                logger.info(f"üìÑ [CREATE_CHUNKS] Found {len(page_matches)} page markers in document")
                
                for i, match in enumerate(page_matches):
                    page_num = int(match.group(1))
                    start_pos = match.end()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞)
                    if i + 1 < len(page_matches):
                        end_pos = page_matches[i + 1].start()
                    else:
                        end_pos = len(text)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text = text[start_pos:end_pos].strip()
                    
                    if page_text:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)
                        page_structure = self._extract_page_structure(page_text, page_num)
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —á–∞–Ω–∫–∏
                        page_chunks = self._split_page_into_chunks(page_text, chunk_size=1000)
                        
                        for chunk_text in page_chunks:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                            chunk_structure = self._identify_chunk_structure(chunk_text, page_structure)
                            
                            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                            structure_metadata = self._create_structure_metadata(chunk_text, page_structure, chunk_structure)
                            
                            chunks.append({
                                'chunk_id': f"{document_id}_{page_num}_{chunk_id}",
                                'document_id': document_id,
                                'document_title': filename,
                                'content': chunk_text.strip(),
                                'chunk_type': 'paragraph',
                                'page': page_num,
                                'chapter': chunk_structure.get('chapter', ''),
                                'section': chunk_structure.get('section', ''),
                                'structure_metadata': structure_metadata
                            })
                            chunk_id += 1
            else:
                # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                logger.info(f"üìÑ [CREATE_CHUNKS] No page markers found, treating as single page document")
                page_chunks = self._split_page_into_chunks(text, chunk_size=1000)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document_structure = self._extract_document_structure(text)
                
                for chunk_text in page_chunks:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                    chunk_structure = self._identify_chunk_structure(chunk_text, document_structure)
                    
                    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                    structure_metadata = self._create_structure_metadata(chunk_text, document_structure, chunk_structure)
                    
                    chunks.append({
                        'chunk_id': f"{document_id}_1_{chunk_id}",
                        'document_id': document_id,
                        'document_title': filename,
                        'content': chunk_text.strip(),
                        'chunk_type': 'paragraph',
                        'page': 1,
                        'chapter': chunk_structure.get('chapter', ''),
                        'section': chunk_structure.get('section', ''),
                        'structure_metadata': structure_metadata
                    })
                    chunk_id += 1
            
            logger.info(f"‚úÖ [CREATE_CHUNKS] Created {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS] Error creating chunks: {e}")
            return []
    
    def _split_page_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            from config.chunking_config import get_chunking_config, validate_chunking_config
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            config = get_chunking_config('default')
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if not validate_chunking_config(config):
                logger.warning("‚ö†Ô∏è [CHUNKING] Invalid chunking config, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–æ –ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if config.get('semantic_chunking', True):
                logger.info("üìù [CHUNKING] Using semantic chunking with meaning-based analysis")
                logger.info(f"üìù [CHUNKING] Input text length: {len(text)} characters")
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
                semantic_chunks = self._create_semantic_chunks(text, config)
                
                if semantic_chunks:
                    logger.info(f"‚úÖ [CHUNKING] Created {len(semantic_chunks)} semantic chunks")
                    return semantic_chunks
                else:
                    logger.warning("‚ö†Ô∏è [CHUNKING] No semantic chunks created, falling back to hierarchical")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–æ –ª–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if not config.get('hierarchical_chunking', True):
                logger.info("üìù [CHUNKING] Hierarchical chunking disabled, using standard chunking")
                return self._standard_chunking(text, config)
            
            logger.info("üìù [CHUNKING] Using hierarchical chunking with structure preservation")
            logger.info(f"üìù [CHUNKING] Input text length: {len(text)} characters")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_structure = self._extract_document_structure(text)
            logger.info(f"üìù [CHUNKING] Extracted structure: {len(document_structure['chapters'])} chapters, {len(document_structure['sections'])} sections")
            
            # –°–æ–∑–¥–∞–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
            hierarchical_chunks = self._create_hierarchical_chunks(text, document_structure, config)
            
            if not hierarchical_chunks:
                logger.warning("‚ö†Ô∏è [CHUNKING] No hierarchical chunks created, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            logger.info(f"‚úÖ [CHUNKING] Created {len(hierarchical_chunks)} hierarchical chunks")
            return hierarchical_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Error creating hierarchical chunks: {e}")
            import traceback
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Traceback: {traceback.format_exc()}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ä–∞–∑–±–∏–µ–Ω–∏—é
            return self._simple_split_into_chunks(text, chunk_size)

    def _standard_chunking(self, text: str, config: dict) -> List[str]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏–∏"""
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            
            logger.info(f"üìù [STANDARD_CHUNKING] Using config: target={target_tokens}, min={min_tokens}, max={max_tokens}, overlap={overlap_ratio}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [STANDARD_CHUNKING] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [STANDARD_CHUNKING] No sentences found, using fallback")
                return self._simple_split_into_chunks(text, 1000)
            
            chunks = []
            current_chunk = []
            current_tokens = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            for i, sentence in enumerate(sentences):
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_tokens >= target_tokens and current_tokens >= min_tokens:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            if current_chunk and current_tokens >= min_tokens:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            elif current_chunk:
                if chunks:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    last_chunk = chunks[-1]
                    merged_chunk = last_chunk + ' ' + ' '.join(current_chunk)
                    chunks[-1] = merged_chunk
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
            
            logger.info(f"‚úÖ [STANDARD_CHUNKING] Created {len(chunks)} standard chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [STANDARD_CHUNKING] Error in standard chunking: {e}")
            return self._simple_split_into_chunks(text, 1000)

    def _create_semantic_chunks(self, text: str, config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º—ã—Å–ª–∞"""
        try:
            logger.info("üìù [SEMANTIC_CHUNKS] Creating semantic chunks with meaning-based analysis")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            semantic_threshold = config.get('semantic_similarity_threshold', 0.7)
            window_size = config.get('semantic_window_size', 3)
            topic_change_threshold = config.get('topic_change_threshold', 0.3)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [SEMANTIC_CHUNKS] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [SEMANTIC_CHUNKS] No sentences found")
                return []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            semantic_analysis = self._analyze_semantic_structure(sentences, config)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤
            semantic_boundaries = self._find_semantic_boundaries(sentences, semantic_analysis, config)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
            semantic_chunks = self._create_chunks_from_semantic_boundaries(
                sentences, semantic_boundaries, config
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö —á–∞–Ω–∫–æ–≤
            if config.get('semantic_merge_threshold', 0.85) > 0:
                semantic_chunks = self._merge_semantically_similar_chunks(semantic_chunks, config)
            
            logger.info(f"‚úÖ [SEMANTIC_CHUNKS] Created {len(semantic_chunks)} semantic chunks")
            return semantic_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_CHUNKS] Error creating semantic chunks: {e}")
            return []

    def _analyze_semantic_structure(self, sentences: List[str], config: dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        try:
            from config.chunking_config import get_semantic_patterns
            import re
            
            patterns = get_semantic_patterns()
            
            analysis = {
                'topic_indicators': [],
                'coherence_indicators': [],
                'semantic_boundaries': [],
                'domain_keywords': [],
                'sentence_semantics': []
            }
            
            for i, sentence in enumerate(sentences):
                sentence_analysis = {
                    'index': i,
                    'text': sentence,
                    'topic_indicators': [],
                    'coherence_indicators': [],
                    'semantic_boundaries': [],
                    'domain_keywords': [],
                    'semantic_score': 0.0
                }
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ç–µ–º
                for pattern in patterns['topic_indicators']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['topic_indicators'].append(pattern)
                        analysis['topic_indicators'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏
                for pattern in patterns['coherence_indicators']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['coherence_indicators'].append(pattern)
                        analysis['coherence_indicators'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
                for pattern in patterns['semantic_boundaries']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['semantic_boundaries'].append(pattern)
                        analysis['semantic_boundaries'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                for domain, domain_patterns in patterns['domain_specific'].items():
                    for pattern in domain_patterns:
                        if re.search(pattern, sentence, re.IGNORECASE):
                            sentence_analysis['domain_keywords'].append((domain, pattern))
                            analysis['domain_keywords'].append((i, domain, pattern))
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentence_analysis['semantic_score'] = self._calculate_semantic_score(sentence_analysis)
                
                analysis['sentence_semantics'].append(sentence_analysis)
            
            logger.info(f"üìù [SEMANTIC_ANALYSIS] Found {len(analysis['topic_indicators'])} topic indicators, {len(analysis['semantic_boundaries'])} boundaries")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_ANALYSIS] Error analyzing semantic structure: {e}")
            return {'sentence_semantics': []}

    def _calculate_semantic_score(self, sentence_analysis: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        try:
            score = 0.0
            
            # –ë–∞–ª–ª—ã –∑–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            score += len(sentence_analysis['topic_indicators']) * 0.3
            score += len(sentence_analysis['coherence_indicators']) * 0.2
            score += len(sentence_analysis['semantic_boundaries']) * 0.4
            score += len(sentence_analysis['domain_keywords']) * 0.1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 0-1
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_SCORE] Error calculating semantic score: {e}")
            return 0.0

    def _find_semantic_boundaries(self, sentences: List[str], analysis: Dict[str, Any], config: dict) -> List[int]:
        """–ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            boundaries = []
            window_size = config.get('semantic_window_size', 3)
            topic_change_threshold = config.get('topic_change_threshold', 0.3)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            for boundary_idx in analysis['semantic_boundaries']:
                if boundary_idx not in boundaries:
                    boundaries.append(boundary_idx)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–º—ã –≤ —Å–∫–æ–ª—å–∑—è—â–µ–º –æ–∫–Ω–µ
            for i in range(window_size, len(sentences) - window_size):
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ –æ–∫–Ω–µ
                window_similarity = self._calculate_window_similarity(
                    sentences, i, window_size, analysis
                )
                
                # –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞, —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–∞
                if window_similarity < topic_change_threshold:
                    if i not in boundaries:
                        boundaries.append(i)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
            boundaries.sort()
            
            logger.info(f"üìù [SEMANTIC_BOUNDARIES] Found {len(boundaries)} semantic boundaries")
            return boundaries
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_BOUNDARIES] Error finding semantic boundaries: {e}")
            return []

    def _calculate_window_similarity(self, sentences: List[str], center_idx: int, window_size: int, analysis: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ –æ–∫–Ω–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ–∫–Ω–µ
            start_idx = max(0, center_idx - window_size)
            end_idx = min(len(sentences), center_idx + window_size + 1)
            
            window_sentences = sentences[start_idx:end_idx]
            
            if len(window_sentences) < 2:
                return 1.0
            
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            similarity_scores = []
            
            for i in range(len(window_sentences) - 1):
                sent1 = window_sentences[i].lower()
                sent2 = window_sentences[i + 1].lower()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø—Ä–æ—Å—Ç—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ)
                words1 = set([w for w in sent1.split() if len(w) > 3 and w.isalpha()])
                words2 = set([w for w in sent2.split() if len(w) > 3 and w.isalpha()])
                
                if words1 and words2:
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
                    intersection = len(words1.intersection(words2))
                    union = len(words1.union(words2))
                    jaccard = intersection / union if union > 0 else 0.0
                    similarity_scores.append(jaccard)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå [WINDOW_SIMILARITY] Error calculating window similarity: {e}")
            return 0.0

    def _create_chunks_from_semantic_boundaries(self, sentences: List[str], boundaries: List[int], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü"""
        try:
            chunks = []
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
            all_boundaries = [0] + boundaries + [len(sentences)]
            all_boundaries = sorted(list(set(all_boundaries)))
            
            for i in range(len(all_boundaries) - 1):
                start_idx = all_boundaries[i]
                end_idx = all_boundaries[i + 1]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —á–∞–Ω–∫–∞
                chunk_sentences = sentences[start_idx:end_idx]
                
                if not chunk_sentences:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞
                chunk_text = ' '.join(chunk_sentences)
                chunk_tokens = self._estimate_tokens(chunk_text, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                if chunk_tokens < min_tokens:
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º
                    if i + 2 < len(all_boundaries):
                        next_start = all_boundaries[i + 1]
                        next_end = all_boundaries[i + 2]
                        next_sentences = sentences[next_start:next_end]
                        chunk_sentences.extend(next_sentences)
                        chunk_text = ' '.join(chunk_sentences)
                        chunk_tokens = self._estimate_tokens(chunk_text, config)
                
                if chunk_tokens > max_tokens:
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
                    sub_chunks = self._split_large_chunk(chunk_sentences, config)
                    chunks.extend(sub_chunks)
                else:
                    chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_CHUNKS_CREATION] Error creating chunks from boundaries: {e}")
            return []

    def _split_large_chunk(self, sentences: List[str], config: dict) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏"""
        try:
            chunks = []
            target_tokens = config['target_tokens']
            max_tokens = config['max_tokens']
            
            current_chunk = []
            current_tokens = 0
            
            for sentence in sentences:
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                    current_chunk = [sentence]
                    current_tokens = sentence_tokens
                else:
                    current_chunk.append(sentence)
                    current_tokens += sentence_tokens
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SPLIT_LARGE_CHUNK] Error splitting large chunk: {e}")
            return []

    def _merge_semantically_similar_chunks(self, chunks: List[str], config: dict) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            merge_threshold = config.get('semantic_merge_threshold', 0.85)
            
            if len(chunks) <= 1:
                return chunks
            
            merged_chunks = []
            i = 0
            
            while i < len(chunks):
                current_chunk = chunks[i]
                merged_chunk = current_chunk
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
                j = i + 1
                while j < len(chunks):
                    next_chunk = chunks[j]
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarity = self._calculate_chunk_similarity(current_chunk, next_chunk)
                    
                    if similarity >= merge_threshold:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                        merged_chunk += ' ' + next_chunk
                        j += 1
                    else:
                        break
                
                merged_chunks.append(merged_chunk.strip())
                i = j
            
            logger.info(f"üìù [SEMANTIC_MERGE] Merged {len(chunks)} chunks into {len(merged_chunks)} chunks")
            return merged_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_MERGE] Error merging similar chunks: {e}")
            return chunks

    def _calculate_chunk_similarity(self, chunk1: str, chunk2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            words1 = set([w.lower() for w in chunk1.split() if len(w) > 3 and w.isalpha()])
            words2 = set([w.lower() for w in chunk2.split() if len(w) > 3 and w.isalpha()])
            
            if not words1 or not words2:
                return 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå [CHUNK_SIMILARITY] Error calculating chunk similarity: {e}")
            return 0.0

    def _create_hierarchical_chunks(self, text: str, structure: Dict[str, Any], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            logger.info("üìù [HIERARCHICAL_CHUNKS] Creating hierarchical chunks with structure preservation")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            preserve_structure = config.get('preserve_structure', True)
            chapter_boundaries = config.get('chapter_boundaries', True)
            section_boundaries = config.get('section_boundaries', True)
            
            chunks = []
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if not structure['chapters'] and not structure['sections']:
                logger.info("üìù [HIERARCHICAL_CHUNKS] No structure found, using standard chunking")
                return self._standard_chunking(text, config)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [HIERARCHICAL_CHUNKS] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [HIERARCHICAL_CHUNKS] No sentences found")
                return []
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            sentence_structure_map = self._map_sentences_to_structure(sentences, structure)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –µ–¥–∏–Ω–∏—Ü–∞–º
            structural_units = self._group_sentences_by_structure(sentences, sentence_structure_map, structure)
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            for unit in structural_units:
                unit_chunks = self._create_chunks_for_structural_unit(unit, config)
                chunks.extend(unit_chunks)
            
            logger.info(f"‚úÖ [HIERARCHICAL_CHUNKS] Created {len(chunks)} hierarchical chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Error creating hierarchical chunks: {e}")
            return []

    def _map_sentences_to_structure(self, sentences: List[str], structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º"""
        try:
            sentence_map = []
            
            for i, sentence in enumerate(sentences):
                sentence_info = {
                    'index': i,
                    'text': sentence,
                    'chapter': None,
                    'section': None,
                    'subsection': None,
                    'paragraph': None,
                    'special_structure': None
                }
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≥–ª–∞–≤–µ
                for chapter in structure['chapters']:
                    if chapter['title'].lower() in sentence.lower() or chapter['number'] in sentence:
                        sentence_info['chapter'] = chapter
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–¥–µ–ª—É
                for section in structure['sections']:
                    if section['number'] in sentence or section['title'].lower() in sentence.lower():
                        sentence_info['section'] = section
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∞–±–∑–∞—Ü—É
                for paragraph in structure['paragraphs']:
                    if paragraph['text'].lower() in sentence.lower():
                        sentence_info['paragraph'] = paragraph
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
                for special in structure['special_structures']:
                    if special['text'].lower() in sentence.lower():
                        sentence_info['special_structure'] = special
                        break
                
                sentence_map.append(sentence_info)
            
            return sentence_map
            
        except Exception as e:
            logger.error(f"‚ùå [MAP_SENTENCES] Error mapping sentences to structure: {e}")
            return []

    def _group_sentences_by_structure(self, sentences: List[str], sentence_map: List[Dict[str, Any]], structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –µ–¥–∏–Ω–∏—Ü–∞–º"""
        try:
            structural_units = []
            current_unit = None
            
            for i, sentence_info in enumerate(sentence_map):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
                current_chapter = sentence_info.get('chapter')
                current_section = sentence_info.get('section')
                current_paragraph = sentence_info.get('paragraph')
                current_special = sentence_info.get('special_structure')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
                should_start_new_unit = False
                
                if current_unit is None:
                    should_start_new_unit = True
                elif current_chapter and current_chapter != current_unit.get('chapter'):
                    should_start_new_unit = True
                elif current_section and current_section != current_unit.get('section'):
                    should_start_new_unit = True
                elif current_special and current_special != current_unit.get('special_structure'):
                    should_start_new_unit = True
                
                if should_start_new_unit:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –µ–¥–∏–Ω–∏—Ü—É
                    if current_unit:
                        structural_units.append(current_unit)
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –µ–¥–∏–Ω–∏—Ü—É
                    current_unit = {
                        'chapter': current_chapter,
                        'section': current_section,
                        'paragraph': current_paragraph,
                        'special_structure': current_special,
                        'sentences': [sentence_info['text']],
                        'start_index': i,
                        'end_index': i
                    }
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–π –µ–¥–∏–Ω–∏—Ü–µ
                    current_unit['sentences'].append(sentence_info['text'])
                    current_unit['end_index'] = i
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –µ–¥–∏–Ω–∏—Ü—É
            if current_unit:
                structural_units.append(current_unit)
            
            logger.info(f"üìù [GROUP_SENTENCES] Created {len(structural_units)} structural units")
            return structural_units
            
        except Exception as e:
            logger.error(f"‚ùå [GROUP_SENTENCES] Error grouping sentences: {e}")
            return []

    def _create_chunks_for_structural_unit(self, unit: Dict[str, Any], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã"""
        try:
            sentences = unit['sentences']
            if not sentences:
                return []
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            
            chunks = []
            current_chunk = []
            current_tokens = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã
            for sentence in sentences:
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_tokens >= target_tokens and current_tokens >= min_tokens:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            if current_chunk and current_tokens >= min_tokens:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            elif current_chunk:
                if chunks:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    last_chunk = chunks[-1]
                    merged_chunk = last_chunk + ' ' + ' '.join(current_chunk)
                    chunks[-1] = merged_chunk
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS_FOR_UNIT] Error creating chunks for structural unit: {e}")
            return []

    def _create_structure_metadata(self, chunk_text: str, structure: Dict[str, Any], chunk_structure: Dict[str, str]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —á–∞–Ω–∫–∞"""
        try:
            metadata = {
                'hierarchy_level': 0,
                'chapter_info': None,
                'section_info': None,
                'subsection_info': None,
                'paragraph_info': None,
                'special_structure_info': None,
                'structural_context': [],
                'document_type': 'unknown'
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∏–µ—Ä–∞—Ä—Ö–∏–∏
            if chunk_structure.get('chapter'):
                metadata['hierarchy_level'] = 1
            if chunk_structure.get('section'):
                metadata['hierarchy_level'] = 2
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–ª–∞–≤–µ
            for chapter in structure.get('chapters', []):
                if chapter['title'].lower() in chunk_text.lower() or chapter['number'] in chunk_text:
                    metadata['chapter_info'] = {
                        'number': chapter['number'],
                        'title': chapter['title'],
                        'line': chapter.get('line', 0),
                        'level': chapter.get('level', 1)
                    }
                    break
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–¥–µ–ª–µ
            for section in structure.get('sections', []):
                if section['number'] in chunk_text or section['title'].lower() in chunk_text.lower():
                    metadata['section_info'] = {
                        'number': section['number'],
                        'title': section['title'],
                        'line': section.get('line', 0),
                        'level': section.get('level', 2),
                        'chapter': section.get('chapter')
                    }
                    break
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–±–∑–∞—Ü–µ
            for paragraph in structure.get('paragraphs', []):
                if paragraph['text'].lower() in chunk_text.lower():
                    metadata['paragraph_info'] = {
                        'text': paragraph['text'],
                        'line': paragraph.get('line', 0),
                        'section': paragraph.get('section'),
                        'chapter': paragraph.get('chapter')
                    }
                    break
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            for special in structure.get('special_structures', []):
                if special['text'].lower() in chunk_text.lower():
                    metadata['special_structure_info'] = {
                        'type': special['type'],
                        'number': special.get('number'),
                        'text': special['text'],
                        'line': special.get('line', 0)
                    }
                    break
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            structural_context = []
            if metadata['chapter_info']:
                structural_context.append(f"–ì–ª–∞–≤–∞ {metadata['chapter_info']['number']}: {metadata['chapter_info']['title']}")
            if metadata['section_info']:
                structural_context.append(f"–†–∞–∑–¥–µ–ª {metadata['section_info']['number']}: {metadata['section_info']['title']}")
            if metadata['paragraph_info']:
                structural_context.append(f"–ê–±–∑–∞—Ü: {metadata['paragraph_info']['text'][:50]}...")
            if metadata['special_structure_info']:
                structural_context.append(f"{metadata['special_structure_info']['type']}: {metadata['special_structure_info']['text']}")
            
            metadata['structural_context'] = structural_context
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            chunk_lower = chunk_text.lower()
            if any(keyword in chunk_lower for keyword in ['–≥–æ—Å—Ç', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç']):
                metadata['document_type'] = 'gost'
            elif any(keyword in chunk_lower for keyword in ['—Å–ø', '—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª']):
                metadata['document_type'] = 'sp'
            elif any(keyword in chunk_lower for keyword in ['—Å–Ω–∏–ø', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã']):
                metadata['document_type'] = 'snip'
            elif any(keyword in chunk_lower for keyword in ['–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π', '–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π']):
                metadata['document_type'] = 'corporate'
            
            return metadata
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_STRUCTURE_METADATA] Error creating structure metadata: {e}")
            return {
                'hierarchy_level': 0,
                'chapter_info': None,
                'section_info': None,
                'subsection_info': None,
                'paragraph_info': None,
                'special_structure_info': None,
                'structural_context': [],
                'document_type': 'unknown'
            }

    def _split_into_sentences(self, text: str, config: dict) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            sentence_patterns = config.get('sentence_patterns', [
                r'[.!?]+(?=\s+[–ê-–Ø–Å\d])',  # –û–±—ã—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                r'[.!?]+(?=\s+\d+\.)',      # –ü–µ—Ä–µ–¥ –Ω–æ–º–µ—Ä–∞–º–∏ –ø—É–Ω–∫—Ç–æ–≤
                r'[.!?]+(?=\s+[–ê-–Ø–Å]\s)',  # –ü–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                r'[.!?]+(?=\s*$)'           # –í –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
            ])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            combined_pattern = '|'.join(sentence_patterns)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç
            sentences = re.split(combined_pattern, text)
            
            # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            min_length = config.get('min_sentence_length', 10)
            cleaned_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > min_length:
                    cleaned_sentences.append(sentence)
            
            return cleaned_sentences
            
        except Exception as e:
            logger.error(f"‚ùå [SENTENCE_SPLIT] Error splitting into sentences: {e}")
            # Fallback: –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º
            return [s.strip() for s in text.split('.') if s.strip()]
    
    def _estimate_tokens(self, text: str, config: dict) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            tokens_per_char = config.get('tokens_per_char', 4)
            return max(1, len(text) // tokens_per_char)
        except Exception as e:
            logger.error(f"‚ùå [TOKEN_ESTIMATION] Error estimating tokens: {e}")
            return len(text) // 4
    
    def _get_overlap_sentences(self, sentences: List[str], overlap_ratio: float, config: dict) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        try:
            if not sentences:
                return []
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            min_overlap = config.get('min_overlap_sentences', 1)
            overlap_count = max(min_overlap, int(len(sentences) * overlap_ratio))
            return sentences[-overlap_count:]
            
        except Exception as e:
            logger.error(f"‚ùå [OVERLAP] Error getting overlap sentences: {e}")
            return sentences[-1:] if sentences else []
    
    def _merge_chunks_with_headers(self, chunks: List[str], config: dict) -> List[str]:
        """–°–∫–ª–µ–π–∫–∞ —á–∞–Ω–∫–æ–≤ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–±—Ä—ã–≤–∞ —Ü–∏—Ç–∞—Ç"""
        try:
            if len(chunks) <= 1:
                return chunks
            
            merged_chunks = []
            current_chunk = chunks[0]
            
            for i in range(1, len(chunks)):
                next_chunk = chunks[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —á–∞–Ω–∫–∏
                should_merge = self._should_merge_chunks(current_chunk, next_chunk, config)
                
                if should_merge:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                    current_chunk = current_chunk + ' ' + next_chunk
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                    merged_chunks.append(current_chunk)
                    current_chunk = next_chunk
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            merged_chunks.append(current_chunk)
            
            logger.info(f"üìù [MERGE_HEADERS] Merged {len(chunks)} chunks into {len(merged_chunks)} chunks")
            return merged_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_HEADERS] Error merging chunks: {e}")
            return chunks
    
    def _should_merge_chunks(self, chunk1: str, chunk2: str, config: dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
            combined_tokens = self._estimate_tokens(chunk1, config) + self._estimate_tokens(chunk2, config)
            
            # –ï—Å–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            max_merged = config.get('max_merged_tokens', 1200)
            if combined_tokens > max_merged:
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            header_patterns = config.get('header_patterns', ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–ø—É–Ω–∫—Ç'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            if any(pattern in chunk1.lower() for pattern in header_patterns):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –≤—Ç–æ—Ä–æ–π —á–∞–Ω–∫ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if chunk2 and not chunk2[0].isupper():
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            unfinished_patterns = config.get('unfinished_patterns', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–≤—ã—á–∫–∏
            quotes = unfinished_patterns.get('quotes', ['"', '¬´', '¬ª'])
            if any(chunk1.count(q) % 2 != 0 for q in quotes):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–±–∫–∏
            brackets = unfinished_patterns.get('brackets', ['(', '[', '{'])
            if any(chunk1.count(b) != chunk1.count(self._get_closing_bracket(b)) for b in brackets):
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_LOGIC] Error in merge logic: {e}")
            return False
    
    def _get_closing_bracket(self, opening_bracket: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –¥–ª—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π"""
        bracket_pairs = {
            '(': ')',
            '[': ']',
            '{': '}',
            '<': '>'
        }
        return bracket_pairs.get(opening_bracket, '')

    def _simple_split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è."""
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence + ". "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    def _extract_page_structure(self, page_text: str, page_num: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)"""
        try:
            structure = {
                'page': page_num,
                'chapters': [],
                'sections': [],
                'headers': []
            }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤ –∏ —Ä–∞–∑–¥–µ–ª–æ–≤
            chapter_patterns = [
                r'^–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–ê–°–¢–¨\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–∞—Å—Ç—å\s+(\d+)\s*[\.\-]?\s*(.+)$'
            ]
            
            section_patterns = [
                r'^(\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+)\s+(.+)$'
            ]
            
            lines = page_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        structure['chapters'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure['sections'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
                if line.isupper() and len(line) > 5 and len(line) < 100:
                    structure['headers'].append(line)
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PAGE_STRUCTURE] Error extracting page structure: {e}")
            return {'page': page_num, 'chapters': [], 'sections': [], 'headers': []}
    
    def _extract_document_structure(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–µ–π"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            from config.chunking_config import get_hierarchical_patterns
            
            structure = {
                'chapters': [],
                'sections': [],
                'paragraphs': [],
                'special_structures': [],
                'headers': []
            }
            
            # –ü–æ–ª—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            patterns = get_hierarchical_patterns()
            chapter_patterns = patterns['chapters']
            section_patterns = patterns['sections']
            paragraph_patterns = patterns['paragraphs']
            special_patterns = patterns['special_structures']
            
            lines = text.split('\n')
            current_chapter = None
            current_section = None
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        current_chapter = {
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line_num,
                            'level': 1
                        }
                        structure['chapters'].append(current_chapter)
                        current_section = None
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        section_number = match.group(1)
                        section_title = match.group(2).strip()
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
                        level = len(section_number.split('.'))
                        
                        current_section = {
                            'number': section_number,
                            'title': section_title,
                            'line': line_num,
                            'level': level,
                            'chapter': current_chapter['number'] if current_chapter else None
                        }
                        structure['sections'].append(current_section)
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–±–∑–∞—Ü–µ–≤
                for pattern in paragraph_patterns:
                    match = re.match(pattern, line)
                    if match:
                        paragraph = {
                            'text': match.group(1).strip(),
                            'line': line_num,
                            'section': current_section['number'] if current_section else None,
                            'chapter': current_chapter['number'] if current_chapter else None
                        }
                        structure['paragraphs'].append(paragraph)
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                for pattern in special_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        special = {
                            'type': 'table' if '–¢–∞–±–ª–∏—Ü–∞' in line else 'figure' if '–†–∏—Å—É–Ω–æ–∫' in line else 'appendix' if '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' in line else 'other',
                            'number': match.group(1) if match.groups() else None,
                            'line': line_num,
                            'text': line
                        }
                        structure['special_structures'].append(special)
                        break
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_STRUCTURE] Error extracting document structure: {e}")
            return {'chapters': [], 'sections': [], 'paragraphs': [], 'special_structures': [], 'headers': []}
    
    def _identify_chunk_structure(self, chunk_text: str, structure: Dict[str, Any]) -> Dict[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫"""
        try:
            result = {'chapter': '', 'section': ''}
            
            if not structure or not chunk_text:
                return result
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞ –≤ —á–∞–Ω–∫–µ
            section_patterns = [
                r'(\d+\.\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+)\s+(.+)',
                r'(\d+)\s+(.+)'
            ]
            
            for pattern in section_patterns:
                match = re.search(pattern, chunk_text)
                if match:
                    section_number = match.group(1)
                    section_title = match.group(2).strip()
                    
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≥–ª–∞–≤—É
                    chapter_num = section_number.split('.')[0]
                    for chapter in structure.get('chapters', []):
                        if chapter['number'] == chapter_num:
                            result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter['title']}"
                            break
                    
                    result['section'] = f"{section_number}. {section_title}"
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª, –∏—â–µ–º –≥–ª–∞–≤—É
            if not result['section']:
                chapter_patterns = [
                    r'–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)'
                ]
                
                for pattern in chapter_patterns:
                    match = re.search(pattern, chunk_text, re.IGNORECASE)
                    if match:
                        chapter_num = match.group(1)
                        chapter_title = match.group(2).strip()
                        result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter_title}"
                        break
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [IDENTIFY_CHUNK_STRUCTURE] Error identifying chunk structure: {e}")
            return {'chapter': '', 'section': ''}
