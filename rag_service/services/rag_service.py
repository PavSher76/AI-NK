import logging
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
import psycopg2
from psycopg2.extras import RealDictCursor
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö PostgreSQL"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    def get_cursor(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        connection = psycopg2.connect(self.connection_string)
        return connection.cursor(cursor_factory=RealDictCursor)

class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG-—Å–∏—Å—Ç–µ–º–æ–π"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"
        self.POSTGRES_URL = "postgresql://norms_user:norms_password@norms-db:5432/norms_db"
        self.VECTOR_COLLECTION = "normative_documents"
        self.VECTOR_SIZE = 1024
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        self.qdrant_client = qdrant_client.QdrantClient(self.QDRANT_URL)
        self.db_manager = DatabaseManager(self.POSTGRES_URL)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self.embedding_model = None
        
        logger.info("üöÄ [RAG_SERVICE] RAG Service initialized")
    
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ BGE-M3"""
        if self.embedding_model is None:
            try:
                logger.info("ü§ñ [EMBEDDING_MODEL] Loading BGE-M3 model...")
                self.embedding_model = SentenceTransformer('BAAI/bge-m3')
                logger.info("‚úÖ [EMBEDDING_MODEL] BGE-M3 model loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå [EMBEDDING_MODEL] Failed to load BGE-M3 model: {e}")
                raise e
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BGE-M3"""
        try:
            self._load_embedding_model()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.embedding_model.encode(text, normalize_embeddings=True)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
            embedding_list = embedding.tolist()
            
            model_logger.info(f"‚úÖ [EMBEDDING] Generated embedding for text: '{text[:100]}...'")
            return embedding_list
            
        except Exception as e:
            logger.error(f"‚ùå [EMBEDDING] Error creating embedding: {e}")
            raise e
    
    def extract_document_code(self, document_title: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (–ì–û–°–¢, –°–ü, –°–ù–∏–ü –∏ —Ç.–¥.)
        """
        try:
            patterns = [
                r'–ì–û–°–¢\s+[\d\.-]+', r'–°–ü\s+[\d\.-]+', r'–°–ù–∏–ü\s+[\d\.-]+',
                r'–¢–†\s+–¢–°\s+[\d\.-]+', r'–°–¢–û\s+[\d\.-]+', r'–†–î\s+[\d\.-]+',
            ]
            for pattern in patterns:
                match = re.search(pattern, document_title, re.IGNORECASE)
                if match:
                    return match.group(0).strip()
            return ""
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] Error extracting document code: {e}")
            return ""
    
    def index_document_chunks(self, document_id: int, chunks: List[Dict[str, Any]]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        try:
            logger.info(f"üìù [INDEXING] Starting indexing for document {document_id} with {len(chunks)} chunks")
            
            points = []
            
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                content = chunk.get('content', '')
                if not content.strip():
                    continue
                
                embedding = self.create_embedding(content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                document_title = chunk.get('document_title', '')
                code = self.extract_document_code(document_title)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
                point = {
                    'id': chunk.get('id'),
                    'vector': embedding,
                    'payload': {
                        'document_id': document_id,
                        'code': code,  # –ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ì–û–°–¢, –°–ü –∏ —Ç.–¥.)
                        'title': document_title,  # –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        'section_title': chunk.get('section_title', ''),  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
                        'content': chunk.get('content', ''),
                        'chunk_type': chunk.get('chunk_type', 'paragraph'),
                        'page': chunk.get('page', 1),
                        'section': chunk.get('section', ''),
                        'metadata': chunk.get('metadata', {})
                    }
                }
                points.append(point)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
            if points:
                self.qdrant_client.upsert(
                    collection_name=self.VECTOR_COLLECTION,
                    points=points
                )
                
                logger.info(f"‚úÖ [INDEXING] Successfully indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING] No valid chunks to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING] Error indexing document {document_id}: {e}")
            return False
    
    def hybrid_search(self, query: str, k: int = 8,
                     document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None,
                     chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        try:
            logger.info(f"üîç [SEARCH] Performing hybrid search for query: '{query}' with k={k}")
            
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_results = self.vector_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] Vector search completed. Found {len(vector_results)} results.")
            
            # BM25 –ø–æ–∏—Å–∫ (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π)
            bm25_results = []
            logger.info(f"‚úÖ [SEARCH] BM25 search completed. Found {len(bm25_results)} results.")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results = self.combine_search_results(vector_results, bm25_results, k)
            logger.info(f"‚úÖ [SEARCH] Combined search results. Total {len(combined_results)} results.")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filtered_results = self.apply_search_filters(combined_results, document_filter, chapter_filter, chunk_type_filter)
            logger.info(f"‚úÖ [SEARCH] Applied filters. Final {len(filtered_results)} results.")
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"‚ùå [SEARCH] Hybrid search error: {e}")
            return []
    
    def vector_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant"""
        try:
            logger.info(f"üîç [VECTOR] Performing vector search for query: '{query}' with k={k}")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.create_embedding(query)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_start = datetime.now()
            model_logger.info(f"üîç [QDRANT_SEARCH] Searching in Qdrant collection: {self.VECTOR_COLLECTION}")
            
            results = self.qdrant_client.search(
                collection_name=self.VECTOR_COLLECTION,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,
                with_vectors=False
            )
            
            search_time = (datetime.now() - search_start).total_seconds()
            total_time = search_time
            
            logger.info(f"‚úÖ [VECTOR] Vector search completed in {total_time:.3f}s. Found {len(results)} results.")
            model_logger.info(f"‚úÖ [QDRANT_SEARCH] Qdrant search completed in {search_time:.3f}s, found {len(results)} results")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            for result in results:
                processed_result = {
                    'id': result.id,
                    'score': result.score,
                    'document_id': result.payload.get('document_id'),
                    'code': result.payload.get('code', ''),
                    'title': result.payload.get('title', ''),
                    'section_title': result.payload.get('section_title', ''),
                    'content': result.payload.get('content', ''),
                    'chunk_type': result.payload.get('chunk_type', 'paragraph'),
                    'page': result.payload.get('page', 1),
                    'section': result.payload.get('section', ''),
                    'metadata': result.payload.get('metadata', {}),
                    'search_type': 'vector'
                }
                processed_results.append(processed_result)
            
            return processed_results
            
        except Exception as e:
            total_time = (datetime.now() - datetime.now()).total_seconds()
            logger.error(f"‚ùå [VECTOR] Vector search error after {total_time:.3f}s: {type(e).__name__}: {str(e)}")
            return []
    
    def bm25_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 –ø–æ–∏—Å–∫ (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)"""
        try:
            logger.info(f"üîç [BM25] Performing BM25 search for query: '{query}' with k={k}")
            
            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å BM25 –ø–æ–∏—Å–∫
            results = []
            
            logger.info(f"‚úÖ [BM25] BM25 search completed. Found {len(results)} results.")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            for result in results:
                processed_result = {
                    'id': result.get('id'),
                    'score': result.get('score', 0),
                    'document_id': result.get('document_id'),
                    'code': result.get('code', ''),
                    'title': result.get('title', ''),
                    'section_title': result.get('section_title', ''),
                    'content': result.get('content', ''),
                    'chunk_type': result.get('chunk_type', 'paragraph'),
                    'page': result.get('page', 1),
                    'section': result.get('section', ''),
                    'metadata': result.get('metadata', {}),
                    'search_type': 'bm25'
                }
                processed_results.append(processed_result)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"‚ùå [BM25] BM25 search error: {e}")
            return []
    
    def combine_search_results(self, vector_results: List[Dict], bm25_results: List[Dict], k: int) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ BM25 –ø–æ–∏—Å–∫–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            combined_dict = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in vector_results:
                result_id = result['id']
                if result_id not in combined_dict:
                    combined_dict[result_id] = result
                    combined_dict[result_id]['combined_score'] = result['score']
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                    combined_dict[result_id]['combined_score'] = max(
                        combined_dict[result_id]['combined_score'],
                        result['score']
                    )
            
            # –î–æ–±–∞–≤–ª—è–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in bm25_results:
                result_id = result['id']
                if result_id not in combined_dict:
                    combined_dict[result_id] = result
                    combined_dict[result_id]['combined_score'] = result['score']
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                    combined_dict[result_id]['combined_score'] = max(
                        combined_dict[result_id]['combined_score'],
                        result['score']
                    )
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
            combined_results = list(combined_dict.values())
            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return combined_results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå [COMBINE] Error combining search results: {e}")
            return []
    
    def apply_search_filters(self, results: List[Dict], 
                           document_filter: Optional[str] = None,
                           chapter_filter: Optional[str] = None,
                           chunk_type_filter: Optional[str] = None) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        try:
            filtered_results = results
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
            if document_filter:
                filtered_results = [
                    result for result in filtered_results
                    if document_filter.lower() in result.get('title', '').lower()
                ]
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –≥–ª–∞–≤–µ
            if chapter_filter:
                filtered_results = [
                    result for result in filtered_results
                    if chapter_filter.lower() in result.get('section_title', '').lower()
                ]
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
            if chunk_type_filter:
                filtered_results = [
                    result for result in filtered_results
                    if chunk_type_filter.lower() == result.get('chunk_type', '').lower()
                ]
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"‚ùå [FILTER] Error applying filters: {e}")
            return results
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT id, original_filename, category, processing_status, created_at
                    FROM uploaded_documents
                    ORDER BY created_at DESC
                """)
                documents = cursor.fetchall()
                return [dict(doc) for doc in documents]
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []
    
    def get_documents_from_uploaded(self, document_type: str = 'normative') -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã uploaded_documents"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT id, original_filename, category, processing_status, created_at, 
                           file_size, COALESCE(token_count, 0) as token_count
                    FROM uploaded_documents
                    WHERE category = %s OR %s = 'all'
                    ORDER BY created_at DESC
                """, (document_type, document_type))
                documents = cursor.fetchall()
                return [dict(doc) for doc in documents]
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS_FROM_UPLOADED] Error getting documents: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_documents,
                        COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                        COUNT(CASE WHEN processing_status = 'processing' THEN 1 END) as processing_documents,
                        COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents
                    FROM uploaded_documents
                """)
                doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant
            try:
                collection_info = self.qdrant_client.get_collection(self.VECTOR_COLLECTION)
                qdrant_points = collection_info.points_count
            except Exception:
                qdrant_points = 0
            
            return {
                "documents": dict(doc_stats) if doc_stats else {},
                "qdrant": {
                    "collection": self.VECTOR_COLLECTION,
                    "points_count": qdrant_points
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            return {}
    
    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT chunk_id, content, page_number, chapter, section
                    FROM normative_chunks
                    WHERE document_id = %s
                    ORDER BY page_number, chunk_id
                """, (document_id,))
                chunks = cursor.fetchall()
                return [dict(chunk) for chunk in chunks]
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting chunks: {e}")
            return []
    
    def get_ntd_consultation(self, message: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ù–¢–î –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        try:
            logger.info(f"üí¨ [NTD_CONSULTATION] Processing consultation request: '{message[:100]}...'")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
            search_results = self.hybrid_search(message, k=5)
            
            if not search_results:
                return {
                    "status": "success",
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "timestamp": datetime.now().isoformat()
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            sources = []
            response_parts = []
            
            for result in search_results[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                source = {
                    "title": result.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç'),
                    "code": result.get('code', ''),
                    "content": result.get('content', '')[:200] + '...',
                    "page": result.get('page', 1),
                    "score": result.get('score', 0)
                }
                sources.append(source)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ –æ—Ç–≤–µ—Ç
                if result.get('code'):
                    response_parts.append(f"üìÑ **{result['code']}** - {result.get('title', '')}")
                else:
                    response_parts.append(f"üìÑ {result.get('title', '')}")
                
                response_parts.append(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {result.get('content', '')[:300]}...")
                response_parts.append("")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            if response_parts:
                response = "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤–æ—Ç —á—Ç–æ —è –Ω–∞—à–µ–ª:\n\n" + "\n".join(response_parts)
                response += f"\n\n–ù–∞–π–¥–µ–Ω–æ {len(sources)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
            else:
                response = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
            
            return {
                "status": "success",
                "response": response,
                "sources": sources,
                "confidence": min(0.9, max(0.1, search_results[0].get('score', 0.5))),
                "documents_used": len(sources),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error processing consultation: {e}")
            return {
                "status": "error",
                "response": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
                "sources": [],
                "confidence": 0.0,
                "documents_used": 0,
                "timestamp": datetime.now().isoformat()
            }
