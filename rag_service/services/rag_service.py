import logging
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
import psycopg2
from psycopg2.extras import RealDictCursor
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö PostgreSQL"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    def get_cursor(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        connection = psycopg2.connect(self.connection_string)
        return connection.cursor(cursor_factory=RealDictCursor)

class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG-—Å–∏—Å—Ç–µ–º–æ–π"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"
        self.POSTGRES_URL = "postgresql://norms_user:norms_password@norms-db:5432/norms_db"
        self.VECTOR_COLLECTION = "normative_documents"
        self.VECTOR_SIZE = 1024
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        self.qdrant_client = qdrant_client.QdrantClient(self.QDRANT_URL)
        self.db_manager = DatabaseManager(self.POSTGRES_URL)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self.embedding_model = None
        
        logger.info("üöÄ [RAG_SERVICE] RAG Service initialized")
    
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ BGE-M3"""
        if self.embedding_model is None:
            try:
                logger.info("ü§ñ [EMBEDDING_MODEL] Loading BGE-M3 model...")
                self.embedding_model = SentenceTransformer('BAAI/bge-m3')
                logger.info("‚úÖ [EMBEDDING_MODEL] BGE-M3 model loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå [EMBEDDING_MODEL] Failed to load BGE-M3 model: {e}")
                raise e
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BGE-M3"""
        try:
            self._load_embedding_model()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.embedding_model.encode(text, normalize_embeddings=True)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
            embedding_list = embedding.tolist()
            
            model_logger.info(f"‚úÖ [EMBEDDING] Generated embedding for text: '{text[:100]}...'")
            return embedding_list
            
        except Exception as e:
            logger.error(f"‚ùå [EMBEDDING] Error creating embedding: {e}")
            raise e
    
    def extract_document_code(self, document_title: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (–ì–û–°–¢, –°–ü, –°–ù–∏–ü –∏ —Ç.–¥.)
        """
        try:
            patterns = [
                r'–ì–û–°–¢\s+[\d\.-]+', r'–°–ü\s+[\d\.-]+', r'–°–ù–∏–ü\s+[\d\.-]+',
                r'–¢–†\s+–¢–°\s+[\d\.-]+', r'–°–¢–û\s+[\d\.-]+', r'–†–î\s+[\d\.-]+',
            ]
            for pattern in patterns:
                match = re.search(pattern, document_title, re.IGNORECASE)
                if match:
                    return match.group(0).strip()
            return ""
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] Error extracting document code: {e}")
            return ""
    
    def index_document_chunks(self, document_id: int, chunks: List[Dict[str, Any]]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        try:
            logger.info(f"üìù [INDEXING] Starting indexing for document {document_id} with {len(chunks)} chunks")
            
            points = []
            
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                content = chunk.get('content', '')
                if not content.strip():
                    continue
                
                embedding = self.create_embedding(content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                document_title = chunk.get('document_title', '')
                code = self.extract_document_code(document_title)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
                point = {
                    'id': chunk.get('id'),
                    'vector': embedding,
                    'payload': {
                        'document_id': document_id,
                        'code': code,  # –ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ì–û–°–¢, –°–ü –∏ —Ç.–¥.)
                        'title': document_title,  # –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        'section_title': chunk.get('section_title', ''),  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
                        'content': chunk.get('content', ''),
                        'chunk_type': chunk.get('chunk_type', 'paragraph'),
                        'page': chunk.get('page', 1),
                        'section': chunk.get('section', ''),
                        'metadata': chunk.get('metadata', {})
                    }
                }
                points.append(point)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
            if points:
                self.qdrant_client.upsert(
                    collection_name=self.VECTOR_COLLECTION,
                    points=points
                )
                
                logger.info(f"‚úÖ [INDEXING] Successfully indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING] No valid chunks to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING] Error indexing document {document_id}: {e}")
            return False
    
    def hybrid_search(self, query: str, k: int = 8,
                     document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None,
                     chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        try:
            logger.info(f"üîç [SEARCH] Performing hybrid search for query: '{query}' with k={k}")
            
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_results = self.vector_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] Vector search completed. Found {len(vector_results)} results.")
            
            # BM25 –ø–æ–∏—Å–∫ (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π)
            bm25_results = []
            logger.info(f"‚úÖ [SEARCH] BM25 search completed. Found {len(bm25_results)} results.")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results = self.combine_search_results(vector_results, bm25_results, k)
            logger.info(f"‚úÖ [SEARCH] Combined search results. Total {len(combined_results)} results.")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filtered_results = self.apply_search_filters(combined_results, document_filter, chapter_filter, chunk_type_filter)
            logger.info(f"‚úÖ [SEARCH] Applied filters. Final {len(filtered_results)} results.")
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"‚ùå [SEARCH] Hybrid search error: {e}")
            return []
    
    def vector_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant"""
        try:
            logger.info(f"üîç [VECTOR] Performing vector search for query: '{query}' with k={k}")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.create_embedding(query)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_start = datetime.now()
            model_logger.info(f"üîç [QDRANT_SEARCH] Searching in Qdrant collection: {self.VECTOR_COLLECTION}")
            
            results = self.qdrant_client.search(
                collection_name=self.VECTOR_COLLECTION,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,
                with_vectors=False
            )
            
            search_time = (datetime.now() - search_start).total_seconds()
            total_time = search_time
            
            logger.info(f"‚úÖ [VECTOR] Vector search completed in {total_time:.3f}s. Found {len(results)} results.")
            model_logger.info(f"‚úÖ [QDRANT_SEARCH] Qdrant search completed in {search_time:.3f}s, found {len(results)} results")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            for result in results:
                processed_result = {
                    'id': result.id,
                    'score': result.score,
                    'document_id': result.payload.get('document_id'),
                    'code': result.payload.get('code', ''),
                    'title': result.payload.get('title', ''),
                    'section_title': result.payload.get('section_title', ''),
                    'content': result.payload.get('content', ''),
                    'chunk_type': result.payload.get('chunk_type', 'paragraph'),
                    'page': result.payload.get('page', 1),
                    'section': result.payload.get('section', ''),
                    'metadata': result.payload.get('metadata', {}),
                    'search_type': 'vector'
                }
                processed_results.append(processed_result)
            
            return processed_results
            
        except Exception as e:
            total_time = (datetime.now() - datetime.now()).total_seconds()
            logger.error(f"‚ùå [VECTOR] Vector search error after {total_time:.3f}s: {type(e).__name__}: {str(e)}")
            return []
    
    def bm25_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 –ø–æ–∏—Å–∫ (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)"""
        try:
            logger.info(f"üîç [BM25] Performing BM25 search for query: '{query}' with k={k}")
            
            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å BM25 –ø–æ–∏—Å–∫
            results = []
            
            logger.info(f"‚úÖ [BM25] BM25 search completed. Found {len(results)} results.")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            for result in results:
                processed_result = {
                    'id': result.get('id'),
                    'score': result.get('score', 0),
                    'document_id': result.get('document_id'),
                    'code': result.get('code', ''),
                    'title': result.get('title', ''),
                    'section_title': result.get('section_title', ''),
                    'content': result.get('content', ''),
                    'chunk_type': result.get('chunk_type', 'paragraph'),
                    'page': result.get('page', 1),
                    'section': result.get('section', ''),
                    'metadata': result.get('metadata', {}),
                    'search_type': 'bm25'
                }
                processed_results.append(processed_result)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"‚ùå [BM25] BM25 search error: {e}")
            return []
    
    def combine_search_results(self, vector_results: List[Dict], bm25_results: List[Dict], k: int) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ BM25 –ø–æ–∏—Å–∫–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            combined_dict = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in vector_results:
                result_id = result['id']
                if result_id not in combined_dict:
                    combined_dict[result_id] = result
                    combined_dict[result_id]['combined_score'] = result['score']
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                    combined_dict[result_id]['combined_score'] = max(
                        combined_dict[result_id]['combined_score'],
                        result['score']
                    )
            
            # –î–æ–±–∞–≤–ª—è–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in bm25_results:
                result_id = result['id']
                if result_id not in combined_dict:
                    combined_dict[result_id] = result
                    combined_dict[result_id]['combined_score'] = result['score']
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                    combined_dict[result_id]['combined_score'] = max(
                        combined_dict[result_id]['combined_score'],
                        result['score']
                    )
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
            combined_results = list(combined_dict.values())
            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return combined_results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå [COMBINE] Error combining search results: {e}")
            return []
    
    def apply_search_filters(self, results: List[Dict], 
                           document_filter: Optional[str] = None,
                           chapter_filter: Optional[str] = None,
                           chunk_type_filter: Optional[str] = None) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        try:
            filtered_results = results
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
            if document_filter:
                filtered_results = [
                    result for result in filtered_results
                    if document_filter.lower() in result.get('title', '').lower()
                ]
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –≥–ª–∞–≤–µ
            if chapter_filter:
                filtered_results = [
                    result for result in filtered_results
                    if chapter_filter.lower() in result.get('section_title', '').lower()
                ]
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
            if chunk_type_filter:
                filtered_results = [
                    result for result in filtered_results
                    if chunk_type_filter.lower() == result.get('chunk_type', '').lower()
                ]
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"‚ùå [FILTER] Error applying filters: {e}")
            return results
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT id, original_filename, category, processing_status, created_at
                    FROM uploaded_documents
                    ORDER BY created_at DESC
                """)
                documents = cursor.fetchall()
                return [dict(doc) for doc in documents]
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_documents,
                        COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed_documents,
                        COUNT(CASE WHEN processing_status = 'processing' THEN 1 END) as processing_documents,
                        COUNT(CASE WHEN processing_status = 'error' THEN 1 END) as error_documents
                    FROM uploaded_documents
                """)
                doc_stats = cursor.fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant
            try:
                collection_info = self.qdrant_client.get_collection(self.VECTOR_COLLECTION)
                qdrant_points = collection_info.points_count
            except Exception:
                qdrant_points = 0
            
            return {
                "documents": dict(doc_stats) if doc_stats else {},
                "qdrant": {
                    "collection": self.VECTOR_COLLECTION,
                    "points_count": qdrant_points
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            return {}
    
    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT chunk_id, content, page_number, chapter, section
                    FROM normative_chunks
                    WHERE document_id = %s
                    ORDER BY page_number, chunk_id
                """, (document_id,))
                chunks = cursor.fetchall()
                return [dict(chunk) for chunk in chunks]
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting chunks: {e}")
            return []
