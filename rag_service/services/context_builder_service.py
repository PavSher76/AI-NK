import logging
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
import os
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

@dataclass
class ContextCandidate:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    doc: str  # –ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ì–û–°–¢, –°–ü –∏ —Ç.–¥.)
    section: str  # –†–∞–∑–¥–µ–ª/–≥–ª–∞–≤–∞
    page: int  # –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    snippet: str  # –§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞
    why: str  # –ü—Ä–∏—á–∏–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (scope match, terms, etc.)
    score: float  # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    content: str  # –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–∞
    chunk_id: str  # ID —á–∞–Ω–∫–∞
    document_title: str  # –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    section_title: str  # –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞
    chunk_type: str  # –¢–∏–ø —á–∞–Ω–∫–∞
    metadata: Dict[str, Any]  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

@dataclass
class ContextSummary:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Å–≤–æ–¥–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    topic: str  # –û —á–µ–º —Ä–∞–∑–¥–µ–ª
    norm_type: str  # –¢–∏–ø –Ω–æ—Ä–º—ã (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è/—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è)
    key_points: List[str]  # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
    relevance_reason: str  # –ü—Ä–∏—á–∏–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

class ContextBuilderService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self, ollama_url: str = None):
        self.ollama_url = ollama_url or OLLAMA_URL
        self.model_name = "llama3.2:3b"  # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–æ–∫
        logger.info(f"üèóÔ∏è [CONTEXT_BUILDER] Initialized with {self.model_name} at {self.ollama_url}")
    
    def build_structured_context(self, search_results: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Args:
            search_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞-—Å–≤–æ–¥–∫–æ–π
        """
        try:
            logger.info(f"üèóÔ∏è [CONTEXT_BUILDER] Building structured context for {len(search_results)} results")
            
            # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            candidates = self._convert_to_candidates(search_results)
            
            # 2. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–ª–∏—è–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤
            deduplicated_candidates = self._deduplicate_and_merge(candidates)
            
            # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º auto-summary –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            enriched_candidates = self._generate_summaries(deduplicated_candidates, query)
            
            # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            structured_context = self._build_final_context(enriched_candidates, query)
            
            logger.info(f"‚úÖ [CONTEXT_BUILDER] Structured context built with {len(enriched_candidates)} candidates")
            return structured_context
            
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT_BUILDER] Error building structured context: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –∫–æ–Ω—Ç–µ–∫—Å—Ç
            return self._build_fallback_context(search_results, query)
    
    def _convert_to_candidates(self, search_results: List[Dict[str, Any]]) -> List[ContextCandidate]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        candidates = []
        
        for result in search_results:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            why = self._determine_relevance_reason(result)
            
            candidate = ContextCandidate(
                doc=result.get('code', ''),
                section=result.get('section', ''),
                page=result.get('page', 1),
                snippet=result.get('content', '')[:200] + '...' if len(result.get('content', '')) > 200 else result.get('content', ''),
                why=why,
                score=result.get('score', 0.0),
                content=result.get('content', ''),
                chunk_id=result.get('chunk_id', ''),
                document_title=result.get('document_title', ''),
                section_title=result.get('section_title', ''),
                chunk_type=result.get('chunk_type', ''),
                metadata=result.get('metadata', {})
            )
            candidates.append(candidate)
        
        return candidates
    
    def _determine_relevance_reason(self, result: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        search_type = result.get('search_type', '')
        score = result.get('score', 0.0)
        
        if search_type == 'exact_match':
            return "exact_match"
        elif search_type == 'semantic':
            return "semantic_match"
        elif search_type == 'keyword':
            return "keyword_match"
        elif score > 0.8:
            return "high_relevance"
        elif score > 0.6:
            return "medium_relevance"
        else:
            return "low_relevance"
    
    def _deduplicate_and_merge(self, candidates: List[ContextCandidate]) -> List[ContextCandidate]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ doc+section –∏ —Å–ª–∏—è–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤"""
        logger.info(f"üîÑ [CONTEXT_BUILDER] Deduplicating {len(candidates)} candidates")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ doc+section
        grouped = {}
        for candidate in candidates:
            key = f"{candidate.doc}_{candidate.section}"
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(candidate)
        
        # –°–ª–∏–≤–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
        merged_candidates = []
        for key, group in grouped.items():
            if len(group) == 1:
                merged_candidates.append(group[0])
            else:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ page –∏ —Å–ª–∏–≤–∞–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ
                group.sort(key=lambda x: x.page)
                merged = self._merge_adjacent_chunks(group)
                merged_candidates.extend(merged)
        
        logger.info(f"‚úÖ [CONTEXT_BUILDER] Deduplication completed: {len(candidates)} -> {len(merged_candidates)}")
        return merged_candidates
    
    def _merge_adjacent_chunks(self, chunks: List[ContextCandidate]) -> List[ContextCandidate]:
        """–°–ª–∏—è–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –æ–¥–Ω–æ–π —Å–µ–∫—Ü–∏–∏"""
        if len(chunks) <= 1:
            return chunks
        
        merged = []
        current_chunk = chunks[0]
        
        for next_chunk in chunks[1:]:
            # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Å–µ–¥–Ω–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞ <= 2), —Å–ª–∏–≤–∞–µ–º
            if abs(next_chunk.page - current_chunk.page) <= 2:
                # –°–ª–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                current_chunk.content += f"\n\n{next_chunk.content}"
                current_chunk.snippet = current_chunk.content[:200] + '...' if len(current_chunk.content) > 200 else current_chunk.content
                # –û–±–Ω–æ–≤–ª—è–µ–º score (–±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π)
                current_chunk.score = max(current_chunk.score, next_chunk.score)
                # –û–±–Ω–æ–≤–ª—è–µ–º why
                if next_chunk.score > current_chunk.score:
                    current_chunk.why = next_chunk.why
            else:
                # –ù–µ —Å–æ—Å–µ–¥–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                merged.append(current_chunk)
                current_chunk = next_chunk
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        merged.append(current_chunk)
        
        return merged
    
    def _generate_summaries(self, candidates: List[ContextCandidate], query: str) -> List[ContextCandidate]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è auto-summary –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        logger.info(f"üìù [CONTEXT_BUILDER] Generating summaries for {len(candidates)} candidates")
        
        enriched_candidates = []
        for candidate in candidates:
            try:
                summary = self._generate_candidate_summary(candidate, query)
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—É
                candidate.summary = summary
                enriched_candidates.append(candidate)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [CONTEXT_BUILDER] Failed to generate summary for {candidate.doc}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –±–µ–∑ —Å–≤–æ–¥–∫–∏
                candidate.summary = None
                enriched_candidates.append(candidate)
        
        return enriched_candidates
    
    def _generate_candidate_summary(self, candidate: ContextCandidate, query: str) -> Optional[ContextSummary]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏
            prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É (5-7 —Å—Ç—Ä–æ–∫):

–î–æ–∫—É–º–µ–Ω—Ç: {candidate.doc} - {candidate.document_title}
–†–∞–∑–¥–µ–ª: {candidate.section} - {candidate.section_title}
–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:
{candidate.content[:1000]}

–°–æ–∑–¥–∞–π —Å–≤–æ–¥–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
–¢–ï–ú–ê: [–æ —á–µ–º —Ä–∞–∑–¥–µ–ª –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö]
–¢–ò–ü_–ù–û–†–ú–´: [–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è/—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è/–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è]
–ö–õ–Æ–ß–ï–í–´–ï_–ú–û–ú–ï–ù–¢–´: [3-4 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π]
–ü–†–ò–ß–ò–ù–ê_–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò: [–ø–æ—á–µ–º—É —ç—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –∑–∞–ø—Ä–æ—Å—É]
"""

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.0,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
                        "top_p": 0.9,
                        "max_tokens": 200
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                summary_text = result.get('response', '').strip()
                
                # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
                return self._parse_summary_response(summary_text)
            else:
                logger.warning(f"‚ö†Ô∏è [CONTEXT_BUILDER] Ollama request failed: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT_BUILDER] Error generating summary: {e}")
            return None
    
    def _parse_summary_response(self, summary_text: str) -> Optional[ContextSummary]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ —Å —Å–≤–æ–¥–∫–æ–π"""
        try:
            lines = summary_text.split('\n')
            topic = ""
            norm_type = ""
            key_points = []
            relevance_reason = ""
            
            for line in lines:
                line = line.strip()
                if line.startswith('–¢–ï–ú–ê:'):
                    topic = line.replace('–¢–ï–ú–ê:', '').strip()
                elif line.startswith('–¢–ò–ü_–ù–û–†–ú–´:'):
                    norm_type = line.replace('–¢–ò–ü_–ù–û–†–ú–´:', '').strip()
                elif line.startswith('–ö–õ–Æ–ß–ï–í–´–ï_–ú–û–ú–ï–ù–¢–´:'):
                    points_text = line.replace('–ö–õ–Æ–ß–ï–í–´–ï_–ú–û–ú–ï–ù–¢–´:', '').strip()
                    key_points = [p.strip() for p in points_text.split(';') if p.strip()]
                elif line.startswith('–ü–†–ò–ß–ò–ù–ê_–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò:'):
                    relevance_reason = line.replace('–ü–†–ò–ß–ò–ù–ê_–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò:', '').strip()
            
            return ContextSummary(
                topic=topic or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–º—É",
                norm_type=norm_type or "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π",
                key_points=key_points or [],
                relevance_reason=relevance_reason or "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
            )
            
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT_BUILDER] Error parsing summary: {e}")
            return None
    
    def _build_final_context(self, candidates: List[ContextCandidate], query: str) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º JSON-–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤
        context_array = []
        for candidate in candidates:
            context_obj = {
                "doc": candidate.doc,
                "section": candidate.section,
                "page": candidate.page,
                "snippet": candidate.snippet,
                "why": candidate.why,
                "score": round(candidate.score, 3),
                "document_title": candidate.document_title,
                "section_title": candidate.section_title,
                "chunk_type": candidate.chunk_type,
                "metadata": candidate.metadata
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(candidate, 'summary') and candidate.summary:
                context_obj["summary"] = {
                    "topic": candidate.summary.topic,
                    "norm_type": candidate.summary.norm_type,
                    "key_points": candidate.summary.key_points,
                    "relevance_reason": candidate.summary.relevance_reason
                }
            
            context_array.append(context_obj)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-—Å–≤–æ–¥–∫—É –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
        meta_summary = self._generate_meta_summary(context_array, query)
        
        return {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "context": context_array,
            "meta_summary": meta_summary,
            "total_candidates": len(context_array),
            "avg_score": round(sum(c["score"] for c in context_array) / len(context_array), 3) if context_array else 0
        }
    
    def _generate_meta_summary(self, context_array: List[Dict], query: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞-—Å–≤–æ–¥–∫–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è"""
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏
            docs = list(set(c["doc"] for c in context_array if c["doc"]))
            sections = list(set(c["section"] for c in context_array if c["section"]))
            avg_score = sum(c["score"] for c in context_array) / len(context_array) if context_array else 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
            query_lower = query.lower()
            if any(word in query_lower for word in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ']):
                query_type = "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"
            elif any(word in query_lower for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
                query_type = "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
            elif any(word in query_lower for word in ['–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–æ–∑–Ω–∞—á–∞–µ—Ç']):
                query_type = "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"
            else:
                query_type = "–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
            
            return {
                "query_type": query_type,
                "documents_found": len(docs),
                "sections_covered": len(sections),
                "avg_relevance": round(avg_score, 3),
                "coverage_quality": "–≤—ã—Å–æ–∫–∞—è" if avg_score > 0.7 else "—Å—Ä–µ–¥–Ω—è—è" if avg_score > 0.5 else "–Ω–∏–∑–∫–∞—è",
                "key_documents": docs[:3],  # –¢–æ–ø-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                "key_sections": sections[:3]  # –¢–æ–ø-3 —Ä–∞–∑–¥–µ–ª–∞
            }
            
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT_BUILDER] Error generating meta summary: {e}")
            return {
                "query_type": "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π",
                "documents_found": 0,
                "sections_covered": 0,
                "avg_relevance": 0.0,
                "coverage_quality": "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "key_documents": [],
                "key_sections": []
            }
    
    def _build_fallback_context(self, search_results: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
        """Fallback –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏"""
        logger.warning("‚ö†Ô∏è [CONTEXT_BUILDER] Using fallback context")
        
        context_array = []
        for result in search_results[:5]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-5
            context_obj = {
                "doc": result.get('code', ''),
                "section": result.get('section', ''),
                "page": result.get('page', 1),
                "snippet": result.get('content', '')[:200] + '...' if len(result.get('content', '')) > 200 else result.get('content', ''),
                "why": "fallback",
                "score": result.get('score', 0.0),
                "document_title": result.get('document_title', ''),
                "section_title": result.get('section_title', ''),
                "chunk_type": result.get('chunk_type', ''),
                "metadata": result.get('metadata', {})
            }
            context_array.append(context_obj)
        
        return {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "context": context_array,
            "meta_summary": {
                "query_type": "fallback",
                "documents_found": len(context_array),
                "sections_covered": 0,
                "avg_relevance": 0.0,
                "coverage_quality": "fallback",
                "key_documents": [],
                "key_sections": []
            },
            "total_candidates": len(context_array),
            "avg_score": 0.0
        }
