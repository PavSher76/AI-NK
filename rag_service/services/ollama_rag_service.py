import logging
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct
import psycopg2
from psycopg2.extras import RealDictCursor
import re
import requests
import json
import os
from qdrant_client.models import Filter, FieldCondition, MatchAny

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

class OllamaEmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —á–µ—Ä–µ–∑ Ollama BGE-M3"""
    
    def __init__(self, ollama_url: str = None):
        self.ollama_url = ollama_url or OLLAMA_URL
        self.model_name = "bge-m3"
        logger.info(f"ü§ñ [OLLAMA_EMBEDDING] Initialized with {self.model_name} at {self.ollama_url}")
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            payload = {
                "model": self.model_name,
                "prompt": text,
                "options": {
                    "embedding_only": True
                }
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                embedding = result.get("embedding", [])
                
                if embedding:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                    embedding_array = np.array(embedding)
                    normalized_embedding = embedding_array / np.linalg.norm(embedding_array)
                    
                    model_logger.info(f"‚úÖ [EMBEDDING] Generated embedding for text: '{text[:100]}...'")
                    return normalized_embedding.tolist()
                else:
                    raise ValueError("Empty embedding received from Ollama")
            else:
                raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå [EMBEDDING] Error creating embedding: {e}")
            raise e

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö PostgreSQL"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    def get_cursor(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        connection = psycopg2.connect(self.connection_string)
        return connection.cursor(cursor_factory=RealDictCursor)

class OllamaRAGService:
    """RAG —Å–µ—Ä–≤–∏—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"  # Qdrant –≤ Docker
        self.POSTGRES_URL = "postgresql://norms_user:norms_password@norms-db:5432/norms_db"  # –ë–î –≤ Docker
        self.VECTOR_COLLECTION = "normative_documents"
        self.VECTOR_SIZE = 1024  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ BGE-M3
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        self.qdrant_client = qdrant_client.QdrantClient(self.QDRANT_URL)
        self.db_manager = DatabaseManager(self.POSTGRES_URL)
        self.embedding_service = OllamaEmbeddingService()
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self._ensure_collection_exists()
        
        logger.info("üöÄ [OLLAMA_RAG_SERVICE] Ollama RAG Service initialized")
    
    def _ensure_collection_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
            collections = self.qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.VECTOR_COLLECTION not in collection_names:
                logger.info(f"üìù [COLLECTION] Creating collection '{self.VECTOR_COLLECTION}'...")
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
                self.qdrant_client.create_collection(
                    collection_name=self.VECTOR_COLLECTION,
                    vectors_config=qdrant_client.models.VectorParams(
                        size=self.VECTOR_SIZE,
                        distance=qdrant_client.models.Distance.COSINE
                    )
                )
                
                logger.info(f"‚úÖ [COLLECTION] Collection '{self.VECTOR_COLLECTION}' created successfully")
            else:
                logger.info(f"‚úÖ [COLLECTION] Collection '{self.VECTOR_COLLECTION}' already exists")
                
        except Exception as e:
            logger.error(f"‚ùå [COLLECTION] Error ensuring collection exists: {e}")
            raise e
    
    def extract_document_code(self, document_title: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (–ì–û–°–¢, –°–ü, –°–ù–∏–ü –∏ —Ç.–¥.)
        """
        try:
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            title_without_ext = re.sub(r'\.(pdf|txt|doc|docx)$', '', document_title, flags=re.IGNORECASE)
            
            patterns = [
                r'–ì–û–°–¢\s+[\d\.-]+', 
                r'–°–ü\s+[\d\.-]+', 
                r'–°–ù–∏–ü\s+[\d\.-]+',
                r'–¢–†\s+–¢–°\s+[\d\.-]+', 
                r'–°–¢–û\s+[\d\.-]+', 
                r'–†–î\s+[\d\.-]+',
                r'–¢–£\s+[\d\.-]+',
                r'–ü–ë\s+[\d\.-]+',
                r'–ù–ü–ë\s+[\d\.-]+',
                r'–°–ü–±\s+[\d\.-]+',
                r'–ú–ì–°–ù\s+[\d\.-]+'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, title_without_ext, re.IGNORECASE)
                if match:
                    code = match.group(0).strip()
                    logger.info(f"üîç [CODE_EXTRACTION] Extracted code '{code}' from title '{document_title}'")
                    return code
            
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] No code pattern found in title: '{document_title}'")
            return ""
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] Error extracting document code: {e}")
            return ""
    
    def index_document_chunks(self, document_id: int, chunks: List[Dict[str, Any]]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        try:
            logger.info(f"üìù [INDEXING] Starting indexing for document {document_id} with {len(chunks)} chunks")
            
            points = []
            
            for chunk in chunks:
                try:
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                    content = chunk.get('content', '')
                    if not content.strip():
                        continue
                    
                    embedding = self.embedding_service.create_embedding(content)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤–æ–π ID –¥–ª—è Qdrant
                    qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                    if qdrant_id < 0:
                        qdrant_id = abs(qdrant_id)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    document_title = chunk.get('document_title', '')
                    code = self.extract_document_code(document_title)
                    
                    logger.info(f"üîç [INDEXING] Document title: '{document_title}', extracted code: '{code}'")
                    
                    # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
                    point = PointStruct(
                        id=qdrant_id,
                        vector=embedding,
                        payload={
                            'document_id': document_id,
                            'chunk_id': chunk['chunk_id'],
                            'code': code,
                            'title': document_title,
                            'section_title': chunk.get('section_title', ''),
                            'content': content,
                            'chunk_type': chunk.get('chunk_type', ''),
                            'page': chunk.get('page', 1),
                            'section': chunk.get('section', ''),
                            'metadata': chunk.get('metadata', {})
                        }
                    )
                    points.append(point)
                    
                except Exception as e:
                    logger.error(f"‚ùå [INDEXING] Error processing chunk {chunk.get('chunk_id', 'unknown')}: {e}")
                    continue
            
            if points:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
                self.qdrant_client.upsert(
                    collection_name=self.VECTOR_COLLECTION,
                    points=points
                )
                logger.info(f"‚úÖ [INDEXING] Successfully indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING] No valid chunks to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING] Error indexing document {document_id}: {e}")
            return False
    
    def hybrid_search(self, query: str, k: int = 8, document_filter: Optional[str] = None, 
                     chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        try:
            logger.info(f"üîç [HYBRID_SEARCH] Performing hybrid search for query: '{query}' with k={k}")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_service.create_embedding(query)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            must_conditions = []
            
            if document_filter and document_filter != 'all':
                must_conditions.append({
                    "key": "code",
                    "match": {"value": document_filter}
                })
            
            if chapter_filter:
                must_conditions.append({
                    "key": "section",
                    "match": {"value": chapter_filter}
                })
            
            if chunk_type_filter:
                must_conditions.append({
                    "key": "chunk_type",
                    "match": {"value": chunk_type_filter}
                })
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_result = self.qdrant_client.search(
                collection_name=self.VECTOR_COLLECTION,
                query_vector=query_embedding,
                query_filter={"must": must_conditions} if must_conditions else None,
                limit=k,
                with_payload=True,
                with_vectors=False
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for point in search_result:
                result = {
                    'id': point.id,
                    'score': point.score,
                    'document_id': point.payload.get('document_id'),
                    'chunk_id': point.payload.get('chunk_id'),
                    'code': point.payload.get('code'),
                    'document_title': point.payload.get('title'),
                    'section_title': point.payload.get('section_title'),
                    'content': point.payload.get('content'),
                    'chunk_type': point.payload.get('chunk_type'),
                    'page': point.payload.get('page'),
                    'section': point.payload.get('section'),
                    'metadata': point.payload.get('metadata', {})
                }
                results.append(result)
            
            logger.info(f"‚úÖ [HYBRID_SEARCH] Found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_SEARCH] Error during hybrid search: {e}")
            raise e
    
    def get_ntd_consultation(self, message: str, user_id: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ø–æ –ù–¢–î"""
        try:
            logger.info(f"üí¨ [NTD_CONSULTATION] Processing consultation request: '{message[:100]}...'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            document_code = self.extract_document_code_from_query(message)
            logger.info(f"üîç [NTD_CONSULTATION] Extracted document code: {document_code}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
            search_results = self.hybrid_search(message, k=10)
            
            if not search_results:
                return {
                    "status": "success",
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "timestamp": datetime.now().isoformat()
                }
            
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ–≥–æ –Ω–∞–ª–∏—á–∏–µ
            if document_code:
                # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –∫–æ–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                exact_match = None
                for result in search_results:
                    if result.get('code') == document_code:
                        exact_match = result
                        break
                
                if exact_match:
                    logger.info(f"‚úÖ [NTD_CONSULTATION] Found exact match for {document_code}")
                    top_result = exact_match
                    confidence = 1.0  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                else:
                    logger.warning(f"‚ö†Ô∏è [NTD_CONSULTATION] Document {document_code} not found in system")
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    return {
                        "status": "warning",
                        "response": f"‚ö†Ô∏è **–í–Ω–∏–º–∞–Ω–∏–µ!** –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç **{document_code}** –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ.\n\n"
                                  f"–í–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n"
                                  f"**{search_results[0]['document_title']}**\n"
                                  f"–†–∞–∑–¥–µ–ª: {search_results[0]['section']}\n\n"
                                  f"{search_results[0]['content'][:500]}...\n\n"
                                  f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç {document_code} –≤ —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.",
                        "sources": [{
                            'document_code': search_results[0]['code'],
                            'document_title': search_results[0]['document_title'],
                            'section': search_results[0]['section'],
                            'page': search_results[0]['page'],
                            'content_preview': search_results[0]['content'][:200] + "..." if len(search_results[0]['content']) > 200 else search_results[0]['content'],
                            'relevance_score': search_results[0]['score'],
                            'note': '–î–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É, –Ω–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–º'
                        }],
                        "confidence": 0.5,
                        "documents_used": 1,
                        "missing_document": document_code,
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                # –ï—Å–ª–∏ –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
                top_result = search_results[0]
                confidence = min(top_result['score'], 1.0) if top_result['score'] > 0 else 0.0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = []
            for result in search_results[:3]:  # –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                source = {
                    'document_code': result['code'],
                    'document_title': result['document_title'],
                    'section': result['section'],
                    'page': result['page'],
                    'content_preview': result['content'][:200] + "..." if len(result['content']) > 200 else result['content'],
                    'relevance_score': result['score']
                }
                sources.append(source)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤–æ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å:\n\n"
            response += f"**{top_result['document_title']}**\n"
            response += f"–†–∞–∑–¥–µ–ª: {top_result['section']}\n\n"
            response += f"{top_result['content']}\n\n"
            response += f"–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞."
            
            return {
                "status": "success",
                "response": response,
                "sources": sources,
                "confidence": confidence,
                "documents_used": len(search_results),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error during consultation: {e}")
            return {
                "status": "error",
                "response": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "documents_used": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    def extract_document_code_from_query(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–¥–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            patterns = [
                r'–°–ü\s+(\d+\.\d+\.\d+)',  # –°–ü 22.13330.2016
                r'–°–ù–∏–ü\s+(\d+\.\d+\.\d+)',  # –°–ù–∏–ü 2.01.01-82
                r'–ì–û–°–¢\s+(\d+\.\d+\.\d+)',  # –ì–û–°–¢ 27751-2014
                r'–¢–£\s+(\d+\.\d+\.\d+)',   # –¢–£ 3812-001-12345678-2016
                r'–ü–ë\s+(\d+\.\d+\.\d+)',   # –ü–ë 03-428-02
                r'–ù–ü–ë\s+(\d+\.\d+\.\d+)',  # –ù–ü–ë 5-2000
                r'–°–ü–±\s+(\d+\.\d+\.\d+)',  # –°–ü–± 70.13330.2012
                r'–ú–ì–°–ù\s+(\d+\.\d+\.\d+)'  # –ú–ì–°–ù 4.19-2005
            ]
            
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    if '–°–ü' in pattern:
                        return f"–°–ü {match.group(1)}"
                    elif '–°–ù–∏–ü' in pattern:
                        return f"–°–ù–∏–ü {match.group(1)}"
                    elif '–ì–û–°–¢' in pattern:
                        return f"–ì–û–°–¢ {match.group(1)}"
                    elif '–¢–£' in pattern:
                        return f"–¢–£ {match.group(1)}"
                    elif '–ü–ë' in pattern:
                        return f"–ü–ë {match.group(1)}"
                    elif '–ù–ü–ë' in pattern:
                        return f"–ù–ü–ë {match.group(1)}"
                    elif '–°–ü–±' in pattern:
                        return f"–°–ü–± {match.group(1)}"
                    elif '–ú–ì–°–ù' in pattern:
                        return f"–ú–ì–°–ù {match.group(1)}"
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå [DOCUMENT_CODE_EXTRACTION] Error extracting document code: {e}")
            return None
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                           ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                           COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                    FROM uploaded_documents ud
                    LEFT JOIN (
                        SELECT document_id, COUNT(*) as chunks_count 
                        FROM normative_chunks 
                        GROUP BY document_id
                    ) chunk_counts ON ud.id = chunk_counts.document_id
                    ORDER BY ud.upload_date DESC
                """)
                documents = cursor.fetchall()
                
                result = []
                for doc in documents:
                    result.append({
                        'id': doc['id'],
                        'title': doc['original_filename'],
                        'original_filename': doc['original_filename'],
                        'filename': doc['original_filename'],
                        'category': doc['category'],
                        'status': doc['processing_status'],
                        'processing_status': doc['processing_status'],
                        'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                        'file_size': doc['file_size'],
                        'token_count': doc['token_count'],
                        'vector_indexed': doc['processing_status'] == 'completed',
                        'chunks_count': doc['chunks_count']
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []
    
    def get_documents_from_uploaded(self, document_type: str = 'all') -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã uploaded_documents"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                           ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                           COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                    FROM uploaded_documents ud
                    LEFT JOIN (
                        SELECT document_id, COUNT(*) as chunks_count 
                        FROM normative_chunks 
                        GROUP BY document_id
                    ) chunk_counts ON ud.id = chunk_counts.document_id
                    WHERE ud.category = %s OR %s = 'all'
                    ORDER BY ud.upload_date DESC
                """, (document_type, document_type))
                documents = cursor.fetchall()
                
                result = []
                for doc in documents:
                    result.append({
                        'id': doc['id'],
                        'title': doc['original_filename'],
                        'original_filename': doc['original_filename'],
                        'filename': doc['original_filename'],
                        'category': doc['category'],
                        'status': doc['processing_status'],
                        'processing_status': doc['processing_status'],
                        'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                        'file_size': doc['file_size'],
                        'token_count': doc['token_count'],
                        'vector_indexed': doc['processing_status'] == 'completed',
                        'chunks_count': doc['chunks_count']
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS_FROM_UPLOADED] Error getting documents: {e}")
            return []
    
    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("""
                    SELECT original_filename 
                    FROM uploaded_documents 
                    WHERE id = %s
                """, (document_id,))
                document_result = cursor.fetchone()
                document_title = document_result['original_filename'] if document_result else f"Document_{document_id}"
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("""
                    SELECT chunk_id, content, chapter as section_title, chunk_type, page_number as page, section
                    FROM normative_chunks
                    WHERE document_id = %s
                    ORDER BY page_number, chunk_id
                """, (document_id,))
                chunks = cursor.fetchall()
                
                result = []
                for chunk in chunks:
                    result.append({
                        'chunk_id': chunk['chunk_id'],
                        'content': chunk['content'],
                        'section_title': chunk['section_title'],
                        'chunk_type': chunk['chunk_type'],
                        'page': chunk['page'],
                        'section': chunk['section'],
                        'document_title': document_title  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    })
                
                logger.info(f"üìã [GET_DOCUMENT_CHUNKS] Retrieved {len(result)} chunks for document {document_id} ({document_title})")
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting chunks for document {document_id}: {e}")
            return []
    
    def delete_document_indexes(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Qdrant"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_INDEXES] Deleting indexes for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.get_document_chunks(document_id)
            if not chunks:
                logger.warning(f"‚ö†Ô∏è [DELETE_INDEXES] No chunks found for document {document_id}")
                return True
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ Qdrant
            point_ids = []
            for chunk in chunks:
                qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                if qdrant_id < 0:
                    qdrant_id = abs(qdrant_id)
                point_ids.append(qdrant_id)
            
            # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
            if point_ids:
                self.qdrant_client.delete(
                    collection_name=self.VECTOR_COLLECTION,
                    points_selector=point_ids
                )
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted {len(point_ids)} points from Qdrant for document {document_id}")
            
            # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ –∏–∑ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks WHERE document_id = %s", (document_id,))
                deleted_chunks = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted {deleted_chunks} chunks from PostgreSQL for document {document_id}")
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_INDEXES] Error deleting indexes for document {document_id}: {e}")
            return False
    
    def delete_document(self, document_id: int) -> bool:
        """–ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_DOCUMENT] Deleting document {document_id}")
            
            # 1. –£–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ Qdrant
            indexes_deleted = self.delete_document_indexes(document_id)
            
            # 2. –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            with self.db_manager.get_cursor() as cursor:
                # –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                deleted_elements = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_elements} extracted elements for document {document_id}")
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM uploaded_documents WHERE id = %s", (document_id,))
                deleted_documents = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_documents} documents for document {document_id}")
                
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            if deleted_documents == 0:
                logger.warning(f"‚ö†Ô∏è [DELETE_DOCUMENT] Document {document_id} not found")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_DOCUMENT] Error deleting document {document_id}: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å
            import requests
            qdrant_response = requests.get(f"{self.QDRANT_URL}/collections/{self.VECTOR_COLLECTION}", timeout=5)
            qdrant_stats = {
                'collection_name': self.VECTOR_COLLECTION,
                'vectors_count': 0,
                'indexed_vectors': 0,
                'status': 'unknown'
            }
            
            if qdrant_response.status_code == 200:
                qdrant_data = qdrant_response.json()
                result = qdrant_data.get('result', {})
                qdrant_stats.update({
                    'vectors_count': result.get('points_count', 0),
                    'indexed_vectors': result.get('indexed_vectors_count', 0),
                    'status': result.get('status', 'unknown')
                })
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("SELECT COUNT(*) as total_documents FROM uploaded_documents")
                total_docs = cursor.fetchone()['total_documents']
                
                cursor.execute("SELECT COUNT(*) as total_chunks FROM normative_chunks")
                total_chunks = cursor.fetchone()['total_chunks']
                
                cursor.execute("SELECT COUNT(*) as pending_docs FROM uploaded_documents WHERE processing_status = 'pending'")
                pending_docs = cursor.fetchone()['pending_docs']
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                cursor.execute("SELECT COALESCE(SUM(token_count), 0) as total_tokens FROM uploaded_documents")
                total_tokens = cursor.fetchone()['total_tokens']
            
            db_stats = {
                'total_documents': total_docs,
                'total_chunks': total_chunks,
                'pending_documents': pending_docs,
                'total_tokens': total_tokens
            }
            
            return {
                'qdrant': qdrant_stats,
                'postgresql': db_stats,
                'embedding_model': 'bge-m3 (Ollama)',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            try:
                with self.db_manager.get_cursor() as cursor:
                    cursor.execute("SELECT COUNT(*) as total_documents FROM uploaded_documents")
                    total_docs = cursor.fetchone()['total_documents']
                    
                    cursor.execute("SELECT COUNT(*) as total_chunks FROM normative_chunks")
                    total_chunks = cursor.fetchone()['total_chunks']
                    
                    cursor.execute("SELECT COALESCE(SUM(token_count), 0) as total_tokens FROM uploaded_documents")
                    total_tokens = cursor.fetchone()['total_tokens']
                
                return {
                    'qdrant': {
                        'collection_name': self.VECTOR_COLLECTION,
                        'vectors_count': 0,  # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Qdrant
                        'indexed_vectors': 0,
                        'status': 'error'
                    },
                    'postgresql': {
                        'total_documents': total_docs,
                        'total_chunks': total_chunks,
                        'pending_documents': 0,
                        'total_tokens': total_tokens
                    },
                    'embedding_model': 'bge-m3 (Ollama)',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
            except Exception as fallback_error:
                logger.error(f"‚ùå [GET_STATS] Fallback error: {fallback_error}")
                return {
                    'error': f"Primary error: {str(e)}, Fallback error: {str(fallback_error)}",
                    'timestamp': datetime.now().isoformat()
                }

    def save_document_to_db(self, document_id: int, filename: str, original_filename: str, 
                           file_type: str, file_size: int, document_hash: str, 
                           category: str, document_type: str) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.db_manager.get_cursor() as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ö–µ—à–µ–º
                cursor.execute("""
                    SELECT id FROM uploaded_documents 
                    WHERE document_hash = %s
                """, (document_hash,))
                
                if cursor.fetchone():
                    raise Exception("Document with this content already exists")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                cursor.execute("""
                    INSERT INTO uploaded_documents 
                    (id, filename, original_filename, file_type, file_size, document_hash, 
                     category, document_type, processing_status)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, 'pending')
                    RETURNING id
                """, (
                    document_id,
                    filename,
                    original_filename,
                    file_type,
                    file_size,
                    document_hash,
                    category,
                    document_type
                ))
                
                saved_id = cursor.fetchone()['id']
                cursor.connection.commit()
                logger.info(f"‚úÖ [SAVE_DOCUMENT] Document saved with ID: {saved_id}")
                return saved_id
                
        except Exception as e:
            logger.error(f"‚ùå [SAVE_DOCUMENT] Error saving document: {e}")
            raise

    def update_document_status(self, document_id: int, status: str, error_message: str = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                if error_message:
                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET processing_status = %s, processing_error = %s
                        WHERE id = %s
                    """, (status, error_message, document_id))
                else:
                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET processing_status = %s, processing_error = NULL
                        WHERE id = %s
                    """, (status, document_id))
                
                cursor.connection.commit()
                logger.info(f"‚úÖ [UPDATE_STATUS] Document {document_id} status updated to: {status}")
                
        except Exception as e:
            logger.error(f"‚ùå [UPDATE_STATUS] Error updating document {document_id} status: {e}")

    async def process_document_async(self, document_id: int, content: bytes, filename: str) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üîÑ [PROCESS_ASYNC] Starting processing for document {document_id}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            text_content = await self.extract_text_from_document(content, filename)
            if not text_content:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to extract text from document {document_id}")
                return False
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.create_chunks(text_content, document_id, filename)
            if not chunks:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to create chunks for document {document_id}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            success = await self.index_chunks_async(chunks, document_id)
            if not success:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to index chunks for document {document_id}")
                return False
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            token_count = len(text_content.split())
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET token_count = %s
                    WHERE id = %s
                """, (token_count, document_id))
                cursor.connection.commit()
            
            logger.info(f"‚úÖ [PROCESS_ASYNC] Document {document_id} processed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [PROCESS_ASYNC] Error processing document {document_id}: {e}")
            return False

    async def extract_text_from_document(self, content: bytes, filename: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            import tempfile
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{filename.split('.')[-1]}") as temp_file:
                temp_file.write(content)
                temp_file_path = temp_file.name
            
            try:
                if filename.lower().endswith('.pdf'):
                    return await self.extract_text_from_pdf(temp_file_path)
                elif filename.lower().endswith('.docx'):
                    return await self.extract_text_from_docx(temp_file_path)
                elif filename.lower().endswith('.txt'):
                    return content.decode('utf-8', errors='ignore')
                else:
                    logger.error(f"‚ùå [EXTRACT_TEXT] Unsupported file type: {filename}")
                    return ""
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                    
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_TEXT] Error extracting text: {e}")
            return ""

    async def extract_text_from_pdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        try:
            import PyPDF2
            
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PDF] Error extracting text from PDF: {e}")
            return ""

    async def extract_text_from_docx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX"""
        try:
            from docx import Document
            
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCX] Error extracting text from DOCX: {e}")
            return ""

    def create_chunks(self, text: str, document_id: int, filename: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü"""
        try:
            logger.info(f"üìù [CREATE_CHUNKS] Creating chunks for document {document_id}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º "–°—Ç—Ä–∞–Ω–∏—Ü–∞ X –∏–∑ Y"
            page_pattern = r'–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+(\d+)\s+–∏–∑\s+(\d+)'
            page_matches = list(re.finditer(page_pattern, text))
            
            chunks = []
            chunk_id = 1
            
            if page_matches:
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –Ω–∏–º
                logger.info(f"üìÑ [CREATE_CHUNKS] Found {len(page_matches)} page markers in document")
                
                for i, match in enumerate(page_matches):
                    page_num = int(match.group(1))
                    start_pos = match.end()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞)
                    if i + 1 < len(page_matches):
                        end_pos = page_matches[i + 1].start()
                    else:
                        end_pos = len(text)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text = text[start_pos:end_pos].strip()
                    
                    if page_text:
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —á–∞–Ω–∫–∏
                        page_chunks = self._split_page_into_chunks(page_text, chunk_size=1000)
                        
                        for chunk_text in page_chunks:
                            chunks.append({
                                'chunk_id': f"doc_{document_id}_page_{page_num}_chunk_{chunk_id}",
                                'document_id': document_id,
                                'document_title': filename,
                                'content': chunk_text.strip(),
                                'chunk_type': 'paragraph',
                                'page': page_num
                            })
                            chunk_id += 1
            else:
                # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                logger.info(f"üìÑ [CREATE_CHUNKS] No page markers found, treating as single page document")
                page_chunks = self._split_page_into_chunks(text, chunk_size=1000)
                
                for chunk_text in page_chunks:
                    chunks.append({
                        'chunk_id': f"doc_{document_id}_page_1_chunk_{chunk_id}",
                        'document_id': document_id,
                        'document_title': filename,
                        'content': chunk_text.strip(),
                        'chunk_type': 'paragraph',
                        'page': 1
                    })
                    chunk_id += 1
            
            logger.info(f"‚úÖ [CREATE_CHUNKS] Created {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS] Error creating chunks: {e}")
            return []
    
    def _split_page_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence + ". "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    async def index_chunks_async(self, chunks: List[Dict[str, Any]], document_id: int) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                for chunk in chunks:
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, document_id, document_title, chunk_type, content, page_number)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                    """, (
                        chunk['chunk_id'],
                        chunk['chunk_id'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk_id –∫–∞–∫ clause_id
                        chunk['document_id'],
                        chunk['document_title'],
                        chunk['chunk_type'],
                        chunk['content'],
                        chunk.get('page', 1)  # –î–æ–±–∞–≤–ª—è–µ–º page_number
                    ))
                cursor.connection.commit()
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = self.embedding_service.create_embedding(chunk['content'])
                if embedding is None:
                    logger.warning(f"‚ö†Ô∏è [INDEX_CHUNKS] Failed to create embedding for chunk {chunk['chunk_id']}")
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
                point_id = hash(chunk['chunk_id']) % (2**63 - 1)
                if point_id < 0:
                    point_id = abs(point_id)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ —Å–ø–∏—Å–æ–∫
                if hasattr(embedding, 'tolist'):
                    vector = embedding.tolist()
                else:
                    vector = list(embedding)
                
                point = PointStruct(
                    id=point_id,
                    vector=vector,
                    payload={
                        'chunk_id': chunk['chunk_id'],
                        'document_id': chunk['document_id'],
                        'document_title': chunk['document_title'],
                        'content': chunk['content'],
                        'chunk_type': chunk['chunk_type'],
                        'page': chunk.get('page', 1)  # –î–æ–±–∞–≤–ª—è–µ–º page –≤ payload
                    }
                )
                
                self.qdrant_client.upsert(
                    collection_name=self.VECTOR_COLLECTION,
                    points=[point]
                )
            
            logger.info(f"‚úÖ [INDEX_CHUNKS] Indexed {len(chunks)} chunks for document {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX_CHUNKS] Error indexing chunks: {e}")
            return False

    def clear_collection(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant"""
        try:
            logger.info("üßπ [CLEAR_COLLECTION] Clearing entire collection...")
            
            # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.qdrant_client.delete(
                collection_name=self.VECTOR_COLLECTION,
                points_selector=Filter(
                    must=[
                        FieldCondition(
                            key="document_id",
                            match=MatchAny(any=list(range(1, 100000000)))  # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–æ—á–∫–∏
                        )
                    ]
                )
            )
            
            logger.info("‚úÖ [CLEAR_COLLECTION] Collection cleared successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [CLEAR_COLLECTION] Error clearing collection: {e}")
            return False
