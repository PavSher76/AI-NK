import logging
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import re
import requests
import json
import os
from .qdrant_service import QdrantService
from .reranker_service import BGERerankerService
from .bge_reranker_service import BGERankingService
from .hybrid_search_service import HybridSearchService
from .mmr_service import MMRService
from .intent_classifier_service import IntentClassifierService
from .context_builder_service import ContextBuilderService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
model_logger = logging.getLogger("model")

# –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

class OllamaEmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —á–µ—Ä–µ–∑ Ollama BGE-M3"""
    
    def __init__(self, ollama_url: str = None):
        self.ollama_url = ollama_url or OLLAMA_URL
        self.model_name = "bge-m3"
        logger.info(f"ü§ñ [OLLAMA_EMBEDDING] Initialized with {self.model_name} at {self.ollama_url}")
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            payload = {
                "model": self.model_name,
                "prompt": text,
                "options": {
                    "embedding_only": True
                }
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                embedding = result.get("embedding", [])
                
                if embedding:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                    embedding_array = np.array(embedding)
                    normalized_embedding = embedding_array / np.linalg.norm(embedding_array)
                    
                    model_logger.info(f"‚úÖ [EMBEDDING] Generated embedding for text: '{text[:100]}...'")
                    return normalized_embedding.tolist()
                else:
                    raise ValueError("Empty embedding received from Ollama")
            else:
                raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå [EMBEDDING] Error creating embedding: {e}")
            raise e

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö PostgreSQL"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.connection = None
    
    def get_connection(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        if not self.connection or self.connection.closed:
            self.connection = psycopg2.connect(self.connection_string)
        return self.connection
    
    def get_cursor(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        connection = psycopg2.connect(self.connection_string)
        return connection.cursor(cursor_factory=RealDictCursor)

class OllamaRAGService:
    """RAG —Å–µ—Ä–≤–∏—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama BGE-M3 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.QDRANT_URL = "http://qdrant:6333"  # Qdrant –≤ Docker
        self.POSTGRES_URL = "postgresql://norms_user:norms_password@norms-db:5432/norms_db"  # –ë–î –≤ Docker
        self.VECTOR_COLLECTION = "normative_documents"
        self.VECTOR_SIZE = 1024  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ BGE-M3
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        self.qdrant_service = QdrantService(self.QDRANT_URL, self.VECTOR_COLLECTION, self.VECTOR_SIZE)
        self.db_manager = DatabaseManager(self.POSTGRES_URL)
        self.embedding_service = OllamaEmbeddingService()
        self.reranker_service = BGERerankerService()  # –°—Ç–∞—Ä—ã–π —Ä–µ—Ä–∞–Ω–∫–µ—Ä –¥–ª—è fallback
        self.bge_reranker_service = BGERankingService()  # –ù–æ–≤—ã–π BGE —Ä–µ—Ä–∞–Ω–∫–µ—Ä
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.hybrid_search_service = HybridSearchService(
            db_connection=self.db_manager.get_connection(),
            embedding_service=self.embedding_service,
            qdrant_service=self.qdrant_service,
            alpha=0.6,  # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –¥–ª—è dense –ø–æ–∏—Å–∫–∞
            use_rrf=True,
            rrf_k=60
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MMR —Å–µ—Ä–≤–∏—Å–∞
        self.mmr_service = MMRService(
            lambda_param=0.7,  # –ë–∞–ª–∞–Ω—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            similarity_threshold=0.8
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        self.intent_classifier = IntentClassifierService()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.context_builder = ContextBuilderService()
        
        logger.info("üöÄ [OLLAMA_RAG_SERVICE] Ollama RAG Service initialized with hybrid search and structured context")
    
    def get_structured_context(self, query: str, k: int = 8, document_filter: Optional[str] = None, 
                              chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None,
                              use_reranker: bool = True, fast_mode: bool = False, use_mmr: bool = True,
                              use_intent_classification: bool = True) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            document_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chapter_filter: –§–∏–ª—å—Ç—Ä –ø–æ –≥–ª–∞–≤–µ
            chunk_type_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
            use_reranker: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
            fast_mode: –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º
            use_mmr: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ MMR
            use_intent_classification: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞–º–µ—Ä–µ–Ω–∏–π
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞-—Å–≤–æ–¥–∫–æ–π
        """
        try:
            logger.info(f"üèóÔ∏è [STRUCTURED_CONTEXT] Building structured context for query: '{query[:50]}...'")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            search_results = self.hybrid_search(
                query=query,
                k=k,
                document_filter=document_filter,
                chapter_filter=chapter_filter,
                chunk_type_filter=chunk_type_filter,
                use_reranker=use_reranker,
                fast_mode=fast_mode,
                use_mmr=use_mmr,
                use_intent_classification=use_intent_classification
            )
            
            if not search_results:
                logger.warning(f"‚ö†Ô∏è [STRUCTURED_CONTEXT] No search results found for query: '{query}'")
                return {
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "context": [],
                    "meta_summary": {
                        "query_type": "no_results",
                        "documents_found": 0,
                        "sections_covered": 0,
                        "avg_relevance": 0.0,
                        "coverage_quality": "–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                        "key_documents": [],
                        "key_sections": []
                    },
                    "total_candidates": 0,
                    "avg_score": 0.0
                }
            
            # –°—Ç—Ä–æ–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            structured_context = self.context_builder.build_structured_context(search_results, query)
            
            logger.info(f"‚úÖ [STRUCTURED_CONTEXT] Structured context built with {structured_context['total_candidates']} candidates")
            return structured_context
            
        except Exception as e:
            logger.error(f"‚ùå [STRUCTURED_CONTEXT] Error building structured context: {e}")
            return {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "context": [],
                "meta_summary": {
                    "query_type": "error",
                    "documents_found": 0,
                    "sections_covered": 0,
                    "avg_relevance": 0.0,
                    "coverage_quality": "–æ—à–∏–±–∫–∞",
                    "key_documents": [],
                    "key_sections": []
                },
                "total_candidates": 0,
                "avg_score": 0.0,
                "error": str(e)
            }

    
    def extract_document_code(self, document_title: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (–ì–û–°–¢, –°–ü, –°–ù–∏–ü –∏ —Ç.–¥.)
        """
        try:
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            title_without_ext = re.sub(r'\.(pdf|txt|doc|docx)$', '', document_title, flags=re.IGNORECASE)
            
            patterns = [
                r'–ì–û–°–¢\s+[\d\.-]+', 
                r'–°–ü\s+[\d\.-]+', 
                r'–°–ù–∏–ü\s+[\d\.-]+',
                r'–¢–†\s+–¢–°\s+[\d\.-]+', 
                r'–°–¢–û\s+[\d\.-]+', 
                r'–†–î\s+[\d\.-]+',
                r'–¢–£\s+[\d\.-]+',
                r'–ü–ë\s+[\d\.-]+',
                r'–ù–ü–ë\s+[\d\.-]+',
                r'–°–ü–±\s+[\d\.-]+',
                r'–ú–ì–°–ù\s+[\d\.-]+'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, title_without_ext, re.IGNORECASE)
                if match:
                    code = match.group(0).strip()
                    logger.info(f"üîç [CODE_EXTRACTION] Extracted code '{code}' from title '{document_title}'")
                    return code
            
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] No code pattern found in title: '{document_title}'")
            return ""
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [CODE_EXTRACTION] Error extracting document code: {e}")
            return ""
    
    def index_document_chunks(self, document_id: int, chunks: List[Dict[str, Any]]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        try:
            logger.info(f"üìù [INDEXING] Starting indexing for document {document_id} with {len(chunks)} chunks")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîç [INDEXING] Getting metadata for document_id: {document_id}")
            document_metadata = self._get_document_metadata(document_id)
            logger.info(f"üîç [INDEXING] Retrieved metadata: {document_metadata}")
            
            points = []
            
            for chunk in chunks:
                try:
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                    content = chunk.get('content', '')
                    if not content.strip():
                        continue
                    
                    embedding = self.embedding_service.create_embedding(content)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤–æ–π ID –¥–ª—è Qdrant
                    qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                    if qdrant_id < 0:
                        qdrant_id = abs(qdrant_id)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    document_title = chunk.get('document_title', '')
                    code = self.extract_document_code(document_title)
                    
                    logger.info(f"üîç [INDEXING] Document title: '{document_title}', extracted code: '{code}'")
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                    chunk_metadata = self._create_chunk_metadata(chunk, document_metadata)
                    
                    # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
                    point = self.qdrant_service.create_point(
                        point_id=qdrant_id,
                        vector=embedding,
                        payload={
                            'document_id': document_id,
                            'chunk_id': chunk['chunk_id'],
                            'code': code,
                            'title': document_title,
                            'section_title': chunk.get('section_title', ''),
                            'content': content,
                            'chunk_type': chunk.get('chunk_type', ''),
                            'page': chunk.get('page', 1),
                            'section': chunk.get('section', ''),
                            'metadata': chunk_metadata
                        }
                    )
                    points.append(point)
                    
                except Exception as e:
                    logger.error(f"‚ùå [INDEXING] Error processing chunk {chunk.get('chunk_id', 'unknown')}: {e}")
                    continue
            
            if points:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
                self.qdrant_service.upsert_points_batch(points)
                logger.info(f"‚úÖ [INDEXING] Successfully indexed {len(points)} chunks for document {document_id}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [INDEXING] No valid chunks to index for document {document_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [INDEXING] Error indexing document {document_id}: {e}")
            return False
    
    def hybrid_search(self, query: str, k: int = 8, document_filter: Optional[str] = None, 
                     chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None, 
                     use_reranker: bool = True, fast_mode: bool = False, use_mmr: bool = True, 
                     use_intent_classification: bool = True) -> List[Dict[str, Any]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            document_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chapter_filter: –§–∏–ª—å—Ç—Ä –ø–æ –≥–ª–∞–≤–µ
            chunk_type_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
            use_reranker: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
            fast_mode: –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º (–æ—Ç–∫–ª—é—á–∞–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
            use_mmr: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ MMR –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
            use_intent_classification: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞–º–µ—Ä–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        """
        try:
            logger.info(f"üîç [HYBRID_SEARCH] Performing advanced hybrid search for query: '{query}' with k={k}")
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            intent_classification = None
            query_rewriting = None
            enhanced_queries = [query]
            enhanced_filters = {
                'document_filter': document_filter,
                'chapter_filter': chapter_filter,
                'chunk_type_filter': chunk_type_filter
            }
            
            if use_intent_classification and not fast_mode:
                try:
                    logger.info(f"üéØ [HYBRID_SEARCH] Classifying intent for query: '{query[:50]}...'")
                    intent_classification = self.intent_classifier.classify_intent(query)
                    query_rewriting = self.intent_classifier.rewrite_query(query, intent_classification)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                    enhanced_queries = query_rewriting.rewritten_queries
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
                    if query_rewriting.section_filters:
                        enhanced_filters['chapter_filter'] = query_rewriting.section_filters[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∏–ª—å—Ç—Ä
                    if query_rewriting.chunk_type_filters:
                        enhanced_filters['chunk_type_filter'] = query_rewriting.chunk_type_filters[0]
                    
                    logger.info(f"‚úÖ [HYBRID_SEARCH] Intent classified as: {intent_classification.intent_type.value} "
                              f"(confidence: {intent_classification.confidence:.3f})")
                    logger.info(f"üîÑ [HYBRID_SEARCH] Generated {len(enhanced_queries)} enhanced queries")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [HYBRID_SEARCH] Intent classification failed: {e}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            # –ò—â–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –∏ MMR
            search_k = 50 if use_reranker else (k * 2 if use_mmr else k)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –ª—É—á—à–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
            best_query = enhanced_queries[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π (–ª—É—á—à–∏–π) –∑–∞–ø—Ä–æ—Å
            search_results = self.hybrid_search_service.search(
                query=best_query,
                k=search_k,
                document_filter=enhanced_filters['document_filter'],
                chapter_filter=enhanced_filters['chapter_filter'],
                chunk_type_filter=enhanced_filters['chunk_type_filter'],
                use_alpha_blending=True,
                use_rrf=True
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º SearchResult –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
            results = []
            for result in search_results:
                formatted_result = {
                    'id': result.id,
                    'score': result.score,
                    'document_id': result.document_id,
                    'chunk_id': result.chunk_id,
                    'code': result.code,
                    'document_title': result.document_title,
                    'section_title': result.section_title,
                    'content': result.content,
                    'chunk_type': result.chunk_type,
                    'page': result.page,
                    'section': result.section,
                    'metadata': result.metadata,
                    'search_type': result.search_type,
                    'rank': result.rank
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏
                if intent_classification:
                    formatted_result['intent_type'] = intent_classification.intent_type.value
                    formatted_result['intent_confidence'] = intent_classification.confidence
                    formatted_result['intent_keywords'] = intent_classification.keywords
                    formatted_result['intent_reasoning'] = intent_classification.reasoning
                
                if query_rewriting:
                    formatted_result['enhanced_queries'] = query_rewriting.rewritten_queries
                    formatted_result['section_filters'] = query_rewriting.section_filters
                    formatted_result['chunk_type_filters'] = query_rewriting.chunk_type_filters
                
                results.append(formatted_result)
            
            logger.info(f"‚úÖ [HYBRID_SEARCH] Found {len(results)} hybrid results")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ –Ω–µ –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º
            if use_reranker and not fast_mode and len(results) > k:
                logger.info(f"üîÑ [HYBRID_SEARCH] Applying BGE reranking to {len(results)} results ‚Üí {k} final results")
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π BGE —Ä–µ—Ä–∞–Ω–∫–µ—Ä —Å fallback
                    reranked_results = self.bge_reranker_service.rerank_with_fallback(
                        query=query,
                        search_results=results,
                        top_k=k,
                        initial_top_k=len(results)
                    )
                    
                    if reranked_results:
                        logger.info(f"‚úÖ [HYBRID_SEARCH] BGE reranking completed successfully")
                        final_results = reranked_results
                    else:
                        logger.warning("‚ö†Ô∏è [HYBRID_SEARCH] BGE reranking failed, trying fallback")
                        # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É —Ä–µ—Ä–∞–Ω–∫–µ—Ä—É
                        reranked_results = self.reranker_service.rerank_search_results(
                            query=query,
                            search_results=results,
                            top_k=k,
                            initial_top_k=len(results)
                        )
                        if reranked_results:
                            logger.info(f"‚úÖ [HYBRID_SEARCH] Fallback reranking completed")
                            final_results = reranked_results
                        else:
                            logger.warning("‚ö†Ô∏è [HYBRID_SEARCH] All reranking failed, using original results")
                            final_results = results[:k]
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º MMR –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    if use_mmr and not fast_mode and len(final_results) > k:
                        logger.info(f"üîÑ [HYBRID_SEARCH] Applying MMR diversification to {len(final_results)} results ‚Üí {k}")
                        try:
                            mmr_results = self.mmr_service.diversify_results(
                                results=final_results,
                                k=k,
                                query=query,
                                use_semantic_similarity=True
                            )
                            
                            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º MMR —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
                            diversified_results = []
                            for mmr_result in mmr_results:
                                formatted_result = {
                                    'id': mmr_result.id,
                                    'score': mmr_result.mmr_score,
                                    'document_id': mmr_result.document_id,
                                    'chunk_id': mmr_result.chunk_id,
                                    'code': mmr_result.code,
                                    'document_title': mmr_result.document_title,
                                    'section_title': mmr_result.section_title,
                                    'content': mmr_result.content,
                                    'chunk_type': mmr_result.chunk_type,
                                    'page': mmr_result.page,
                                    'section': mmr_result.section,
                                    'metadata': mmr_result.metadata,
                                    'search_type': mmr_result.search_type,
                                    'rank': mmr_result.rank,
                                    'mmr_score': mmr_result.mmr_score,
                                    'relevance_score': mmr_result.relevance_score,
                                    'diversity_score': mmr_result.diversity_score
                                }
                                diversified_results.append(formatted_result)
                            
                            logger.info(f"‚úÖ [HYBRID_SEARCH] MMR diversification completed")
                            return diversified_results
                            
                        except Exception as e:
                            logger.error(f"‚ùå [HYBRID_SEARCH] Error during MMR diversification: {e}")
                            logger.info("üîÑ [HYBRID_SEARCH] Falling back to reranked results")
                            return final_results[:k]
                    else:
                        return final_results[:k]
                        
                except Exception as e:
                    logger.error(f"‚ùå [HYBRID_SEARCH] Error during BGE reranking: {e}")
                    logger.info("üîÑ [HYBRID_SEARCH] Falling back to original results")
                    return results[:k]
            else:
                # –ï—Å–ª–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞–ª–æ
                if fast_mode:
                    logger.info(f"‚ö° [HYBRID_SEARCH] Fast mode: returning top {k} results without reranking")
                return results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_SEARCH] Error during hybrid search: {e}")
            # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –º–µ—Ç–æ–¥—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return self._fallback_hybrid_search(query, k, document_filter, chapter_filter, chunk_type_filter)
    
    def _fallback_hybrid_search(self, query: str, k: int, document_filter: Optional[str] = None, 
                               chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """Fallback –º–µ—Ç–æ–¥ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (—Å—Ç–∞—Ä–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)"""
        try:
            logger.info(f"üîÑ [FALLBACK] Using fallback hybrid search for query: '{query}'")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_service.create_embedding(query)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            must_conditions = []
            
            if document_filter and document_filter != 'all':
                must_conditions.append({
                    "key": "code",
                    "match": {"value": document_filter}
                })
            
            if chapter_filter:
                must_conditions.append({
                    "key": "section",
                    "match": {"value": chapter_filter}
                })
            
            if chunk_type_filter:
                must_conditions.append({
                    "key": "chunk_type",
                    "match": {"value": chunk_type_filter}
                })
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_result = self.qdrant_service.search_similar(
                query_vector=query_embedding,
                limit=k,
                filters={"must": must_conditions} if must_conditions else None
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for point in search_result:
                result = {
                    'id': point['id'],
                    'score': point['score'],
                    'document_id': point['payload'].get('document_id'),
                    'chunk_id': point['payload'].get('chunk_id'),
                    'code': point['payload'].get('code'),
                    'document_title': point['payload'].get('title'),
                    'section_title': point['payload'].get('section_title'),
                    'content': point['payload'].get('content'),
                    'chunk_type': point['payload'].get('chunk_type'),
                    'page': point['payload'].get('page'),
                    'section': point['payload'].get('section'),
                    'metadata': point['payload'].get('metadata', {}),
                    'search_type': 'fallback'
                }
                results.append(result)
            
            logger.info(f"‚úÖ [FALLBACK] Found {len(results)} fallback results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå [FALLBACK] Error during fallback search: {e}")
            return []
    
    def get_hybrid_search_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            stats = self.hybrid_search_service.get_search_stats()
            bge_reranker_stats = self.bge_reranker_service.get_reranking_stats()
            mmr_stats = self.mmr_service.get_mmr_stats()
            intent_classifier_stats = self.intent_classifier.get_intent_stats()
            
            return {
                "status": "success",
                "stats": stats,
                "bge_reranker": bge_reranker_stats,
                "mmr": mmr_stats,
                "intent_classifier": intent_classifier_stats,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå [HYBRID_STATS] Error getting hybrid search stats: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def get_ntd_consultation(self, message: str, user_id: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ø–æ –ù–¢–î —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            logger.info(f"üí¨ [NTD_CONSULTATION] Processing consultation request: '{message[:100]}...'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            document_code = self.extract_document_code_from_query(message)
            logger.info(f"üîç [NTD_CONSULTATION] Extracted document code: {document_code}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            structured_context = self.get_structured_context(message, k=10)
            
            if not structured_context.get('context'):
                return {
                    "status": "success",
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "structured_context": structured_context,
                    "timestamp": datetime.now().isoformat()
                }
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            search_results = []
            for context_item in structured_context['context']:
                search_results.append({
                    'code': context_item['doc'],
                    'document_title': context_item['document_title'],
                    'section': context_item['section'],
                    'page': context_item['page'],
                    'content': context_item.get('snippet', ''),
                    'score': context_item['score'],
                    'chunk_type': context_item.get('chunk_type', ''),
                    'metadata': context_item.get('metadata', {})
                })
            
            if not search_results:
                return {
                    "status": "success",
                    "response": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                    "sources": [],
                    "confidence": 0.0,
                    "documents_used": 0,
                    "timestamp": datetime.now().isoformat()
                }
            
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ–≥–æ –Ω–∞–ª–∏—á–∏–µ
            if document_code:
                # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –∫–æ–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                exact_match = None
                for result in search_results:
                    if result.get('code') == document_code:
                        exact_match = result
                        break
                
                if exact_match:
                    logger.info(f"‚úÖ [NTD_CONSULTATION] Found exact match for {document_code}")
                    top_result = exact_match
                    confidence = 1.0  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                else:
                    logger.warning(f"‚ö†Ô∏è [NTD_CONSULTATION] Document {document_code} not found in system")
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    return {
                        "status": "warning",
                        "response": f"‚ö†Ô∏è **–í–Ω–∏–º–∞–Ω–∏–µ!** –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç **{document_code}** –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ.\n\n"
                                  f"–í–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n"
                                  f"**{search_results[0]['document_title']}**\n"
                                  f"–†–∞–∑–¥–µ–ª: {search_results[0]['section']}\n\n"
                                  f"{search_results[0]['content'][:500]}...\n\n"
                                  f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç {document_code} –≤ —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.",
                        "sources": [{
                            'document_code': search_results[0]['code'],
                            'document_title': search_results[0]['document_title'],
                            'section': search_results[0]['section'],
                            'page': search_results[0]['page'],
                            'content_preview': search_results[0]['content'][:200] + "..." if len(search_results[0]['content']) > 200 else search_results[0]['content'],
                            'relevance_score': search_results[0]['score'],
                            'note': '–î–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É, –Ω–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–º'
                        }],
                        "confidence": 0.5,
                        "documents_used": 1,
                        "missing_document": document_code,
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                # –ï—Å–ª–∏ –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
                top_result = search_results[0]
                confidence = min(top_result['score'], 1.0) if top_result['score'] > 0 else 0.0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            sources = []
            for result in search_results[:3]:  # –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                source = {
                    'title': result['document_title'],
                    'filename': result['document_title'],
                    'page': result.get('page', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                    'section': result.get('section', '–ù–µ —É–∫–∞–∑–∞–Ω'),
                    'document_code': result.get('code', ''),
                    'content_preview': result['content'][:200] + "..." if len(result['content']) > 200 else result['content'],
                    'relevance_score': result['score']
                }
                sources.append(source)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            response = self._format_consultation_response_with_context(message, structured_context, top_result)
            
            return {
                "status": "success",
                "response": response,
                "sources": sources,
                "confidence": confidence,
                "documents_used": len(search_results),
                "structured_context": structured_context,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [NTD_CONSULTATION] Error during consultation: {e}")
            return {
                "status": "error",
                "response": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "documents_used": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    def extract_document_code_from_query(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–¥–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            patterns = [
                r'–°–ü\s+(\d+\.\d+\.\d+)',  # –°–ü 22.13330.2016
                r'–°–ù–∏–ü\s+(\d+\.\d+\.\d+)',  # –°–ù–∏–ü 2.01.01-82
                r'–ì–û–°–¢\s+(\d+\.\d+\.\d+)',  # –ì–û–°–¢ 27751-2014
                r'–¢–£\s+(\d+\.\d+\.\d+)',   # –¢–£ 3812-001-12345678-2016
                r'–ü–ë\s+(\d+\.\d+\.\d+)',   # –ü–ë 03-428-02
                r'–ù–ü–ë\s+(\d+\.\d+\.\d+)',  # –ù–ü–ë 5-2000
                r'–°–ü–±\s+(\d+\.\d+\.\d+)',  # –°–ü–± 70.13330.2012
                r'–ú–ì–°–ù\s+(\d+\.\d+\.\d+)'  # –ú–ì–°–ù 4.19-2005
            ]
            
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    if '–°–ü' in pattern:
                        return f"–°–ü {match.group(1)}"
                    elif '–°–ù–∏–ü' in pattern:
                        return f"–°–ù–∏–ü {match.group(1)}"
                    elif '–ì–û–°–¢' in pattern:
                        return f"–ì–û–°–¢ {match.group(1)}"
                    elif '–¢–£' in pattern:
                        return f"–¢–£ {match.group(1)}"
                    elif '–ü–ë' in pattern:
                        return f"–ü–ë {match.group(1)}"
                    elif '–ù–ü–ë' in pattern:
                        return f"–ù–ü–ë {match.group(1)}"
                    elif '–°–ü–±' in pattern:
                        return f"–°–ü–± {match.group(1)}"
                    elif '–ú–ì–°–ù' in pattern:
                        return f"–ú–ì–°–ù {match.group(1)}"
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå [DOCUMENT_CODE_EXTRACTION] Error extracting document code: {e}")
            return None
    
    def _format_consultation_response(self, message: str, search_results: List[Dict], top_result: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –æ—Ç–≤–µ—Ç–∞
            query_lower = message.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞
            if any(word in query_lower for word in ['–∫–∞–∫–æ–π', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞']):
                response_type = "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π"
            elif any(word in query_lower for word in ['—Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∏—Ä—É–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç', '—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç']):
                response_type = "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π"
            else:
                response_type = "–æ–±—â–∏–π"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            response_parts = []
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç–≤–µ—Ç–∞
            if response_type == "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π":
                response_parts.append("## üìã –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ")
            elif response_type == "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π":
                response_parts.append("## üí° –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É")
            else:
                response_parts.append("## üìñ –û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
            response_parts.append("")
            response_parts.append(f"**{top_result['document_title']}**")
            response_parts.append(f"*–†–∞–∑–¥–µ–ª: {top_result['section']}*")
            response_parts.append("")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∞–±–∑–∞—Ü—ã
            content = top_result['content']
            if content:
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –≤ –∞–±–∑–∞—Ü—ã
                sentences = content.split('. ')
                paragraphs = []
                current_paragraph = []
                
                for sentence in sentences:
                    if sentence.strip():
                        current_paragraph.append(sentence.strip())
                        # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å—Ç–∞–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–º, –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                        if len(' '.join(current_paragraph)) > 200:
                            paragraphs.append(' '.join(current_paragraph))
                            current_paragraph = []
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü—ã –≤ –æ—Ç–≤–µ—Ç
                for paragraph in paragraphs:
                    if paragraph.strip():
                        response_parts.append(paragraph.strip())
                        response_parts.append("")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥—Ä—É–≥–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if len(search_results) > 1:
                response_parts.append("---")
                response_parts.append("## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
                response_parts.append("")
                
                for i, result in enumerate(search_results[1:3], 1):  # –ë–µ—Ä–µ–º –µ—â–µ 2 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    if result['document_title'] != top_result['document_title']:
                        response_parts.append(f"**{result['document_title']}**")
                        response_parts.append(f"*–†–∞–∑–¥–µ–ª: {result['section']}*")
                        response_parts.append("")
                        
                        # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                        preview = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
                        response_parts.append(preview)
                        response_parts.append("")
            
            # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
            response_parts.append("---")
            response_parts.append("## üìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            response_parts.append("")
            response_parts.append("–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            logger.error(f"‚ùå [FORMAT_RESPONSE] Error formatting response: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
            return f"**{top_result['document_title']}**\n\n{top_result['content']}"
    
    def _format_consultation_response_with_context(self, message: str, structured_context: Dict[str, Any], top_result: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –æ—Ç–≤–µ—Ç–∞
            query_lower = message.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞
            if any(word in query_lower for word in ['–∫–∞–∫–æ–π', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞']):
                response_type = "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π"
            elif any(word in query_lower for word in ['—Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∏—Ä—É–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç', '—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç']):
                response_type = "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π"
            else:
                response_type = "–æ–±—â–∏–π"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            response_parts = []
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç–≤–µ—Ç–∞
            if response_type == "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π":
                response_parts.append("## üìã –ù–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ")
            elif response_type == "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π":
                response_parts.append("## üí° –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É")
            else:
                response_parts.append("## üìñ –û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –ú–µ—Ç–∞-—Å–≤–æ–¥–∫–∞
            meta_summary = structured_context.get('meta_summary', {})
            if meta_summary:
                response_parts.append("")
                response_parts.append(f"**üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞:** {meta_summary.get('query_type', '–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è')}")
                response_parts.append(f"**üìö –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {meta_summary.get('documents_found', 0)}")
                response_parts.append(f"**üìë –†–∞–∑–¥–µ–ª–æ–≤:** {meta_summary.get('sections_covered', 0)}")
                response_parts.append(f"**‚≠ê –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∫—Ä—ã—Ç–∏—è:** {meta_summary.get('coverage_quality', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                
                if meta_summary.get('key_documents'):
                    response_parts.append(f"**üîë –ö–ª—é—á–µ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:** {', '.join(meta_summary['key_documents'][:3])}")
            
            response_parts.append("")
            response_parts.append("---")
            response_parts.append("")
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_items = structured_context.get('context', [])
            
            for i, item in enumerate(context_items[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
                response_parts.append(f"### {i}. {item['doc']} - {item['document_title']}")
                response_parts.append(f"**–†–∞–∑–¥–µ–ª:** {item['section']} - {item['section_title']}")
                response_parts.append(f"**–°—Ç—Ä–∞–Ω–∏—Ü–∞:** {item['page']}")
                response_parts.append(f"**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** {item['score']:.2f} ({item['why']})")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'summary' in item:
                    summary = item['summary']
                    response_parts.append("")
                    response_parts.append(f"**üìù –û —Ä–∞–∑–¥–µ–ª–µ:** {summary['topic']}")
                    response_parts.append(f"**‚öñÔ∏è –¢–∏–ø –Ω–æ—Ä–º—ã:** {summary['norm_type']}")
                    
                    if summary['key_points']:
                        response_parts.append("**üîë –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:**")
                        for point in summary['key_points'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 3 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
                            response_parts.append(f"‚Ä¢ {point}")
                    
                    response_parts.append(f"**üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** {summary['relevance_reason']}")
                
                response_parts.append("")
                response_parts.append(f"**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:**")
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∞–±–∑–∞—Ü—ã
                content = item.get('snippet', '')
                if content:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –≤ –∞–±–∑–∞—Ü—ã
                    sentences = content.split('. ')
                    paragraphs = []
                    current_paragraph = []
                    
                    for sentence in sentences:
                        if sentence.strip():
                            current_paragraph.append(sentence.strip())
                            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å—Ç–∞–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–º, –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                            if len(' '.join(current_paragraph)) > 200:
                                paragraphs.append(' '.join(current_paragraph))
                                current_paragraph = []
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü
                    if current_paragraph:
                        paragraphs.append(' '.join(current_paragraph))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü—ã –≤ –æ—Ç–≤–µ—Ç
                    for paragraph in paragraphs:
                        response_parts.append(paragraph)
                        response_parts.append("")
                else:
                    response_parts.append("–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
                
                response_parts.append("---")
                response_parts.append("")
            
            # –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            response_parts.append(f"**üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞:**")
            response_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {structured_context.get('total_candidates', 0)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            response_parts.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {structured_context.get('avg_score', 0):.2f}")
            response_parts.append(f"‚Ä¢ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {structured_context.get('timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            logger.error(f"‚ùå [CONSULTATION_FORMAT] Error formatting response: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
            return self._format_consultation_response(message, [top_result], top_result)
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                           ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                           COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                    FROM uploaded_documents ud
                    LEFT JOIN (
                        SELECT document_id, COUNT(*) as chunks_count 
                        FROM normative_chunks 
                        GROUP BY document_id
                    ) chunk_counts ON ud.id = chunk_counts.document_id
                    ORDER BY ud.upload_date DESC
                """)
                documents = cursor.fetchall()
                
                result = []
                for doc in documents:
                    result.append({
                        'id': doc['id'],
                        'title': doc['original_filename'],
                        'original_filename': doc['original_filename'],
                        'filename': doc['original_filename'],
                        'category': doc['category'],
                        'status': doc['processing_status'],
                        'processing_status': doc['processing_status'],
                        'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                        'file_size': doc['file_size'],
                        'token_count': doc['token_count'],
                        'vector_indexed': doc['processing_status'] == 'completed',
                        'chunks_count': doc['chunks_count']
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []
    
    def get_documents_from_uploaded(self, document_type: str = 'all') -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã uploaded_documents"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                           ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                           COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                    FROM uploaded_documents ud
                    LEFT JOIN (
                        SELECT document_id, COUNT(*) as chunks_count 
                        FROM normative_chunks 
                        GROUP BY document_id
                    ) chunk_counts ON ud.id = chunk_counts.document_id
                    WHERE ud.category = %s OR %s = 'all'
                    ORDER BY ud.upload_date DESC
                """, (document_type, document_type))
                documents = cursor.fetchall()
                
                result = []
                for doc in documents:
                    result.append({
                        'id': doc['id'],
                        'title': doc['original_filename'],
                        'original_filename': doc['original_filename'],
                        'filename': doc['original_filename'],
                        'category': doc['category'],
                        'status': doc['processing_status'],
                        'processing_status': doc['processing_status'],
                        'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                        'file_size': doc['file_size'],
                        'token_count': doc['token_count'],
                        'vector_indexed': doc['processing_status'] == 'completed',
                        'chunks_count': doc['chunks_count']
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS_FROM_UPLOADED] Error getting documents: {e}")
            return []
    
    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("""
                    SELECT original_filename 
                    FROM uploaded_documents 
                    WHERE id = %s
                """, (document_id,))
                document_result = cursor.fetchone()
                # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                import re
                original_filename = document_result['original_filename'] if document_result else f"Document_{document_id}"
                document_title = re.sub(r'\.(pdf|txt|doc|docx)$', '', original_filename, flags=re.IGNORECASE)
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("""
                    SELECT chunk_id, content, chapter as section_title, chunk_type, page_number as page, section
                    FROM normative_chunks
                    WHERE document_id = %s
                    ORDER BY page_number, chunk_id
                """, (document_id,))
                chunks = cursor.fetchall()
                
                result = []
                for chunk in chunks:
                    result.append({
                        'chunk_id': chunk['chunk_id'],
                        'content': chunk['content'],
                        'section_title': chunk['section_title'],
                        'chunk_type': chunk['chunk_type'],
                        'page': chunk['page'],
                        'section': chunk['section'],
                        'document_title': document_title  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    })
                
                logger.info(f"üìã [GET_DOCUMENT_CHUNKS] Retrieved {len(result)} chunks for document {document_id} ({document_title})")
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting chunks for document {document_id}: {e}")
            return []
    
    def delete_document_indexes(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Qdrant"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_INDEXES] Deleting indexes for document {document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.get_document_chunks(document_id)
            if not chunks:
                logger.warning(f"‚ö†Ô∏è [DELETE_INDEXES] No chunks found for document {document_id}")
                return True
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ Qdrant
            point_ids = []
            for chunk in chunks:
                qdrant_id = hash(f"{document_id}_{chunk['chunk_id']}") % (2**63 - 1)
                if qdrant_id < 0:
                    qdrant_id = abs(qdrant_id)
                point_ids.append(qdrant_id)
            
            # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
            if point_ids:
                # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
                self.qdrant_service.delete_points_by_document(document_id)
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted points from Qdrant for document {document_id}")
            
            # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ –∏–∑ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks WHERE document_id = %s", (document_id,))
                deleted_chunks = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_INDEXES] Deleted {deleted_chunks} chunks from PostgreSQL for document {document_id}")
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_INDEXES] Error deleting indexes for document {document_id}: {e}")
            return False
    
    def delete_document(self, document_id: int) -> bool:
        """–ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info(f"üóëÔ∏è [DELETE_DOCUMENT] Deleting document {document_id}")
            
            # 1. –£–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ Qdrant
            indexes_deleted = self.delete_document_indexes(document_id)
            
            # 2. –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            with self.db_manager.get_cursor() as cursor:
                # –£–¥–∞–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                cursor.execute("DELETE FROM extracted_elements WHERE uploaded_document_id = %s", (document_id,))
                deleted_elements = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_elements} extracted elements for document {document_id}")
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                cursor.execute("DELETE FROM uploaded_documents WHERE id = %s", (document_id,))
                deleted_documents = cursor.rowcount
                logger.info(f"‚úÖ [DELETE_DOCUMENT] Deleted {deleted_documents} documents for document {document_id}")
                
                # –§–∏–∫—Å–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
                cursor.connection.commit()
            
            if deleted_documents == 0:
                logger.warning(f"‚ö†Ô∏è [DELETE_DOCUMENT] Document {document_id} not found")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_DOCUMENT] Error deleting document {document_id}: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–∏—Å
            qdrant_info = self.qdrant_service.get_collection_info()
            qdrant_stats = {
                'collection_name': self.VECTOR_COLLECTION,
                'vectors_count': qdrant_info.get('points_count', 0),
                'indexed_vectors': qdrant_info.get('points_count', 0),
                'status': 'ok' if qdrant_info else 'unknown'
            }
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("SELECT COUNT(*) as total_documents FROM uploaded_documents")
                total_docs = cursor.fetchone()['total_documents']
                
                cursor.execute("SELECT COUNT(*) as total_chunks FROM normative_chunks")
                total_chunks = cursor.fetchone()['total_chunks']
                
                cursor.execute("SELECT COUNT(*) as pending_docs FROM uploaded_documents WHERE processing_status = 'pending'")
                pending_docs = cursor.fetchone()['pending_docs']
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                cursor.execute("SELECT COALESCE(SUM(token_count), 0) as total_tokens FROM uploaded_documents")
                total_tokens = cursor.fetchone()['total_tokens']
            
            db_stats = {
                'total_documents': total_docs,
                'total_chunks': total_chunks,
                'pending_documents': pending_docs,
                'total_tokens': total_tokens
            }
            
            return {
                'qdrant': qdrant_stats,
                'postgresql': db_stats,
                'embedding_model': 'bge-m3 (Ollama)',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            try:
                with self.db_manager.get_cursor() as cursor:
                    cursor.execute("SELECT COUNT(*) as total_documents FROM uploaded_documents")
                    total_docs = cursor.fetchone()['total_documents']
                    
                    cursor.execute("SELECT COUNT(*) as total_chunks FROM normative_chunks")
                    total_chunks = cursor.fetchone()['total_chunks']
                    
                    cursor.execute("SELECT COALESCE(SUM(token_count), 0) as total_tokens FROM uploaded_documents")
                    total_tokens = cursor.fetchone()['total_tokens']
                
                return {
                    'qdrant': {
                        'collection_name': self.VECTOR_COLLECTION,
                        'vectors_count': 0,  # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Qdrant
                        'indexed_vectors': 0,
                        'status': 'error'
                    },
                    'postgresql': {
                        'total_documents': total_docs,
                        'total_chunks': total_chunks,
                        'pending_documents': 0,
                        'total_tokens': total_tokens
                    },
                    'embedding_model': 'bge-m3 (Ollama)',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
            except Exception as fallback_error:
                logger.error(f"‚ùå [GET_STATS] Fallback error: {fallback_error}")
                return {
                    'error': f"Primary error: {str(e)}, Fallback error: {str(fallback_error)}",
                    'timestamp': datetime.now().isoformat()
                }

    def save_document_to_db(self, document_id: int, filename: str, original_filename: str, 
                           file_type: str, file_size: int, document_hash: str, 
                           category: str, document_type: str) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.db_manager.get_cursor() as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ö–µ—à–µ–º
                cursor.execute("""
                    SELECT id FROM uploaded_documents 
                    WHERE document_hash = %s
                """, (document_hash,))
                
                if cursor.fetchone():
                    raise Exception("Document with this content already exists")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                cursor.execute("""
                    INSERT INTO uploaded_documents 
                    (id, filename, original_filename, file_type, file_size, document_hash, 
                     category, document_type, processing_status)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, 'pending')
                    RETURNING id
                """, (
                    document_id,
                    filename,
                    original_filename,
                    file_type,
                    file_size,
                    document_hash,
                    category,
                    document_type
                ))
                
                saved_id = cursor.fetchone()['id']
                cursor.connection.commit()
                logger.info(f"‚úÖ [SAVE_DOCUMENT] Document saved with ID: {saved_id}")
                return saved_id
                
        except Exception as e:
            logger.error(f"‚ùå [SAVE_DOCUMENT] Error saving document: {e}")
            raise

    def update_document_status(self, document_id: int, status: str, error_message: str = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            with self.db_manager.get_cursor() as cursor:
                if error_message:
                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET processing_status = %s, processing_error = %s
                        WHERE id = %s
                    """, (status, error_message, document_id))
                else:
                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET processing_status = %s, processing_error = NULL
                        WHERE id = %s
                    """, (status, document_id))
                
                cursor.connection.commit()
                logger.info(f"‚úÖ [UPDATE_STATUS] Document {document_id} status updated to: {status}")
                
        except Exception as e:
            logger.error(f"‚ùå [UPDATE_STATUS] Error updating document {document_id} status: {e}")

    async def process_document_async(self, document_id: int, content: bytes, filename: str) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            logger.info(f"üîÑ [PROCESS_ASYNC] Starting processing for document {document_id}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            text_content = await self.extract_text_from_document(content, filename)
            if not text_content:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to extract text from document {document_id}")
                return False
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.create_chunks(text_content, document_id, filename)
            if not chunks:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to create chunks for document {document_id}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            success = await self.index_chunks_async(chunks, document_id)
            if not success:
                logger.error(f"‚ùå [PROCESS_ASYNC] Failed to index chunks for document {document_id}")
                return False
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            token_count = len(text_content.split())
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    UPDATE uploaded_documents 
                    SET token_count = %s
                    WHERE id = %s
                """, (token_count, document_id))
                cursor.connection.commit()
            
            logger.info(f"‚úÖ [PROCESS_ASYNC] Document {document_id} processed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [PROCESS_ASYNC] Error processing document {document_id}: {e}")
            return False

    async def extract_text_from_document(self, content: bytes, filename: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            import tempfile
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{filename.split('.')[-1]}") as temp_file:
                temp_file.write(content)
                temp_file_path = temp_file.name
            
            try:
                if filename.lower().endswith('.pdf'):
                    return await self.extract_text_from_pdf(temp_file_path)
                elif filename.lower().endswith('.docx'):
                    return await self.extract_text_from_docx(temp_file_path)
                elif filename.lower().endswith('.txt'):
                    return content.decode('utf-8', errors='ignore')
                else:
                    logger.error(f"‚ùå [EXTRACT_TEXT] Unsupported file type: {filename}")
                    return ""
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                    
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_TEXT] Error extracting text: {e}")
            return ""

    async def extract_text_from_pdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        try:
            import PyPDF2
            
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PDF] Error extracting text from PDF: {e}")
            return ""

    async def extract_text_from_docx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX"""
        try:
            from docx import Document
            
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCX] Error extracting text from DOCX: {e}")
            return ""

    def create_chunks(self, text: str, document_id: int, filename: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            logger.info(f"üìù [CREATE_CHUNKS] Creating chunks for document {document_id}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º "–°—Ç—Ä–∞–Ω–∏—Ü–∞ X –∏–∑ Y"
            page_pattern = r'–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+(\d+)\s+–∏–∑\s+(\d+)'
            page_matches = list(re.finditer(page_pattern, text))
            
            chunks = []
            chunk_id = 1
            
            if page_matches:
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –Ω–∏–º
                logger.info(f"üìÑ [CREATE_CHUNKS] Found {len(page_matches)} page markers in document")
                
                for i, match in enumerate(page_matches):
                    page_num = int(match.group(1))
                    start_pos = match.end()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞)
                    if i + 1 < len(page_matches):
                        end_pos = page_matches[i + 1].start()
                    else:
                        end_pos = len(text)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text = text[start_pos:end_pos].strip()
                    
                    if page_text:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)
                        page_structure = self._extract_page_structure(page_text, page_num)
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —á–∞–Ω–∫–∏
                        page_chunks = self._split_page_into_chunks(page_text, chunk_size=1000)
                        
                        for chunk_text in page_chunks:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                            chunk_structure = self._identify_chunk_structure(chunk_text, page_structure)
                            
                            chunks.append({
                                'chunk_id': f"doc_{document_id}_page_{page_num}_chunk_{chunk_id}",
                                'document_id': document_id,
                                'document_title': filename,
                                'content': chunk_text.strip(),
                                'chunk_type': 'paragraph',
                                'page': page_num,
                                'chapter': chunk_structure.get('chapter', ''),
                                'section': chunk_structure.get('section', '')
                            })
                            chunk_id += 1
            else:
                # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                logger.info(f"üìÑ [CREATE_CHUNKS] No page markers found, treating as single page document")
                page_chunks = self._split_page_into_chunks(text, chunk_size=1000)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document_structure = self._extract_document_structure(text)
                
                for chunk_text in page_chunks:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫
                    chunk_structure = self._identify_chunk_structure(chunk_text, document_structure)
                    
                    chunks.append({
                        'chunk_id': f"doc_{document_id}_page_1_chunk_{chunk_id}",
                        'document_id': document_id,
                        'document_title': filename,
                        'content': chunk_text.strip(),
                        'chunk_type': 'paragraph',
                        'page': 1,
                        'chapter': chunk_structure.get('chapter', ''),
                        'section': chunk_structure.get('section', '')
                    })
                    chunk_id += 1
            
            logger.info(f"‚úÖ [CREATE_CHUNKS] Created {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS] Error creating chunks: {e}")
            return []
    
    def _split_page_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            from config.chunking_config import get_chunking_config, validate_chunking_config
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            config = get_chunking_config('default')
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if not validate_chunking_config(config):
                logger.warning("‚ö†Ô∏è [CHUNKING] Invalid chunking config, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–æ –ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if config.get('semantic_chunking', True):
                logger.info("üìù [CHUNKING] Using semantic chunking with meaning-based analysis")
                logger.info(f"üìù [CHUNKING] Input text length: {len(text)} characters")
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
                semantic_chunks = self._create_semantic_chunks(text, config)
                
                if semantic_chunks:
                    logger.info(f"‚úÖ [CHUNKING] Created {len(semantic_chunks)} semantic chunks")
                    return semantic_chunks
                else:
                    logger.warning("‚ö†Ô∏è [CHUNKING] No semantic chunks created, falling back to hierarchical")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–æ –ª–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if not config.get('hierarchical_chunking', True):
                logger.info("üìù [CHUNKING] Hierarchical chunking disabled, using standard chunking")
                return self._standard_chunking(text, config)
            
            logger.info("üìù [CHUNKING] Using hierarchical chunking with structure preservation")
            logger.info(f"üìù [CHUNKING] Input text length: {len(text)} characters")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_structure = self._extract_document_structure(text)
            logger.info(f"üìù [CHUNKING] Extracted structure: {len(document_structure['chapters'])} chapters, {len(document_structure['sections'])} sections")
            
            # –°–æ–∑–¥–∞–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
            hierarchical_chunks = self._create_hierarchical_chunks(text, document_structure, config)
            
            if not hierarchical_chunks:
                logger.warning("‚ö†Ô∏è [CHUNKING] No hierarchical chunks created, using fallback")
                return self._simple_split_into_chunks(text, chunk_size)
            
            logger.info(f"‚úÖ [CHUNKING] Created {len(hierarchical_chunks)} hierarchical chunks")
            return hierarchical_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Error creating hierarchical chunks: {e}")
            import traceback
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Traceback: {traceback.format_exc()}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ä–∞–∑–±–∏–µ–Ω–∏—é
            return self._simple_split_into_chunks(text, chunk_size)

    def _standard_chunking(self, text: str, config: dict) -> List[str]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏–∏"""
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            
            logger.info(f"üìù [STANDARD_CHUNKING] Using config: target={target_tokens}, min={min_tokens}, max={max_tokens}, overlap={overlap_ratio}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [STANDARD_CHUNKING] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [STANDARD_CHUNKING] No sentences found, using fallback")
                return self._simple_split_into_chunks(text, 1000)
            
            chunks = []
            current_chunk = []
            current_tokens = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            for i, sentence in enumerate(sentences):
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_tokens >= target_tokens and current_tokens >= min_tokens:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            if current_chunk and current_tokens >= min_tokens:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            elif current_chunk:
                if chunks:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    last_chunk = chunks[-1]
                    merged_chunk = last_chunk + ' ' + ' '.join(current_chunk)
                    chunks[-1] = merged_chunk
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
            
            logger.info(f"‚úÖ [STANDARD_CHUNKING] Created {len(chunks)} standard chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [STANDARD_CHUNKING] Error in standard chunking: {e}")
            return self._simple_split_into_chunks(text, 1000)

    def _create_semantic_chunks(self, text: str, config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º—ã—Å–ª–∞"""
        try:
            logger.info("üìù [SEMANTIC_CHUNKS] Creating semantic chunks with meaning-based analysis")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            semantic_threshold = config.get('semantic_similarity_threshold', 0.7)
            window_size = config.get('semantic_window_size', 3)
            topic_change_threshold = config.get('topic_change_threshold', 0.3)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [SEMANTIC_CHUNKS] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [SEMANTIC_CHUNKS] No sentences found")
                return []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            semantic_analysis = self._analyze_semantic_structure(sentences, config)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤
            semantic_boundaries = self._find_semantic_boundaries(sentences, semantic_analysis, config)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
            semantic_chunks = self._create_chunks_from_semantic_boundaries(
                sentences, semantic_boundaries, config
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö —á–∞–Ω–∫–æ–≤
            if config.get('semantic_merge_threshold', 0.85) > 0:
                semantic_chunks = self._merge_semantically_similar_chunks(semantic_chunks, config)
            
            logger.info(f"‚úÖ [SEMANTIC_CHUNKS] Created {len(semantic_chunks)} semantic chunks")
            return semantic_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_CHUNKS] Error creating semantic chunks: {e}")
            return []

    def _analyze_semantic_structure(self, sentences: List[str], config: dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        try:
            from config.chunking_config import get_semantic_patterns
            import re
            
            patterns = get_semantic_patterns()
            
            analysis = {
                'topic_indicators': [],
                'coherence_indicators': [],
                'semantic_boundaries': [],
                'domain_keywords': [],
                'sentence_semantics': []
            }
            
            for i, sentence in enumerate(sentences):
                sentence_analysis = {
                    'index': i,
                    'text': sentence,
                    'topic_indicators': [],
                    'coherence_indicators': [],
                    'semantic_boundaries': [],
                    'domain_keywords': [],
                    'semantic_score': 0.0
                }
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ç–µ–º
                for pattern in patterns['topic_indicators']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['topic_indicators'].append(pattern)
                        analysis['topic_indicators'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏
                for pattern in patterns['coherence_indicators']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['coherence_indicators'].append(pattern)
                        analysis['coherence_indicators'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
                for pattern in patterns['semantic_boundaries']:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        sentence_analysis['semantic_boundaries'].append(pattern)
                        analysis['semantic_boundaries'].append(i)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                for domain, domain_patterns in patterns['domain_specific'].items():
                    for pattern in domain_patterns:
                        if re.search(pattern, sentence, re.IGNORECASE):
                            sentence_analysis['domain_keywords'].append((domain, pattern))
                            analysis['domain_keywords'].append((i, domain, pattern))
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentence_analysis['semantic_score'] = self._calculate_semantic_score(sentence_analysis)
                
                analysis['sentence_semantics'].append(sentence_analysis)
            
            logger.info(f"üìù [SEMANTIC_ANALYSIS] Found {len(analysis['topic_indicators'])} topic indicators, {len(analysis['semantic_boundaries'])} boundaries")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_ANALYSIS] Error analyzing semantic structure: {e}")
            return {'sentence_semantics': []}

    def _calculate_semantic_score(self, sentence_analysis: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        try:
            score = 0.0
            
            # –ë–∞–ª–ª—ã –∑–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            score += len(sentence_analysis['topic_indicators']) * 0.3
            score += len(sentence_analysis['coherence_indicators']) * 0.2
            score += len(sentence_analysis['semantic_boundaries']) * 0.4
            score += len(sentence_analysis['domain_keywords']) * 0.1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 0-1
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_SCORE] Error calculating semantic score: {e}")
            return 0.0

    def _find_semantic_boundaries(self, sentences: List[str], analysis: Dict[str, Any], config: dict) -> List[int]:
        """–ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            boundaries = []
            window_size = config.get('semantic_window_size', 3)
            topic_change_threshold = config.get('topic_change_threshold', 0.3)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            for boundary_idx in analysis['semantic_boundaries']:
                if boundary_idx not in boundaries:
                    boundaries.append(boundary_idx)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–º—ã –≤ —Å–∫–æ–ª—å–∑—è—â–µ–º –æ–∫–Ω–µ
            for i in range(window_size, len(sentences) - window_size):
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ –æ–∫–Ω–µ
                window_similarity = self._calculate_window_similarity(
                    sentences, i, window_size, analysis
                )
                
                # –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞, —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–∞
                if window_similarity < topic_change_threshold:
                    if i not in boundaries:
                        boundaries.append(i)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
            boundaries.sort()
            
            logger.info(f"üìù [SEMANTIC_BOUNDARIES] Found {len(boundaries)} semantic boundaries")
            return boundaries
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_BOUNDARIES] Error finding semantic boundaries: {e}")
            return []

    def _calculate_window_similarity(self, sentences: List[str], center_idx: int, window_size: int, analysis: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ –æ–∫–Ω–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ–∫–Ω–µ
            start_idx = max(0, center_idx - window_size)
            end_idx = min(len(sentences), center_idx + window_size + 1)
            
            window_sentences = sentences[start_idx:end_idx]
            
            if len(window_sentences) < 2:
                return 1.0
            
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            similarity_scores = []
            
            for i in range(len(window_sentences) - 1):
                sent1 = window_sentences[i].lower()
                sent2 = window_sentences[i + 1].lower()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø—Ä–æ—Å—Ç—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ)
                words1 = set([w for w in sent1.split() if len(w) > 3 and w.isalpha()])
                words2 = set([w for w in sent2.split() if len(w) > 3 and w.isalpha()])
                
                if words1 and words2:
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
                    intersection = len(words1.intersection(words2))
                    union = len(words1.union(words2))
                    jaccard = intersection / union if union > 0 else 0.0
                    similarity_scores.append(jaccard)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå [WINDOW_SIMILARITY] Error calculating window similarity: {e}")
            return 0.0

    def _create_chunks_from_semantic_boundaries(self, sentences: List[str], boundaries: List[int], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü"""
        try:
            chunks = []
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
            all_boundaries = [0] + boundaries + [len(sentences)]
            all_boundaries = sorted(list(set(all_boundaries)))
            
            for i in range(len(all_boundaries) - 1):
                start_idx = all_boundaries[i]
                end_idx = all_boundaries[i + 1]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —á–∞–Ω–∫–∞
                chunk_sentences = sentences[start_idx:end_idx]
                
                if not chunk_sentences:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞
                chunk_text = ' '.join(chunk_sentences)
                chunk_tokens = self._estimate_tokens(chunk_text, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                if chunk_tokens < min_tokens:
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º
                    if i + 2 < len(all_boundaries):
                        next_start = all_boundaries[i + 1]
                        next_end = all_boundaries[i + 2]
                        next_sentences = sentences[next_start:next_end]
                        chunk_sentences.extend(next_sentences)
                        chunk_text = ' '.join(chunk_sentences)
                        chunk_tokens = self._estimate_tokens(chunk_text, config)
                
                if chunk_tokens > max_tokens:
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
                    sub_chunks = self._split_large_chunk(chunk_sentences, config)
                    chunks.extend(sub_chunks)
                else:
                    chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_CHUNKS_CREATION] Error creating chunks from boundaries: {e}")
            return []

    def _split_large_chunk(self, sentences: List[str], config: dict) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏"""
        try:
            chunks = []
            target_tokens = config['target_tokens']
            max_tokens = config['max_tokens']
            
            current_chunk = []
            current_tokens = 0
            
            for sentence in sentences:
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                    current_chunk = [sentence]
                    current_tokens = sentence_tokens
                else:
                    current_chunk.append(sentence)
                    current_tokens += sentence_tokens
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SPLIT_LARGE_CHUNK] Error splitting large chunk: {e}")
            return []

    def _merge_semantically_similar_chunks(self, chunks: List[str], config: dict) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            merge_threshold = config.get('semantic_merge_threshold', 0.85)
            
            if len(chunks) <= 1:
                return chunks
            
            merged_chunks = []
            i = 0
            
            while i < len(chunks):
                current_chunk = chunks[i]
                merged_chunk = current_chunk
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
                j = i + 1
                while j < len(chunks):
                    next_chunk = chunks[j]
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarity = self._calculate_chunk_similarity(current_chunk, next_chunk)
                    
                    if similarity >= merge_threshold:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                        merged_chunk += ' ' + next_chunk
                        j += 1
                    else:
                        break
                
                merged_chunks.append(merged_chunk.strip())
                i = j
            
            logger.info(f"üìù [SEMANTIC_MERGE] Merged {len(chunks)} chunks into {len(merged_chunks)} chunks")
            return merged_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [SEMANTIC_MERGE] Error merging similar chunks: {e}")
            return chunks

    def _calculate_chunk_similarity(self, chunk1: str, chunk2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            words1 = set([w.lower() for w in chunk1.split() if len(w) > 3 and w.isalpha()])
            words2 = set([w.lower() for w in chunk2.split() if len(w) > 3 and w.isalpha()])
            
            if not words1 or not words2:
                return 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå [CHUNK_SIMILARITY] Error calculating chunk similarity: {e}")
            return 0.0

    def _create_hierarchical_chunks(self, text: str, structure: Dict[str, Any], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            logger.info("üìù [HIERARCHICAL_CHUNKS] Creating hierarchical chunks with structure preservation")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            preserve_structure = config.get('preserve_structure', True)
            chapter_boundaries = config.get('chapter_boundaries', True)
            section_boundaries = config.get('section_boundaries', True)
            
            chunks = []
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if not structure['chapters'] and not structure['sections']:
                logger.info("üìù [HIERARCHICAL_CHUNKS] No structure found, using standard chunking")
                return self._standard_chunking(text, config)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text, config)
            logger.info(f"üìù [HIERARCHICAL_CHUNKS] Split into {len(sentences)} sentences")
            
            if not sentences:
                logger.warning("‚ö†Ô∏è [HIERARCHICAL_CHUNKS] No sentences found")
                return []
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            sentence_structure_map = self._map_sentences_to_structure(sentences, structure)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –µ–¥–∏–Ω–∏—Ü–∞–º
            structural_units = self._group_sentences_by_structure(sentences, sentence_structure_map, structure)
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            for unit in structural_units:
                unit_chunks = self._create_chunks_for_structural_unit(unit, config)
                chunks.extend(unit_chunks)
            
            logger.info(f"‚úÖ [HIERARCHICAL_CHUNKS] Created {len(chunks)} hierarchical chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [HIERARCHICAL_CHUNKS] Error creating hierarchical chunks: {e}")
            return []

    def _map_sentences_to_structure(self, sentences: List[str], structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º"""
        try:
            sentence_map = []
            
            for i, sentence in enumerate(sentences):
                sentence_info = {
                    'index': i,
                    'text': sentence,
                    'chapter': None,
                    'section': None,
                    'subsection': None,
                    'paragraph': None,
                    'special_structure': None
                }
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≥–ª–∞–≤–µ
                for chapter in structure['chapters']:
                    if chapter['title'].lower() in sentence.lower() or chapter['number'] in sentence:
                        sentence_info['chapter'] = chapter
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–¥–µ–ª—É
                for section in structure['sections']:
                    if section['number'] in sentence or section['title'].lower() in sentence.lower():
                        sentence_info['section'] = section
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∞–±–∑–∞—Ü—É
                for paragraph in structure['paragraphs']:
                    if paragraph['text'].lower() in sentence.lower():
                        sentence_info['paragraph'] = paragraph
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
                for special in structure['special_structures']:
                    if special['text'].lower() in sentence.lower():
                        sentence_info['special_structure'] = special
                        break
                
                sentence_map.append(sentence_info)
            
            return sentence_map
            
        except Exception as e:
            logger.error(f"‚ùå [MAP_SENTENCES] Error mapping sentences to structure: {e}")
            return []

    def _group_sentences_by_structure(self, sentences: List[str], sentence_map: List[Dict[str, Any]], structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –µ–¥–∏–Ω–∏—Ü–∞–º"""
        try:
            structural_units = []
            current_unit = None
            
            for i, sentence_info in enumerate(sentence_map):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
                current_chapter = sentence_info.get('chapter')
                current_section = sentence_info.get('section')
                current_paragraph = sentence_info.get('paragraph')
                current_special = sentence_info.get('special_structure')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
                should_start_new_unit = False
                
                if current_unit is None:
                    should_start_new_unit = True
                elif current_chapter and current_chapter != current_unit.get('chapter'):
                    should_start_new_unit = True
                elif current_section and current_section != current_unit.get('section'):
                    should_start_new_unit = True
                elif current_special and current_special != current_unit.get('special_structure'):
                    should_start_new_unit = True
                
                if should_start_new_unit:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –µ–¥–∏–Ω–∏—Ü—É
                    if current_unit:
                        structural_units.append(current_unit)
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –µ–¥–∏–Ω–∏—Ü—É
                    current_unit = {
                        'chapter': current_chapter,
                        'section': current_section,
                        'paragraph': current_paragraph,
                        'special_structure': current_special,
                        'sentences': [sentence_info['text']],
                        'start_index': i,
                        'end_index': i
                    }
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–π –µ–¥–∏–Ω–∏—Ü–µ
                    current_unit['sentences'].append(sentence_info['text'])
                    current_unit['end_index'] = i
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –µ–¥–∏–Ω–∏—Ü—É
            if current_unit:
                structural_units.append(current_unit)
            
            logger.info(f"üìù [GROUP_SENTENCES] Created {len(structural_units)} structural units")
            return structural_units
            
        except Exception as e:
            logger.error(f"‚ùå [GROUP_SENTENCES] Error grouping sentences: {e}")
            return []

    def _create_chunks_for_structural_unit(self, unit: Dict[str, Any], config: dict) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã"""
        try:
            sentences = unit['sentences']
            if not sentences:
                return []
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            target_tokens = config['target_tokens']
            min_tokens = config['min_tokens']
            max_tokens = config['max_tokens']
            overlap_ratio = config['overlap_ratio']
            
            chunks = []
            current_chunk = []
            current_tokens = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã
            for sentence in sentences:
                sentence_tokens = self._estimate_tokens(sentence, config)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > max_tokens and current_chunk:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_tokens >= target_tokens and current_tokens >= min_tokens:
                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_sentences = self._get_overlap_sentences(current_chunk, overlap_ratio, config)
                    current_chunk = overlap_sentences
                    current_tokens = sum(self._estimate_tokens(s, config) for s in overlap_sentences)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
            if current_chunk and current_tokens >= min_tokens:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text.strip())
            elif current_chunk:
                if chunks:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    last_chunk = chunks[-1]
                    merged_chunk = last_chunk + ' ' + ' '.join(current_chunk)
                    chunks[-1] = merged_chunk
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNKS_FOR_UNIT] Error creating chunks for structural unit: {e}")
            return []

    def _split_into_sentences(self, text: str, config: dict) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            sentence_patterns = config.get('sentence_patterns', [
                r'[.!?]+(?=\s+[–ê-–Ø–Å\d])',  # –û–±—ã—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                r'[.!?]+(?=\s+\d+\.)',      # –ü–µ—Ä–µ–¥ –Ω–æ–º–µ—Ä–∞–º–∏ –ø—É–Ω–∫—Ç–æ–≤
                r'[.!?]+(?=\s+[–ê-–Ø–Å]\s)',  # –ü–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                r'[.!?]+(?=\s*$)'           # –í –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
            ])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            combined_pattern = '|'.join(sentence_patterns)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç
            sentences = re.split(combined_pattern, text)
            
            # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            min_length = config.get('min_sentence_length', 10)
            cleaned_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > min_length:
                    cleaned_sentences.append(sentence)
            
            return cleaned_sentences
            
        except Exception as e:
            logger.error(f"‚ùå [SENTENCE_SPLIT] Error splitting into sentences: {e}")
            # Fallback: –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º
            return [s.strip() for s in text.split('.') if s.strip()]
    
    def _estimate_tokens(self, text: str, config: dict) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            tokens_per_char = config.get('tokens_per_char', 4)
            return max(1, len(text) // tokens_per_char)
        except Exception as e:
            logger.error(f"‚ùå [TOKEN_ESTIMATION] Error estimating tokens: {e}")
            return len(text) // 4
    
    def _get_overlap_sentences(self, sentences: List[str], overlap_ratio: float, config: dict) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"""
        try:
            if not sentences:
                return []
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            min_overlap = config.get('min_overlap_sentences', 1)
            overlap_count = max(min_overlap, int(len(sentences) * overlap_ratio))
            return sentences[-overlap_count:]
            
        except Exception as e:
            logger.error(f"‚ùå [OVERLAP] Error getting overlap sentences: {e}")
            return sentences[-1:] if sentences else []
    
    def _merge_chunks_with_headers(self, chunks: List[str], config: dict) -> List[str]:
        """–°–∫–ª–µ–π–∫–∞ —á–∞–Ω–∫–æ–≤ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–±—Ä—ã–≤–∞ —Ü–∏—Ç–∞—Ç"""
        try:
            if len(chunks) <= 1:
                return chunks
            
            merged_chunks = []
            current_chunk = chunks[0]
            
            for i in range(1, len(chunks)):
                next_chunk = chunks[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —á–∞–Ω–∫–∏
                should_merge = self._should_merge_chunks(current_chunk, next_chunk, config)
                
                if should_merge:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                    current_chunk = current_chunk + ' ' + next_chunk
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                    merged_chunks.append(current_chunk)
                    current_chunk = next_chunk
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            merged_chunks.append(current_chunk)
            
            logger.info(f"üìù [MERGE_HEADERS] Merged {len(chunks)} chunks into {len(merged_chunks)} chunks")
            return merged_chunks
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_HEADERS] Error merging chunks: {e}")
            return chunks
    
    def _should_merge_chunks(self, chunk1: str, chunk2: str, config: dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
            combined_tokens = self._estimate_tokens(chunk1, config) + self._estimate_tokens(chunk2, config)
            
            # –ï—Å–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            max_merged = config.get('max_merged_tokens', 1200)
            if combined_tokens > max_merged:
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            header_patterns = config.get('header_patterns', ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–ø—É–Ω–∫—Ç'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            if any(pattern in chunk1.lower() for pattern in header_patterns):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –≤—Ç–æ—Ä–æ–π —á–∞–Ω–∫ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if chunk2 and not chunk2[0].isupper():
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            unfinished_patterns = config.get('unfinished_patterns', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–≤—ã—á–∫–∏
            quotes = unfinished_patterns.get('quotes', ['"', '¬´', '¬ª'])
            if any(chunk1.count(q) % 2 != 0 for q in quotes):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–±–∫–∏
            brackets = unfinished_patterns.get('brackets', ['(', '[', '{'])
            if any(chunk1.count(b) != chunk1.count(self._get_closing_bracket(b)) for b in brackets):
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå [MERGE_LOGIC] Error in merge logic: {e}")
            return False
    
    def _get_closing_bracket(self, opening_bracket: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –¥–ª—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π"""
        bracket_pairs = {
            '(': ')',
            '[': ']',
            '{': '}',
            '<': '>'
        }
        return bracket_pairs.get(opening_bracket, '')

    def _simple_split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è."""
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence + ". "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    async def index_chunks_async(self, chunks: List[Dict[str, Any]], document_id: int) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"üîç [INDEX_CHUNKS_ASYNC] Getting metadata for document_id: {document_id}")
            document_metadata = self._get_document_metadata(document_id)
            logger.info(f"üîç [INDEX_CHUNKS_ASYNC] Retrieved metadata: {document_metadata}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ PostgreSQL
            with self.db_manager.get_cursor() as cursor:
                for chunk in chunks:
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, document_id, document_title, chunk_type, content, page_number, chapter, section)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        chunk['chunk_id'],
                        chunk['chunk_id'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk_id –∫–∞–∫ clause_id
                        chunk['document_id'],
                        chunk['document_title'],
                        chunk['chunk_type'],
                        chunk['content'],
                        chunk.get('page', 1),  # page_number
                        chunk.get('chapter', ''),  # chapter
                        chunk.get('section', '')   # section
                    ))
                cursor.connection.commit()
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            for chunk in chunks:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = self.embedding_service.create_embedding(chunk['content'])
                if embedding is None:
                    logger.warning(f"‚ö†Ô∏è [INDEX_CHUNKS] Failed to create embedding for chunk {chunk['chunk_id']}")
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
                point_id = hash(chunk['chunk_id']) % (2**63 - 1)
                if point_id < 0:
                    point_id = abs(point_id)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ —Å–ø–∏—Å–æ–∫
                if hasattr(embedding, 'tolist'):
                    vector = embedding.tolist()
                else:
                    vector = list(embedding)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                chunk_metadata = self._create_chunk_metadata(chunk, document_metadata)
                
                payload = {
                    'chunk_id': chunk['chunk_id'],
                    'document_id': chunk['document_id'],
                    'document_title': chunk['document_title'],
                    'content': chunk['content'],
                    'chunk_type': chunk['chunk_type'],
                    'page': chunk.get('page', 1),
                    'chapter': chunk.get('chapter', ''),
                    'section': chunk.get('section', ''),
                    'metadata': chunk_metadata
                }
                
                self.qdrant_service.upsert_point(point_id, vector, payload)
            
            logger.info(f"‚úÖ [INDEX_CHUNKS] Indexed {len(chunks)} chunks for document {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX_CHUNKS] Error indexing chunks: {e}")
            return False

    def clear_collection(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant"""
        try:
            logger.info("üßπ [CLEAR_COLLECTION] Clearing entire collection...")
            
            # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            success = self.qdrant_service.clear_collection()
            
            if success:
                logger.info("‚úÖ [CLEAR_COLLECTION] Collection cleared successfully")
                return True
            else:
                logger.error("‚ùå [CLEAR_COLLECTION] Failed to clear collection")
                return False
            
        except Exception as e:
            logger.error(f"‚ùå [CLEAR_COLLECTION] Error clearing collection: {e}")
            return False
    
    def _extract_page_structure(self, page_text: str, page_num: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)"""
        try:
            structure = {
                'page': page_num,
                'chapters': [],
                'sections': [],
                'headers': []
            }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤ –∏ —Ä–∞–∑–¥–µ–ª–æ–≤
            chapter_patterns = [
                r'^–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–ê–°–¢–¨\s+(\d+)\s*[\.\-]?\s*(.+)$',
                r'^–ß–∞—Å—Ç—å\s+(\d+)\s*[\.\-]?\s*(.+)$'
            ]
            
            section_patterns = [
                r'^(\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+\.\d+\.\d+\.\d+)\s+(.+)$',
                r'^(\d+)\s+(.+)$'
            ]
            
            lines = page_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        structure['chapters'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure['sections'].append({
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line
                        })
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
                if line.isupper() and len(line) > 5 and len(line) < 100:
                    structure['headers'].append(line)
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PAGE_STRUCTURE] Error extracting page structure: {e}")
            return {'page': page_num, 'chapters': [], 'sections': [], 'headers': []}
    
    def _extract_document_structure(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–µ–π"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            from config.chunking_config import get_hierarchical_patterns
            
            structure = {
                'chapters': [],
                'sections': [],
                'paragraphs': [],
                'special_structures': [],
                'headers': []
            }
            
            # –ü–æ–ª—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            patterns = get_hierarchical_patterns()
            chapter_patterns = patterns['chapters']
            section_patterns = patterns['sections']
            paragraph_patterns = patterns['paragraphs']
            special_patterns = patterns['special_structures']
            
            lines = text.split('\n')
            current_chapter = None
            current_section = None
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–ª–∞–≤
                for pattern in chapter_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        current_chapter = {
                            'number': match.group(1),
                            'title': match.group(2).strip(),
                            'line': line_num,
                            'level': 1
                        }
                        structure['chapters'].append(current_chapter)
                        current_section = None
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–¥–µ–ª–æ–≤
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        section_number = match.group(1)
                        section_title = match.group(2).strip()
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
                        level = len(section_number.split('.'))
                        
                        current_section = {
                            'number': section_number,
                            'title': section_title,
                            'line': line_num,
                            'level': level,
                            'chapter': current_chapter['number'] if current_chapter else None
                        }
                        structure['sections'].append(current_section)
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–±–∑–∞—Ü–µ–≤
                for pattern in paragraph_patterns:
                    match = re.match(pattern, line)
                    if match:
                        paragraph = {
                            'text': match.group(1).strip(),
                            'line': line_num,
                            'section': current_section['number'] if current_section else None,
                            'chapter': current_chapter['number'] if current_chapter else None
                        }
                        structure['paragraphs'].append(paragraph)
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                for pattern in special_patterns:
                    match = re.match(pattern, line, re.IGNORECASE)
                    if match:
                        special = {
                            'type': 'table' if '–¢–∞–±–ª–∏—Ü–∞' in line else 'figure' if '–†–∏—Å—É–Ω–æ–∫' in line else 'appendix' if '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' in line else 'other',
                            'number': match.group(1) if match.groups() else None,
                            'line': line_num,
                            'text': line
                        }
                        structure['special_structures'].append(special)
                        break
            
            return structure
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_STRUCTURE] Error extracting document structure: {e}")
            return {'chapters': [], 'sections': [], 'paragraphs': [], 'special_structures': [], 'headers': []}
    
    def _identify_chunk_structure(self, chunk_text: str, structure: Dict[str, Any]) -> Dict[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫ –∫–∞–∫–æ–π –≥–ª–∞–≤–µ/—Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —á–∞–Ω–∫"""
        try:
            result = {'chapter': '', 'section': ''}
            
            if not structure or not chunk_text:
                return result
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞ –≤ —á–∞–Ω–∫–µ
            section_patterns = [
                r'(\d+\.\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+\.\d+)\s+(.+)',
                r'(\d+\.\d+)\s+(.+)',
                r'(\d+)\s+(.+)'
            ]
            
            for pattern in section_patterns:
                match = re.search(pattern, chunk_text)
                if match:
                    section_number = match.group(1)
                    section_title = match.group(2).strip()
                    
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≥–ª–∞–≤—É
                    chapter_num = section_number.split('.')[0]
                    for chapter in structure.get('chapters', []):
                        if chapter['number'] == chapter_num:
                            result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter['title']}"
                            break
                    
                    result['section'] = f"{section_number}. {section_title}"
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª, –∏—â–µ–º –≥–ª–∞–≤—É
            if not result['section']:
                chapter_patterns = [
                    r'–ì–õ–ê–í–ê\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–ì–ª–∞–≤–∞\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–ê–ó–î–ï–õ\s+(\d+)\s*[\.\-]?\s*(.+)',
                    r'–†–∞–∑–¥–µ–ª\s+(\d+)\s*[\.\-]?\s*(.+)'
                ]
                
                for pattern in chapter_patterns:
                    match = re.search(pattern, chunk_text, re.IGNORECASE)
                    if match:
                        chapter_num = match.group(1)
                        chapter_title = match.group(2).strip()
                        result['chapter'] = f"–ì–ª–∞–≤–∞ {chapter_num}. {chapter_title}"
                        break
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [IDENTIFY_CHUNK_STRUCTURE] Error identifying chunk structure: {e}")
            return {'chapter': '', 'section': ''}
    
    def _extract_document_metadata(self, filename: str, document_id: int, file_path: str = None) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
        try:
            import hashlib
            from datetime import datetime
            
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Called with: filename='{filename}', document_id={document_id}, file_path='{file_path}'")
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "doc_id": f"doc_{document_id}",
                "doc_type": "OTHER",
                "doc_number": "",
                "doc_title": filename,
                "edition_year": None,
                "status": "unknown",
                "replaced_by": None,
                "section": None,
                "paragraph": None,
                "page": None,
                "source_path": file_path or "",
                "source_url": None,
                "ingested_at": datetime.now().strftime("%Y-%m-%d"),
                "lang": "ru",
                "tags": [],
                "checksum": ""
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –Ω–æ–º–µ—Ä
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Parsing filename: '{filename}'")
            doc_type, doc_number, edition_year = self._parse_document_name(filename)
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Parsed: doc_type='{doc_type}', doc_number='{doc_number}', edition_year='{edition_year}'")
            metadata["doc_type"] = doc_type
            metadata["doc_number"] = doc_number
            metadata["edition_year"] = edition_year
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π doc_id
            if doc_number and edition_year:
                metadata["doc_id"] = f"{doc_type.lower()}_{doc_number}_{edition_year}"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata["status"] = self._determine_document_status(filename, doc_type, doc_number)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata["tags"] = self._extract_document_tags(doc_type, doc_number, filename)
            
            # –í—ã—á–∏—Å–ª—è–µ–º checksum –µ—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            if file_path:
                metadata["checksum"] = self._calculate_file_checksum(file_path)
            
            return metadata
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_METADATA] Error extracting metadata: {e}")
            return {
                "doc_id": f"doc_{document_id}",
                "doc_type": "OTHER",
                "doc_number": "",
                "doc_title": filename,
                "edition_year": None,
                "status": "unknown",
                "replaced_by": None,
                "section": None,
                "paragraph": None,
                "page": None,
                "source_path": file_path or "",
                "source_url": None,
                "ingested_at": datetime.now().strftime("%Y-%m-%d"),
                "lang": "ru",
                "tags": [],
                "checksum": ""
            }
    
    def _parse_document_name(self, filename: str) -> tuple[str, str, int]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∏–ø–∞, –Ω–æ–º–µ—Ä–∞ –∏ –≥–æ–¥–∞"""
        try:
            import re
            
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            name = filename.replace('.pdf', '').replace('.docx', '').replace('.doc', '')
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            patterns = [
                # –ì–û–°–¢
                (r'–ì–û–°–¢\s+(\d+(?:\.\d+)*)-(\d{4})', 'GOST'),
                (r'–ì–û–°–¢\s+(\d+(?:\.\d+)*)', 'GOST'),
                
                # –°–ü (–°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª)
                (r'–°–ü\s+(\d+(?:\.\d+)*)\.(\d{4})', 'SP'),
                (r'–°–ü\s+(\d+(?:\.\d+)*)', 'SP'),
                
                # –°–ù–∏–ü
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)-(\d{4})', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)\.(\d{4})', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)-(\d{2})(?:\.|$)', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)', 'SNiP'),
                
                # –§–ù–ü
                (r'–§–ù–ü\s+(\d+(?:\.\d+)*)-(\d{4})', 'FNP'),
                (r'–§–ù–ü\s+(\d+(?:\.\d+)*)', 'FNP'),
                
                # –ü–ë (–ü—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
                (r'–ü–ë\s+(\d+(?:\.\d+)*)-(\d{4})', 'CORP_STD'),
                (r'–ü–ë\s+(\d+(?:\.\d+)*)', 'CORP_STD'),
                
                # –ê (–ê–ª—å–±–æ–º)
                (r'–ê(\d+(?:\.\d+)*)\.(\d{4})', 'CORP_STD'),
                (r'–ê(\d+(?:\.\d+)*)', 'CORP_STD'),
            ]
            
            for pattern, doc_type in patterns:
                match = re.search(pattern, name, re.IGNORECASE)
                if match:
                    groups = match.groups()
                    if len(groups) == 2:
                        # –ï—Å—Ç—å –≥–æ–¥
                        doc_number = groups[0]
                        year_str = groups[1]
                        # –ï—Å–ª–∏ –≥–æ–¥ –¥–≤—É—Ö–∑–Ω–∞—á–Ω—ã–π, –¥–æ–±–∞–≤–ª—è–µ–º 19 –∏–ª–∏ 20
                        if len(year_str) == 2:
                            year_int = int(year_str)
                            if year_int >= 0 and year_int <= 30:  # 2000-2030
                                edition_year = 2000 + year_int
                            else:  # 1930-1999
                                edition_year = 1900 + year_int
                        else:
                            edition_year = int(year_str)
                        return doc_type, doc_number, edition_year
                    else:
                        # –ù–µ—Ç –≥–æ–¥–∞, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ
                        doc_number = groups[0]
                        year_match = re.search(r'(\d{4})', name)
                        edition_year = int(year_match.group(1)) if year_match else None
                        return doc_type, doc_number, edition_year
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ–¥
            year_match = re.search(r'(\d{4})', name)
            edition_year = int(year_match.group(1)) if year_match else None
            
            return "OTHER", "", edition_year
            
        except Exception as e:
            logger.error(f"‚ùå [PARSE_DOCUMENT_NAME] Error parsing document name: {e}")
            return "OTHER", "", None
    
    def _determine_document_status(self, filename: str, doc_type: str, doc_number: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
            if any(word in filename.lower() for word in ['–æ—Ç–º–µ–Ω–µ–Ω', '–æ—Ç–º–µ–Ω–µ–Ω', '–Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω', 'repealed']):
                return "repealed"
            elif any(word in filename.lower() for word in ['–∑–∞–º–µ–Ω–µ–Ω', '–∑–∞–º–µ–Ω—è–µ—Ç', 'replaced', '–∏–∑–º']):
                return "replaced"
            elif any(word in filename.lower() for word in ['–¥–µ–π—Å—Ç–≤—É—é—â–∏–π', '–∞–∫—Ç—É–∞–ª—å–Ω—ã–π', 'active']):
                return "active"
            else:
                return "unknown"
                
        except Exception as e:
            logger.error(f"‚ùå [DETERMINE_DOCUMENT_STATUS] Error determining status: {e}")
            return "unknown"
    
    def _extract_document_tags(self, doc_type: str, doc_number: str, filename: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            tags = []
            
            # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            type_tags = {
                "GOST": ["–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≥–æ—Å—Ç"],
                "SP": ["—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ"],
                "SNiP": ["—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ"],
                "FNP": ["—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã", "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å"],
                "CORP_STD": ["–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç"]
            }
            
            if doc_type in type_tags:
                tags.extend(type_tags[doc_type])
            
            # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            content_keywords = {
                "—ç–ª–µ–∫—Ç—Ä": ["—ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ", "—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞"],
                "–ø–æ–∂–∞—Ä": ["–ø–æ–∂–∞—Ä–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–ø–æ–∂–∞—Ä"],
                "—Å—Ç—Ä–æ–∏—Ç": ["—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "–±–µ–∑–æ–ø–∞—Å–Ω": ["–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"],
                "–ø—Ä–æ–µ–∫—Ç": ["–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü": ["–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "—Å—Ç–∞–ª—å–Ω": ["—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "–º–µ—Ç–∞–ª–ª–æ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü": ["–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"]
            }
            
            filename_lower = filename.lower()
            for keyword, tag_list in content_keywords.items():
                if keyword in filename_lower:
                    tags.extend(tag_list)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            return list(set(tags))
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_TAGS] Error extracting tags: {e}")
            return []
    
    def _calculate_file_checksum(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHA256 checksum —Ñ–∞–π–ª–∞"""
        try:
            import hashlib
            
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
            
        except Exception as e:
            logger.error(f"‚ùå [CALCULATE_FILE_CHECKSUM] Error calculating checksum: {e}")
            return ""
    
    def _get_document_metadata(self, document_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.db_manager.get_cursor() as cursor:
                cursor.execute("""
                    SELECT id, filename, original_filename, file_path, document_hash, document_type
                    FROM uploaded_documents 
                    WHERE id = %s
                """, (document_id,))
                
                result = cursor.fetchone()
                if result:
                    logger.info(f"üîç [GET_DOCUMENT_METADATA] Raw result: {result}")
                    logger.info(f"üîç [GET_DOCUMENT_METADATA] Result type: {type(result)}")
                    logger.info(f"üîç [GET_DOCUMENT_METADATA] Result length: {len(result) if result else 0}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ result - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂
                    if isinstance(result, tuple) and len(result) >= 6:
                        doc_id, filename, original_filename, file_path, document_hash, document_type = result
                        logger.info(f"üîç [GET_DOCUMENT_METADATA] Retrieved from DB: doc_id={doc_id}, filename={filename}, original_filename={original_filename}, file_path={file_path}")
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º original_filename –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                        return self._extract_document_metadata(original_filename, doc_id, file_path)
                    else:
                        logger.error(f"‚ùå [GET_DOCUMENT_METADATA] Invalid result format: {result}")
                        return self._extract_document_metadata(f"error_doc_{document_id}", document_id)
                else:
                    logger.warning(f"‚ö†Ô∏è [GET_DOCUMENT_METADATA] Document {document_id} not found")
                    return self._extract_document_metadata(f"unknown_doc_{document_id}", document_id)
                    
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_METADATA] Error getting document metadata: {e}")
            return self._extract_document_metadata(f"error_doc_{document_id}", document_id)
    
    def _create_chunk_metadata(self, chunk: Dict[str, Any], document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ö–æ–ø–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunk_metadata = document_metadata.copy()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞ –ø–æ–ª—è
            chunk_metadata.update({
                "section": chunk.get('section', ''),
                "paragraph": self._extract_paragraph_from_chunk(chunk.get('content', '')),
                "page": chunk.get('page', 1),
                "chunk_id": chunk.get('chunk_id', ''),
                "chunk_type": chunk.get('chunk_type', 'paragraph')
            })
            
            return chunk_metadata
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNK_METADATA] Error creating chunk metadata: {e}")
            return document_metadata
    
    def _extract_paragraph_from_chunk(self, content: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–∞"""
        try:
            import re
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraph_patterns = [
                r'(\d+\.\d+\.\d+\.\d+)',  # 5.2.1.1
                r'(\d+\.\d+\.\d+)',       # 5.2.1
                r'(\d+\.\d+)',            # 5.2
                r'–ø\.\s*(\d+\.\d+)',      # –ø.5.2
                r'–ø—É–Ω–∫—Ç\s*(\d+\.\d+)',    # –ø—É–Ω–∫—Ç 5.2
            ]
            
            for pattern in paragraph_patterns:
                match = re.search(pattern, content)
                if match:
                    return match.group(1)
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PARAGRAPH_FROM_CHUNK] Error extracting paragraph: {e}")
            return ""
