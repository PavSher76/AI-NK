import logging
from typing import Dict, Any, List, Optional, Union
from contextlib import contextmanager
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2.pool import SimpleConnectionPool
import time
import random
from functools import wraps

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

def retry_on_connection_error(max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0, exponential_base: float = 2.0):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except (psycopg2.OperationalError, psycopg2.InterfaceError, psycopg2.DatabaseError) as e:
                    last_exception = e
                    
                    if attempt == max_retries:
                        logger.error(f"‚ùå [RETRY] Max retries ({max_retries}) exceeded for {func.__name__}: {e}")
                        raise e
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff –∏ jitter
                    delay = min(base_delay * (exponential_base ** attempt), max_delay)
                    jitter = random.uniform(0.1, 0.3) * delay
                    total_delay = delay + jitter
                    
                    logger.warning(f"‚ö†Ô∏è [RETRY] Attempt {attempt + 1}/{max_retries + 1} failed for {func.__name__}: {e}")
                    logger.info(f"üîÑ [RETRY] Retrying in {total_delay:.2f} seconds...")
                    time.sleep(total_delay)
                    
                except Exception as e:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫ –Ω–µ –¥–µ–ª–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
                    logger.error(f"‚ùå [ERROR] Non-retryable error in {func.__name__}: {e}")
                    raise e
            
            # –≠—Ç–æ—Ç –∫–æ–¥ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω–∏—Ç—å—Å—è, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            if last_exception:
                raise last_exception
                
        return wrapper
    return decorator

class DatabaseManager:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö PostgreSQL —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å"""
    
    def __init__(self, connection_string: str, min_connections: int = 1, max_connections: int = 10, 
                 max_retries: int = 3, retry_delay: float = 1.0):
        self.connection_string = connection_string
        self.min_connections = min_connections
        self.max_connections = max_connections
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        # –°–æ–∑–¥–∞–µ–º –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –∑–∞–ø–∏—Å–∏
        self._write_pool = None
        # –°–æ–∑–¥–∞–µ–º –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è —á—Ç–µ–Ω–∏—è (read-only)
        self._read_pool = None
        
        # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—É–ª–æ–≤
        self._pools_initialized = False
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É–ª—ã —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        self._init_connection_pools()
    
    @retry_on_connection_error(max_retries=5, base_delay=2.0, max_delay=30.0)
    def _init_connection_pools(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–æ–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        try:
            # –ü—É–ª –¥–ª—è –∑–∞–ø–∏—Å–∏
            self._write_pool = SimpleConnectionPool(
                self.min_connections,
                self.max_connections,
                self.connection_string
            )
            
            # –ü—É–ª –¥–ª—è —á—Ç–µ–Ω–∏—è (–æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—É–ª –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —á—Ç–µ–Ω–∏—è)
            self._read_pool = SimpleConnectionPool(
                self.min_connections,
                self.max_connections,
                self.connection_string
            )
            
            self._pools_initialized = True
            logger.info(f"‚úÖ [DB_MANAGER] Connection pools initialized (read: {self.min_connections}-{self.max_connections}, write: {self.min_connections}-{self.max_connections})")
            
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Error initializing connection pools: {e}")
            self._pools_initialized = False
            raise
    
    def _ensure_pools_initialized(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–æ–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        if not self._pools_initialized or not self._write_pool or not self._read_pool:
            logger.warning("‚ö†Ô∏è [DB_MANAGER] Connection pools not initialized, attempting to reinitialize...")
            self._init_connection_pools()
    
    def _recreate_pools(self):
        """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –ø—É–ª–æ–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö"""
        try:
            logger.warning("üîÑ [DB_MANAGER] Recreating connection pools...")
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—É–ª—ã
            if self._write_pool:
                try:
                    self._write_pool.closeall()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [DB_MANAGER] Error closing write pool: {e}")
            
            if self._read_pool:
                try:
                    self._read_pool.closeall()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [DB_MANAGER] Error closing read pool: {e}")
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –ø—É–ª—ã
            self._write_pool = None
            self._read_pool = None
            self._pools_initialized = False
            
            self._init_connection_pools()
            logger.info("‚úÖ [DB_MANAGER] Connection pools recreated successfully")
            
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Error recreating connection pools: {e}")
            raise
    
    @contextmanager
    def get_read_connection(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ —á—Ç–µ–Ω–∏–µ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        connection = None
        attempt = 0
        max_attempts = self.max_retries + 1
        
        while attempt < max_attempts:
            try:
                self._ensure_pools_initialized()
                connection = self._read_pool.getconn()
                yield connection
                return
                
            except (psycopg2.OperationalError, psycopg2.InterfaceError, psycopg2.DatabaseError) as e:
                attempt += 1
                logger.warning(f"‚ö†Ô∏è [READ_CONNECTION] Attempt {attempt}/{max_attempts} failed: {e}")
                
                if connection:
                    try:
                        self._read_pool.putconn(connection, close=True)
                    except Exception as put_error:
                        logger.warning(f"‚ö†Ô∏è [READ_CONNECTION] Error putting connection back: {put_error}")
                    connection = None
                
                if attempt >= max_attempts:
                    logger.error(f"‚ùå [READ_CONNECTION] Max attempts exceeded, recreating pools...")
                    try:
                        self._recreate_pools()
                    except Exception as recreate_error:
                        logger.error(f"‚ùå [READ_CONNECTION] Failed to recreate pools: {recreate_error}")
                    raise e
                
                # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                delay = min(self.retry_delay * (2 ** (attempt - 1)), 30.0)
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"‚ùå [READ_CONNECTION] Non-retryable error: {e}")
                if connection:
                    try:
                        connection.rollback()
                    except Exception as rollback_error:
                        logger.error(f"‚ùå [READ_CONNECTION] Error during rollback: {rollback_error}")
                raise
            finally:
                if connection and attempt < max_attempts:
                    try:
                        self._read_pool.putconn(connection)
                    except Exception as put_error:
                        logger.warning(f"‚ö†Ô∏è [READ_CONNECTION] Error putting connection back in finally: {put_error}")
    
    @contextmanager
    def get_write_connection(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ –∑–∞–ø–∏—Å—å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        connection = None
        attempt = 0
        max_attempts = self.max_retries + 1
        
        while attempt < max_attempts:
            try:
                self._ensure_pools_initialized()
                connection = self._write_pool.getconn()
                yield connection
                return
                
            except (psycopg2.OperationalError, psycopg2.InterfaceError, psycopg2.DatabaseError) as e:
                attempt += 1
                logger.warning(f"‚ö†Ô∏è [WRITE_CONNECTION] Attempt {attempt}/{max_attempts} failed: {e}")
                
                if connection:
                    try:
                        self._write_pool.putconn(connection, close=True)
                    except Exception as put_error:
                        logger.warning(f"‚ö†Ô∏è [WRITE_CONNECTION] Error putting connection back: {put_error}")
                    connection = None
                
                if attempt >= max_attempts:
                    logger.error(f"‚ùå [WRITE_CONNECTION] Max attempts exceeded, recreating pools...")
                    try:
                        self._recreate_pools()
                    except Exception as recreate_error:
                        logger.error(f"‚ùå [WRITE_CONNECTION] Failed to recreate pools: {recreate_error}")
                    raise e
                
                # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                delay = min(self.retry_delay * (2 ** (attempt - 1)), 30.0)
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"‚ùå [WRITE_CONNECTION] Non-retryable error: {e}")
                if connection:
                    try:
                        connection.rollback()
                    except Exception as rollback_error:
                        logger.error(f"‚ùå [WRITE_CONNECTION] Error during rollback: {rollback_error}")
                raise
            finally:
                if connection and attempt < max_attempts:
                    try:
                        self._write_pool.putconn(connection)
                    except Exception as put_error:
                        logger.warning(f"‚ö†Ô∏è [WRITE_CONNECTION] Error putting connection back in finally: {put_error}")
    
    @contextmanager
    def get_read_cursor(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É—Ä—Å–æ—Ä–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ"""
        with self.get_read_connection() as connection:
            cursor = None
            try:
                cursor = connection.cursor(cursor_factory=RealDictCursor)
                yield cursor
            except Exception as e:
                logger.error(f"‚ùå [DB_MANAGER] Error in read cursor: {e}")
                if connection:
                    try:
                        connection.rollback()
                    except Exception as rollback_error:
                        logger.error(f"‚ùå [DB_MANAGER] Error during read cursor rollback: {rollback_error}")
                raise
            finally:
                if cursor:
                    cursor.close()
    
    @contextmanager
    def get_write_cursor(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É—Ä—Å–æ—Ä–∞ –Ω–∞ –∑–∞–ø–∏—Å—å"""
        with self.get_write_connection() as connection:
            cursor = None
            try:
                cursor = connection.cursor(cursor_factory=RealDictCursor)
                yield cursor, connection
            except Exception as e:
                logger.error(f"‚ùå [DB_MANAGER] Error in write cursor: {e}")
                if connection:
                    try:
                        connection.rollback()
                    except Exception as rollback_error:
                        logger.error(f"‚ùå [DB_MANAGER] Error during write cursor rollback: {rollback_error}")
                raise
            finally:
                if cursor:
                    cursor.close()
    
    def execute_read_query(self, query: str, params: Optional[tuple] = None) -> List[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            with self.get_read_cursor() as cursor:
                cursor.execute(query, params or ())
                results = cursor.fetchall()
                return [dict(row) for row in results]
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Error executing read query: {e}")
            logger.error(f"‚ùå [DB_MANAGER] Query: {query}")
            logger.error(f"‚ùå [DB_MANAGER] Params: {params}")
            return []
    
    def execute_write_query(self, query: str, params: Optional[tuple] = None, commit: bool = True) -> Optional[Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∑–∞–ø–∏—Å—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            with self.get_write_cursor() as (cursor, connection):
                cursor.execute(query, params or ())
                if commit:
                    connection.commit()
                # –î–ª—è INSERT ... RETURNING –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—Å–µ–≥–¥–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                try:
                    result = cursor.fetchone()
                    logger.info(f"üîç [EXECUTE_WRITE_QUERY] fetchone() result: {result}, type: {type(result)}")
                    return result
                except Exception as e:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –æ–±—ã—á–Ω—ã—Ö INSERT –±–µ–∑ RETURNING)
                    logger.info(f"üîç [EXECUTE_WRITE_QUERY] fetchone() failed: {e}")
                    return None
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Error executing write query: {e}")
            logger.error(f"‚ùå [DB_MANAGER] Query: {query}")
            logger.error(f"‚ùå [DB_MANAGER] Params: {params}")
            raise
    
    def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ
            with self.get_read_connection() as read_conn:
                with read_conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    read_ok = cursor.fetchone()[0] == 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ –∑–∞–ø–∏—Å—å
            with self.get_write_connection() as write_conn:
                with write_conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    write_ok = cursor.fetchone()[0] == 1
            
            return {
                "status": "healthy" if read_ok and write_ok else "unhealthy",
                "read_connection": "ok" if read_ok else "error",
                "write_connection": "ok" if write_ok else "error",
                "read_pool_size": len(self._read_pool._pool) if self._read_pool else 0,
                "write_pool_size": len(self._write_pool._pool) if self._write_pool else 0
            }
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    def close_all_connections(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        try:
            if self._read_pool:
                self._read_pool.closeall()
            if self._write_pool:
                self._write_pool.closeall()
            logger.info("‚úÖ [DB_MANAGER] All connections closed")
        except Exception as e:
            logger.error(f"‚ùå [DB_MANAGER] Error closing connections: {e}")
    
    # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    def get_connection(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        logger.warning("‚ö†Ô∏è [DB_MANAGER] Using deprecated get_connection() method. Use get_read_connection() or get_write_connection() instead.")
        return self._write_pool.getconn() if self._write_pool else None
    
    def get_cursor(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        logger.warning("‚ö†Ô∏è [DB_MANAGER] Using deprecated get_cursor() method. Use get_read_cursor() or get_write_cursor() instead.")
        connection = self.get_connection()
        if connection:
            return connection.cursor(cursor_factory=RealDictCursor)
        return None
    
    def save_document_to_db(self, document_id: int, filename: str, original_filename: str, 
                           file_type: str, file_size: int, document_hash: str, 
                           category: str, document_type: str) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ö–µ—à–µ–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ)
            existing_docs = self.execute_read_query("""
                SELECT id FROM uploaded_documents 
                WHERE document_hash = %s
            """, (document_hash,))
            
            if existing_docs:
                raise Exception("Document with this content already exists")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ –∑–∞–ø–∏—Å—å)
            logger.info(f"üîç [SAVE_DOCUMENT] Attempting to save document with ID: {document_id}")
            result = self.execute_write_query("""
                INSERT INTO uploaded_documents 
                (id, filename, original_filename, file_type, file_size, document_hash, 
                 category, document_type, processing_status)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, 'pending')
                RETURNING id
            """, (
                document_id,
                filename,
                original_filename,
                file_type,
                file_size,
                document_hash,
                category,
                document_type
            ))
            
            logger.info(f"üîç [SAVE_DOCUMENT] Query result: {result}, type: {type(result)}")
            
            # –î–ª—è INSERT ... RETURNING result –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å RealDictRow —Å ID
            if result:
                # RealDictRow –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å
                if hasattr(result, 'get'):
                    saved_id = result.get('id', document_id)
                elif hasattr(result, '__getitem__'):
                    saved_id = result[0] if len(result) > 0 else document_id
                else:
                    saved_id = document_id
                logger.info(f"‚úÖ [SAVE_DOCUMENT] Document saved with ID from result: {saved_id}")
            else:
                # Fallback –Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π document_id
                saved_id = document_id
                logger.warning(f"‚ö†Ô∏è [SAVE_DOCUMENT] No result from query, using fallback ID: {saved_id}")
            return saved_id
            
        except Exception as e:
            logger.error(f"‚ùå [SAVE_DOCUMENT] Error saving document: {e}")
            raise

    def update_document_status(self, document_id: int, status: str, error_message: str = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            if error_message:
                self.execute_write_query("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s, processing_error = %s
                    WHERE id = %s
                """, (status, error_message, document_id))
            else:
                self.execute_write_query("""
                    UPDATE uploaded_documents 
                    SET processing_status = %s, processing_error = NULL
                    WHERE id = %s
                """, (status, document_id))
            
            logger.info(f"‚úÖ [UPDATE_STATUS] Document {document_id} status updated to: {status}")
            
        except Exception as e:
            logger.error(f"‚ùå [UPDATE_STATUS] Error updating document {document_id} status: {e}")
            raise

    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ)"""
        try:
            documents = self.execute_read_query("""
                SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                       ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                       COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                FROM uploaded_documents ud
                LEFT JOIN (
                    SELECT document_id, COUNT(*) as chunks_count 
                    FROM normative_chunks 
                    GROUP BY document_id
                ) chunk_counts ON ud.id = chunk_counts.document_id
                ORDER BY ud.upload_date DESC
            """)
            
            result = []
            for doc in documents:
                result.append({
                    'id': doc['id'],
                    'title': doc['original_filename'],
                    'original_filename': doc['original_filename'],
                    'filename': doc['original_filename'],
                    'category': doc['category'],
                    'status': doc['processing_status'],
                    'processing_status': doc['processing_status'],
                    'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                    'file_size': doc['file_size'],
                    'token_count': doc['token_count'],
                    'vector_indexed': doc['processing_status'] == 'completed',
                    'chunks_count': doc['chunks_count']
                })
            
            logger.info(f"üìã [GET_DOCUMENTS] Retrieved {len(result)} documents")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []

    def get_documents_from_uploaded(self, document_type: str = 'all') -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã uploaded_documents (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ)"""
        try:
            documents = self.execute_read_query("""
                SELECT ud.id, ud.original_filename, ud.category, ud.processing_status, ud.upload_date, 
                       ud.file_size, COALESCE(ud.token_count, 0) as token_count,
                       COALESCE(chunk_counts.chunks_count, 0) as chunks_count
                FROM uploaded_documents ud
                LEFT JOIN (
                    SELECT document_id, COUNT(*) as chunks_count 
                    FROM normative_chunks 
                    GROUP BY document_id
                ) chunk_counts ON ud.id = chunk_counts.document_id
                WHERE ud.category = %s OR %s = 'all'
                ORDER BY ud.upload_date DESC
            """, (document_type, document_type))
            
            result = []
            for doc in documents:
                result.append({
                    'id': doc['id'],
                    'title': doc['original_filename'],
                    'original_filename': doc['original_filename'],
                    'filename': doc['original_filename'],
                    'category': doc['category'],
                    'status': doc['processing_status'],
                    'processing_status': doc['processing_status'],
                    'upload_date': doc['upload_date'].isoformat() if doc['upload_date'] else None,
                    'file_size': doc['file_size'],
                    'token_count': doc['token_count'],
                    'vector_indexed': doc['processing_status'] == 'completed',
                    'chunks_count': doc['chunks_count']
                })
            
            logger.info(f"üìã [GET_DOCUMENTS_FROM_UPLOADED] Retrieved {len(result)} documents of type '{document_type}'")
            return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS_FROM_UPLOADED] Error getting documents: {e}")
            return []

    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ)"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_results = self.execute_read_query("""
                SELECT original_filename 
                FROM uploaded_documents 
                WHERE id = %s
            """, (document_id,))
            
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            import re
            original_filename = document_results[0]['original_filename'] if document_results else f"Document_{document_id}"
            document_title = re.sub(r'\.(pdf|txt|doc|docx)$', '', original_filename, flags=re.IGNORECASE)
            logger.info(f"üîç [GET_DOCUMENT_CHUNKS] Document title: {document_title}")
            
            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.execute_read_query("""
                SELECT chunk_id, content, chapter as section_title, page_number as page, section
                FROM normative_chunks
                WHERE document_id = %s
                ORDER BY page_number, chunk_id
            """, (document_id,))
            
            result = []
            for chunk in chunks:
                result.append({
                    'chunk_id': chunk['chunk_id'],
                    'content': chunk['content'],
                    'section_title': chunk['section_title'],
                    'chunk_type': 'paragraph',  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∏–ø –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    'page': chunk['page'],
                    'section': chunk['section'],
                    'document_title': document_title  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                })
            
            logger.info(f"üìã [GET_DOCUMENT_CHUNKS] Retrieved {len(result)} chunks for document {document_id} ({document_title})")
            return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting chunks for document {document_id}: {e}")
            return []

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ)"""
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ
            total_docs_result = self.execute_read_query("SELECT COUNT(*) as total_documents FROM uploaded_documents")
            total_docs = total_docs_result[0]['total_documents'] if total_docs_result else 0
            
            total_chunks_result = self.execute_read_query("SELECT COUNT(*) as total_chunks FROM normative_chunks")
            total_chunks = total_chunks_result[0]['total_chunks'] if total_chunks_result else 0
            
            pending_docs_result = self.execute_read_query("SELECT COUNT(*) as pending_docs FROM uploaded_documents WHERE processing_status = 'pending'")
            pending_docs = pending_docs_result[0]['pending_docs'] if pending_docs_result else 0
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            total_tokens_result = self.execute_read_query("SELECT COALESCE(SUM(token_count), 0) as total_tokens FROM uploaded_documents")
            total_tokens = total_tokens_result[0]['total_tokens'] if total_tokens_result else 0
            
            stats = {
                'total_documents': total_docs,
                'total_chunks': total_chunks,
                'pending_documents': pending_docs,
                'total_tokens': total_tokens
            }
            
            logger.info(f"üìä [GET_STATS] Retrieved stats: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå [GET_STATS] Error getting stats: {e}")
            return {
                'total_documents': 0,
                'total_chunks': 0,
                'pending_documents': 0,
                'total_tokens': 0
            }
    
    def get_pending_documents_for_indexing(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–∂–∏–¥–∞—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        try:
            pending_docs = self.execute_read_query("""
                SELECT id, original_filename, category, processing_status, upload_date
                FROM uploaded_documents 
                WHERE processing_status IN ('pending', 'failed')
                ORDER BY upload_date ASC
            """)
            
            logger.info(f"üìã [PENDING_DOCS] Found {len(pending_docs)} documents pending indexing")
            return pending_docs
            
        except Exception as e:
            logger.error(f"‚ùå [PENDING_DOCS] Error getting pending documents: {e}")
            return []
    
    def mark_document_for_retry(self, document_id: int, error_message: str = None):
        """–ü–æ–º–µ—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        try:
            self.execute_write_query("""
                UPDATE uploaded_documents 
                SET processing_status = 'pending', 
                    processing_error = %s,
                    retry_count = COALESCE(retry_count, 0) + 1,
                    last_retry_attempt = NOW()
                WHERE id = %s
            """, (error_message, document_id))
            
            logger.info(f"üîÑ [RETRY_MARK] Document {document_id} marked for retry")
            
        except Exception as e:
            logger.error(f"‚ùå [RETRY_MARK] Error marking document {document_id} for retry: {e}")
            raise
    
    def get_documents_with_failed_indexing(self, max_retries: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω–µ—É–¥–∞—á–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏"""
        try:
            failed_docs = self.execute_read_query("""
                SELECT id, original_filename, category, processing_status, processing_error, 
                       COALESCE(retry_count, 0) as retry_count, last_retry_attempt
                FROM uploaded_documents 
                WHERE processing_status = 'failed' 
                  AND COALESCE(retry_count, 0) < %s
                ORDER BY last_retry_attempt ASC NULLS FIRST, upload_date ASC
            """, (max_retries,))
            
            logger.info(f"üîÑ [FAILED_DOCS] Found {len(failed_docs)} documents for retry (max_retries={max_retries})")
            return failed_docs
            
        except Exception as e:
            logger.error(f"‚ùå [FAILED_DOCS] Error getting failed documents: {e}")
            return []
    
    def update_indexing_progress(self, document_id: int, progress_percent: int, status_message: str = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            self.execute_write_query("""
                UPDATE uploaded_documents 
                SET processing_status = 'indexing',
                    indexing_progress = %s,
                    processing_error = %s,
                    last_processing_update = NOW()
                WHERE id = %s
            """, (progress_percent, status_message, document_id))
            
            logger.debug(f"üìä [PROGRESS] Document {document_id} indexing progress: {progress_percent}%")
            
        except Exception as e:
            logger.error(f"‚ùå [PROGRESS] Error updating progress for document {document_id}: {e}")
            raise
