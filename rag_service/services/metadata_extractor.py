import logging
import re
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Tuple

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class MetadataExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        pass
    
    def extract_document_metadata(self, filename: str, document_id: int, file_path: str = None) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
        try:
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Called with: filename='{filename}', document_id={document_id}, file_path='{file_path}'")
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "doc_id": f"doc_{document_id}",
                "doc_type": "OTHER",
                "doc_number": "",
                "doc_title": filename,
                "edition_year": None,
                "status": "unknown",
                "replaced_by": None,
                "section": None,
                "paragraph": None,
                "page": None,
                "source_path": file_path or "",
                "source_url": None,
                "ingested_at": datetime.now().strftime("%Y-%m-%d"),
                "lang": "ru",
                "tags": [],
                "checksum": ""
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –Ω–æ–º–µ—Ä
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Parsing filename: '{filename}'")
            doc_type, doc_number, edition_year = self._parse_document_name(filename)
            logger.info(f"üîç [EXTRACT_DOCUMENT_METADATA] Parsed: doc_type='{doc_type}', doc_number='{doc_number}', edition_year='{edition_year}'")
            metadata["doc_type"] = doc_type
            metadata["doc_number"] = doc_number
            metadata["edition_year"] = edition_year
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π doc_id
            if doc_number and edition_year:
                metadata["doc_id"] = f"{doc_type.lower()}_{doc_number}_{edition_year}"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata["status"] = self._determine_document_status(filename, doc_type, doc_number)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata["tags"] = self._extract_document_tags(doc_type, doc_number, filename)
            
            # –í—ã—á–∏—Å–ª—è–µ–º checksum –µ—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            if file_path:
                metadata["checksum"] = self._calculate_file_checksum(file_path)
            
            return metadata
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_METADATA] Error extracting metadata: {e}")
            return {
                "doc_id": f"doc_{document_id}",
                "doc_type": "OTHER",
                "doc_number": "",
                "doc_title": filename,
                "edition_year": None,
                "status": "unknown",
                "replaced_by": None,
                "section": None,
                "paragraph": None,
                "page": None,
                "source_path": file_path or "",
                "source_url": None,
                "ingested_at": datetime.now().strftime("%Y-%m-%d"),
                "lang": "ru",
                "tags": [],
                "checksum": ""
            }
    
    def _parse_document_name(self, filename: str) -> Tuple[str, str, int]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∏–ø–∞, –Ω–æ–º–µ—Ä–∞ –∏ –≥–æ–¥–∞"""
        try:
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            name = filename.replace('.pdf', '').replace('.docx', '').replace('.doc', '')
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            patterns = [
                # –ì–û–°–¢
                (r'–ì–û–°–¢\s+(\d+(?:\.\d+)*)-(\d{4})', 'GOST'),
                (r'–ì–û–°–¢\s+(\d+(?:\.\d+)*)', 'GOST'),
                
                # –°–ü (–°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª)
                (r'–°–ü\s+(\d+(?:\.\d+)*)\.(\d{4})', 'SP'),
                (r'–°–ü\s+(\d+(?:\.\d+)*)', 'SP'),
                
                # –°–ù–∏–ü
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)-(\d{4})', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)\.(\d{4})', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)-(\d{2})(?:\.|$)', 'SNiP'),
                (r'–°–ù–∏–ü\s+(\d+(?:\.\d+)*)', 'SNiP'),
                
                # –§–ù–ü
                (r'–§–ù–ü\s+(\d+(?:\.\d+)*)-(\d{4})', 'FNP'),
                (r'–§–ù–ü\s+(\d+(?:\.\d+)*)', 'FNP'),
                
                # –ü–ë (–ü—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
                (r'–ü–ë\s+(\d+(?:\.\d+)*)-(\d{4})', 'CORP_STD'),
                (r'–ü–ë\s+(\d+(?:\.\d+)*)', 'CORP_STD'),
                
                # –ê (–ê–ª—å–±–æ–º)
                (r'–ê(\d+(?:\.\d+)*)\.(\d{4})', 'CORP_STD'),
                (r'–ê(\d+(?:\.\d+)*)\.(\d{2})', 'CORP_STD'),
                (r'–ê(\d+(?:\.\d+)*)', 'CORP_STD'),
            ]
            
            for pattern, doc_type in patterns:
                match = re.search(pattern, name, re.IGNORECASE)
                if match:
                    groups = match.groups()
                    if len(groups) == 2:
                        # –ï—Å—Ç—å –≥–æ–¥
                        doc_number = groups[0]
                        year_str = groups[1]
                        # –ï—Å–ª–∏ –≥–æ–¥ –¥–≤—É—Ö–∑–Ω–∞—á–Ω—ã–π, –¥–æ–±–∞–≤–ª—è–µ–º 19 –∏–ª–∏ 20
                        if len(year_str) == 2:
                            year_int = int(year_str)
                            if year_int >= 0 and year_int <= 30:  # 2000-2030
                                edition_year = 2000 + year_int
                            else:  # 1930-1999
                                edition_year = 1900 + year_int
                        else:
                            edition_year = int(year_str)
                        return doc_type, doc_number, edition_year
                    else:
                        # –ù–µ—Ç –≥–æ–¥–∞, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ
                        doc_number = groups[0]
                        year_match = re.search(r'(\d{4})', name)
                        edition_year = int(year_match.group(1)) if year_match else None
                        return doc_type, doc_number, edition_year
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ–¥
            year_match = re.search(r'(\d{4})', name)
            edition_year = int(year_match.group(1)) if year_match else None
            
            return "OTHER", "", edition_year
            
        except Exception as e:
            logger.error(f"‚ùå [PARSE_DOCUMENT_NAME] Error parsing document name: {e}")
            return "OTHER", "", None
    
    def _determine_document_status(self, filename: str, doc_type: str, doc_number: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
            if any(word in filename.lower() for word in ['–æ—Ç–º–µ–Ω–µ–Ω', '–æ—Ç–º–µ–Ω–µ–Ω', '–Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω', 'repealed']):
                return "repealed"
            elif any(word in filename.lower() for word in ['–∑–∞–º–µ–Ω–µ–Ω', '–∑–∞–º–µ–Ω—è–µ—Ç', 'replaced', '–∏–∑–º']):
                return "replaced"
            elif any(word in filename.lower() for word in ['–¥–µ–π—Å—Ç–≤—É—é—â–∏–π', '–∞–∫—Ç—É–∞–ª—å–Ω—ã–π', 'active']):
                return "active"
            else:
                return "unknown"
                
        except Exception as e:
            logger.error(f"‚ùå [DETERMINE_DOCUMENT_STATUS] Error determining status: {e}")
            return "unknown"
    
    def _extract_document_tags(self, doc_type: str, doc_number: str, filename: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            tags = []
            
            # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            type_tags = {
                "GOST": ["–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≥–æ—Å—Ç"],
                "SP": ["—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ"],
                "SNiP": ["—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ"],
                "FNP": ["—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã", "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å"],
                "CORP_STD": ["–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç"]
            }
            
            if doc_type in type_tags:
                tags.extend(type_tags[doc_type])
            
            # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            content_keywords = {
                "—ç–ª–µ–∫—Ç—Ä": ["—ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ", "—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞"],
                "–ø–æ–∂–∞—Ä": ["–ø–æ–∂–∞—Ä–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–ø–æ–∂–∞—Ä"],
                "—Å—Ç—Ä–æ–∏—Ç": ["—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "–±–µ–∑–æ–ø–∞—Å–Ω": ["–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"],
                "–ø—Ä–æ–µ–∫—Ç": ["–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü": ["–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "—Å—Ç–∞–ª—å–Ω": ["—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "–º–µ—Ç–∞–ª–ª–æ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
                "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü": ["–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"]
            }
            
            filename_lower = filename.lower()
            for keyword, tag_list in content_keywords.items():
                if keyword in filename_lower:
                    tags.extend(tag_list)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            return list(set(tags))
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_DOCUMENT_TAGS] Error extracting tags: {e}")
            return []
    
    def _calculate_file_checksum(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHA256 checksum —Ñ–∞–π–ª–∞"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
            
        except Exception as e:
            logger.error(f"‚ùå [CALCULATE_FILE_CHECKSUM] Error calculating checksum: {e}")
            return ""
    
    def create_chunk_metadata(self, chunk: Dict[str, Any], document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ö–æ–ø–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunk_metadata = document_metadata.copy()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞ –ø–æ–ª—è
            chunk_metadata.update({
                "section": chunk.get('section', ''),
                "paragraph": self._extract_paragraph_from_chunk(chunk.get('content', '')),
                "page": chunk.get('page', 1),
                "chunk_id": chunk.get('chunk_id', ''),
                "chunk_type": chunk.get('chunk_type', 'paragraph')
            })
            
            return chunk_metadata
            
        except Exception as e:
            logger.error(f"‚ùå [CREATE_CHUNK_METADATA] Error creating chunk metadata: {e}")
            return document_metadata
    
    def _extract_paragraph_from_chunk(self, content: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–∞"""
        try:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraph_patterns = [
                r'(\d+\.\d+\.\d+\.\d+)',  # 5.2.1.1
                r'(\d+\.\d+\.\d+)',       # 5.2.1
                r'(\d+\.\d+)',            # 5.2
                r'–ø\.\s*(\d+\.\d+)',      # –ø.5.2
                r'–ø—É–Ω–∫—Ç\s*(\d+\.\d+)',    # –ø—É–Ω–∫—Ç 5.2
            ]
            
            for pattern in paragraph_patterns:
                match = re.search(pattern, content)
                if match:
                    return match.group(1)
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå [EXTRACT_PARAGRAPH_FROM_CHUNK] Error extracting paragraph: {e}")
            return ""
