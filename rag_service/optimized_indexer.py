"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã (RAG Base)
–í–∫–ª—é—á–∞–µ—Ç —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
"""

import os
import json
import logging
import hashlib
import re
from typing import List, Dict, Any, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue, Range
import numpy as np
# from sentence_transformers import SentenceTransformer  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
import tiktoken

logger = logging.getLogger(__name__)

class DocumentType(Enum):
    """–¢–∏–ø—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    GOST = "gost"
    SP = "sp"
    SNIP = "snip"
    TR = "tr"
    STU = "stu"
    METHODOLOGY = "methodology"
    CORPORATE = "corporate"

class DocumentStage(Enum):
    """–°—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    PD = "pd"  # –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    RD = "rd"  # –†–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

class DocumentMark(Enum):
    """–ú–∞—Ä–∫–∏ –∫–æ–º–ø–ª–µ–∫—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    AR = "ar"  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
    KR = "kr"  # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
    EO = "eo"  # –≠–ª–µ–∫—Ç—Ä–æ–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ
    VK = "vk"  # –í–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ –∏ –∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è
    OV = "ov"  # –û—Ç–æ–ø–ª–µ–Ω–∏–µ –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è
    SS = "ss"  # –°–µ—Ç–∏ —Å–≤—è–∑–∏
    AS = "as"  # –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
    OTHER = "other"

class ContentTag(Enum):
    """–¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    FORMATTING = "–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ"
    SIGNATURES = "–ø–æ–¥–ø–∏—Å–∏"
    STRUCTURE = "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞"
    TEXT = "—Ç–µ–∫—Å—Ç"
    GRAPHICS = "–≥—Ä–∞—Ñ–∏–∫–∞"
    TABLES = "—Ç–∞–±–ª–∏—Ü—ã"
    CALCULATIONS = "—Ä–∞—Å—á–µ—Ç—ã"
    REQUIREMENTS = "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"
    RECOMMENDATIONS = "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
    EXAMPLES = "–ø—Ä–∏–º–µ—Ä—ã"
    NOTES = "–ø—Ä–∏–º–µ—á–∞–Ω–∏—è"

@dataclass
class UnifiedNormativeStructure:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    clause_id: str  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—É–Ω–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ì–û–°–¢ 21.101-2020 –ø.6.5")
    section_title: str  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
    full_text: str  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—É–Ω–∫—Ç–∞
    tags: List[str]  # –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    document_type: DocumentType  # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
    document_number: str  # –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    clause_number: str  # –ù–æ–º–µ—Ä –ø—É–Ω–∫—Ç–∞
    importance_level: int  # –£—Ä–æ–≤–µ–Ω—å –≤–∞–∂–Ω–æ—Å—Ç–∏ (1-5)
    metadata: Dict[str, Any]  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

@dataclass
class SemanticChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    chunk_id: str
    clause_id: str
    content: str
    chunk_type: str  # "requirement", "recommendation", "note", "example"
    semantic_context: str  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    tags: List[str]
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

class OptimizedNormativeIndexer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã"""
    
    def __init__(self, db_conn, qdrant_client, embedding_model):
        self.db_conn = db_conn
        self.qdrant_client = qdrant_client
        self.embedding_model = embedding_model
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
        self.MAX_CHUNK_TOKENS = 300  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        self.MIN_CHUNK_TOKENS = 50   # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        self.OVERLAP_TOKENS = 30     # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
        self.VECTOR_COLLECTION = "optimized_normative_documents"
        self.METADATA_COLLECTION = "normative_metadata"
        
        logger.info("üöÄ [OPTIMIZED_INDEXER] Initialized OptimizedNormativeIndexer")
    
    def extract_clause_id(self, text: str, document_info: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ clause_id –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        doc_type = document_info.get('document_type', '').upper()
        doc_number = document_info.get('document_number', '')
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–∞
        patterns = [
            r'–ø\.\s*(\d+(?:\.\d+)*)',  # –ø. 1.2.3
            r'–ø—É–Ω–∫—Ç\s*(\d+(?:\.\d+)*)',  # –ø—É–Ω–∫—Ç 1.2.3
            r'—Å—Ç–∞—Ç—å—è\s*(\d+)',  # —Å—Ç–∞—Ç—å—è 5
            r'—Ä–∞–∑–¥–µ–ª\s*(\d+)',  # —Ä–∞–∑–¥–µ–ª 5
            r'(\d+\.\d+(?:\.\d+)*)',  # 1.2.3
            r'(\d+\.\d+)',  # 1.2
            r'(\d+)',  # 1
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                clause_number = match.group(1)
                return f"{doc_type} {doc_number} –ø.{clause_number}"
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º —Ö–µ—à
        text_hash = hashlib.md5(text[:100].encode()).hexdigest()[:8]
        return f"{doc_type} {doc_number} –ø.{text_hash}"
    
    def detect_content_tags(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        tags = []
        text_lower = text.lower()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        if any(word in text_lower for word in ['–ø–æ–¥–ø–∏—Å—å', '–ø–æ–¥–ø–∏—Å–∞–Ω', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω', '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω']):
            tags.append(ContentTag.SIGNATURES.value)
        
        if any(word in text_lower for word in ['–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ', '—Ñ–æ—Ä–º–∞—Ç', '—à—Ä–∏—Ñ—Ç', '—Ä–∞–∑–º–µ—Ä']):
            tags.append(ContentTag.FORMATTING.value)
        
        if any(word in text_lower for word in ['—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '—Å–æ—Å—Ç–∞–≤', '—Ä–∞–∑–¥–µ–ª', '–≥–ª–∞–≤–∞']):
            tags.append(ContentTag.STRUCTURE.value)
        
        if any(word in text_lower for word in ['—Ç–∞–±–ª–∏—Ü–∞', '—Ç–∞–±–ª.', '—Ç–∞–±.']):
            tags.append(ContentTag.TABLES.value)
        
        if any(word in text_lower for word in ['—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∏—Å.', '—á–µ—Ä—Ç–µ–∂', '—Å—Ö–µ–º–∞']):
            tags.append(ContentTag.GRAPHICS.value)
        
        if any(word in text_lower for word in ['—Ä–∞—Å—á–µ—Ç', '–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ', '—Ñ–æ—Ä–º—É–ª–∞']):
            tags.append(ContentTag.CALCULATIONS.value)
        
        if any(word in text_lower for word in ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è']):
            tags.append(ContentTag.REQUIREMENTS.value)
        
        if any(word in text_lower for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
            tags.append(ContentTag.RECOMMENDATIONS.value)
        
        if any(word in text_lower for word in ['–Ω–∞–ø—Ä–∏–º–µ—Ä', '–ø—Ä–∏–º–µ—Ä', '–æ–±—Ä–∞–∑–µ—Ü']):
            tags.append(ContentTag.EXAMPLES.value)
        
        if any(word in text_lower for word in ['–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '–ø—Ä–∏–º.', '–∑–∞–º–µ—á–∞–Ω–∏–µ']):
            tags.append(ContentTag.NOTES.value)
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ "—Ç–µ–∫—Å—Ç"
        if not tags:
            tags.append(ContentTag.TEXT.value)
        
        return tags
    
    def determine_importance_level(self, text: str, clause_type: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—É–Ω–∫—Ç–∞"""
        text_lower = text.lower()
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if any(word in text_lower for word in ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è', '–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è']):
            return 5
        
        # –í–∞–∂–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if any(word in text_lower for word in ['—Å–ª–µ–¥—É–µ—Ç', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
            return 4
        
        # –û–±—ã—á–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if clause_type in ['requirement', 'requirement']:
            return 3
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if clause_type in ['recommendation', 'recommendation']:
            return 2
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã
        return 1
    
    def semantic_chunking(self, text: str, clause_id: str) -> List[SemanticChunk]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–º—ã—Å–ª–æ–≤—ã—Ö –∞–±–∑–∞—Ü–µ–≤"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            if not paragraph.strip():
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            chunk_type = self.detect_chunk_type(paragraph)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            semantic_context = self.extract_semantic_context(paragraph, clause_id)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥–∏
            tags = self.detect_content_tags(paragraph)
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
            chunk_id = f"{clause_id}_chunk_{i}"
            chunk = SemanticChunk(
                chunk_id=chunk_id,
                clause_id=clause_id,
                content=paragraph,
                chunk_type=chunk_type,
                semantic_context=semantic_context,
                tags=tags,
                metadata={
                    "paragraph_index": i,
                    "total_paragraphs": len(paragraphs),
                    "token_count": len(self.tokenizer.encode(paragraph)),
                    "created_at": datetime.now().isoformat()
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    def detect_chunk_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —á–∞–Ω–∫–∞"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è']):
            return "requirement"
        
        if any(word in text_lower for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
            return "recommendation"
        
        if any(word in text_lower for word in ['–Ω–∞–ø—Ä–∏–º–µ—Ä', '–ø—Ä–∏–º–µ—Ä', '–æ–±—Ä–∞–∑–µ—Ü']):
            return "example"
        
        if any(word in text_lower for word in ['–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '–ø—Ä–∏–º.', '–∑–∞–º–µ—á–∞–Ω–∏–µ']):
            return "note"
        
        return "text"
    
    def extract_semantic_context(self, text: str, clause_id: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã
        keywords = []
        
        # –ò—â–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        technical_terms = re.findall(r'\b[A-Z–ê-–Ø][a-z–∞-—è]*(?:\s+[A-Z–ê-–Ø][a-z–∞-—è]*)*\b', text)
        keywords.extend(technical_terms[:5])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ç–µ—Ä–º–∏–Ω–æ–≤
        
        # –ò—â–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numbers = re.findall(r'\d+(?:\.\d+)?', text)
        if numbers:
            keywords.append(f"—á–∏—Å–ª–æ–≤—ã–µ_–∑–Ω–∞—á–µ–Ω–∏—è: {len(numbers)}")
        
        # –ò—â–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
        units = re.findall(r'\b(?:–º–º|—Å–º|–º|–∫–º|–∫–≥|—Ç|–ª|–º¬≥|–º¬≤|¬∞C|%|–í—Ç|–∫–í—Ç|–ê|–í|–û–º)\b', text)
        if units:
            keywords.append(f"–µ–¥–∏–Ω–∏—Ü—ã_–∏–∑–º–µ—Ä–µ–Ω–∏—è: {', '.join(set(units))}")
        
        return f"clause_id: {clause_id}, keywords: {', '.join(keywords)}"
    
    def create_metadata_filter(self, document_mark: Optional[str] = None, 
                             document_stage: Optional[str] = None,
                             content_tags: Optional[List[str]] = None) -> Filter:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        conditions = []
        
        if document_mark:
            conditions.append(
                FieldCondition(
                    key="document_mark",
                    match=MatchValue(value=document_mark)
                )
            )
        
        if document_stage:
            conditions.append(
                FieldCondition(
                    key="document_stage",
                    match=MatchValue(value=document_stage)
                )
            )
        
        if content_tags:
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–≥–∞–º (—Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å)
            tag_conditions = []
            for tag in content_tags:
                tag_conditions.append(
                    FieldCondition(
                        key="tags",
                        match=MatchValue(value=tag)
                    )
                )
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OR –ª–æ–≥–∏–∫—É –¥–ª—è —Ç–µ–≥–æ–≤
            conditions.extend(tag_conditions)
        
        return Filter(
            must=conditions
        ) if conditions else None
    
    def index_document(self, document_info: Dict[str, Any], content: str) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            logger.info(f"üìÑ [INDEX] Starting optimized indexing for document: {document_info.get('document_number')}")
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            unified_structure = self.create_unified_structure(document_info, content)
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥
            semantic_chunks = self.semantic_chunking(unified_structure.full_text, unified_structure.clause_id)
            
            # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            self.index_chunks_to_vector_db(semantic_chunks, document_info)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PostgreSQL
            self.save_to_postgresql(unified_structure, semantic_chunks)
            
            logger.info(f"‚úÖ [INDEX] Successfully indexed document with {len(semantic_chunks)} chunks")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX] Error indexing document: {e}")
            return False
    
    def create_unified_structure(self, document_info: Dict[str, Any], content: str) -> UnifiedNormativeStructure:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        clause_id = self.extract_clause_id(content, document_info)
        tags = self.detect_content_tags(content)
        importance_level = self.determine_importance_level(content, document_info.get('clause_type', 'text'))
        
        return UnifiedNormativeStructure(
            clause_id=clause_id,
            section_title=document_info.get('section_title', ''),
            full_text=content,
            tags=tags,
            document_type=DocumentType(document_info.get('document_type', 'gost')),
            document_number=document_info.get('document_number', ''),
            clause_number=document_info.get('clause_number', ''),
            importance_level=importance_level,
            metadata={
                'document_mark': document_info.get('document_mark'),
                'document_stage': document_info.get('document_stage'),
                'page_number': document_info.get('page_number'),
                'created_at': datetime.now().isoformat()
            }
        )
    
    def index_chunks_to_vector_db(self, chunks: List[SemanticChunk], document_info: Dict[str, Any]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîç [VECTOR_DB] Indexing {len(chunks)} chunks to vector database")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.create_vector_collection()
        
        points = []
        for chunk in chunks:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            if self.embedding_model:
                embedding = self.embedding_model.encode(chunk.content).tolist()
            else:
                # Fallback: –ø—Ä–æ—Å—Ç–æ–π —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = self.simple_hash_embedding(chunk.content)
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant
            point = PointStruct(
                id=hash(chunk.chunk_id) % (2**63),  # Qdrant —Ç—Ä–µ–±—É–µ—Ç int64
                vector=embedding,
                payload={
                    "chunk_id": chunk.chunk_id,
                    "clause_id": chunk.clause_id,
                    "content": chunk.content,
                    "chunk_type": chunk.chunk_type,
                    "semantic_context": chunk.semantic_context,
                    "tags": chunk.tags,
                    "document_type": document_info.get('document_type'),
                    "document_number": document_info.get('document_number'),
                    "document_mark": document_info.get('document_mark'),
                    "document_stage": document_info.get('document_stage'),
                    "importance_level": chunk.metadata.get('importance_level', 1),
                    "token_count": chunk.metadata.get('token_count', 0),
                    "created_at": chunk.metadata.get('created_at')
                }
            )
            points.append(point)
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –≤ –±–∞–∑—É
        self.qdrant_client.upsert(
            collection_name=self.VECTOR_COLLECTION,
            points=points
        )
        
        logger.info(f"‚úÖ [VECTOR_DB] Successfully indexed {len(points)} points")
    
    def create_vector_collection(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections = self.qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.VECTOR_COLLECTION not in collection_names:
                logger.info(f"üîß [VECTOR_DB] Creating collection: {self.VECTOR_COLLECTION}")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
                vector_size = 1024  # BGE-M3 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                if not self.embedding_model:
                    vector_size = 768  # Fallback —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                
                self.qdrant_client.create_collection(
                    collection_name=self.VECTOR_COLLECTION,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=Distance.COSINE
                    )
                )
                
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                self.qdrant_client.create_payload_index(
                    collection_name=self.VECTOR_COLLECTION,
                    field_name="document_type",
                    field_schema="keyword"
                )
                
                self.qdrant_client.create_payload_index(
                    collection_name=self.VECTOR_COLLECTION,
                    field_name="tags",
                    field_schema="keyword"
                )
                
                self.qdrant_client.create_payload_index(
                    collection_name=self.VECTOR_COLLECTION,
                    field_name="importance_level",
                    field_schema="integer"
                )
                
                logger.info(f"‚úÖ [VECTOR_DB] Collection {self.VECTOR_COLLECTION} created successfully")
            else:
                logger.info(f"‚ÑπÔ∏è [VECTOR_DB] Collection {self.VECTOR_COLLECTION} already exists")
                
        except Exception as e:
            logger.error(f"‚ùå [VECTOR_DB] Error creating collection: {e}")
            raise
    
    def simple_hash_embedding(self, text: str) -> List[float]:
        """–ü—Ä–æ—Å—Ç–æ–π —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è fallback"""
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à —Ç–µ–∫—Å—Ç–∞
        text_hash = hashlib.sha256(text.encode()).hexdigest()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
        vector = []
        for i in range(0, len(text_hash), 2):
            hex_pair = text_hash[i:i+2]
            vector.append(float(int(hex_pair, 16)) / 255.0)
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        while len(vector) < 768:
            vector.extend(vector[:min(768 - len(vector), len(vector))])
        
        return vector[:768]
    
    def save_to_postgresql(self, unified_structure: UnifiedNormativeStructure, chunks: List[SemanticChunk]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PostgreSQL –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                cursor.execute("""
                    INSERT INTO document_clauses 
                    (clause_id, clause_number, section_title, clause_text, clause_type, importance_level, metadata)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (clause_id) DO UPDATE SET
                    clause_text = EXCLUDED.clause_text,
                    importance_level = EXCLUDED.importance_level,
                    metadata = EXCLUDED.metadata,
                    updated_at = CURRENT_TIMESTAMP
                """, (
                    unified_structure.clause_id,
                    unified_structure.clause_number,
                    unified_structure.section_title,
                    unified_structure.full_text,
                    'requirement',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    unified_structure.importance_level,
                    json.dumps(unified_structure.metadata)
                ))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã (—Ç–µ–≥–∏)
                for tag in unified_structure.tags:
                    cursor.execute("""
                        INSERT INTO clause_attributes 
                        (clause_id, attribute_name, attribute_value, attribute_type)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT DO NOTHING
                    """, (
                        unified_structure.clause_id,
                        'tag',
                        tag,
                        'text'
                    ))
                
                self.db_conn.commit()
                logger.info(f"‚úÖ [POSTGRESQL] Saved unified structure and {len(chunks)} chunks")
                
        except Exception as e:
            logger.error(f"‚ùå [POSTGRESQL] Error saving to PostgreSQL: {e}")
            self.db_conn.rollback()
            raise
    
    def search_with_context_filter(self, query: str, 
                                 document_mark: Optional[str] = None,
                                 document_stage: Optional[str] = None,
                                 content_tags: Optional[List[str]] = None,
                                 limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            logger.info(f"üîç [SEARCH] Contextual search with filters: mark={document_mark}, stage={document_stage}, tags={content_tags}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            if self.embedding_model:
                query_embedding = self.embedding_model.encode(query).tolist()
            else:
                query_embedding = self.simple_hash_embedding(query)
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata_filter = self.create_metadata_filter(document_mark, document_stage, content_tags)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            search_result = self.qdrant_client.search(
                collection_name=self.VECTOR_COLLECTION,
                query_vector=query_embedding,
                query_filter=metadata_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for point in search_result:
                results.append({
                    'chunk_id': point.payload.get('chunk_id'),
                    'clause_id': point.payload.get('clause_id'),
                    'content': point.payload.get('content'),
                    'chunk_type': point.payload.get('chunk_type'),
                    'semantic_context': point.payload.get('semantic_context'),
                    'tags': point.payload.get('tags', []),
                    'importance_level': point.payload.get('importance_level', 1),
                    'similarity_score': point.score,
                    'document_type': point.payload.get('document_type'),
                    'document_number': point.payload.get('document_number')
                })
            
            logger.info(f"‚úÖ [SEARCH] Found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå [SEARCH] Error in contextual search: {e}")
            return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ Qdrant
            collection_info = self.qdrant_client.get_collection(self.VECTOR_COLLECTION)
            vector_count = collection_info.points_count
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ PostgreSQL
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("SELECT COUNT(*) as total_clauses FROM document_clauses")
                clause_count = cursor.fetchone()['total_clauses']
                
                cursor.execute("SELECT COUNT(DISTINCT clause_id) as unique_clauses FROM document_clauses")
                unique_clauses = cursor.fetchone()['unique_clauses']
                
                cursor.execute("SELECT COUNT(*) as total_attributes FROM clause_attributes")
                attribute_count = cursor.fetchone()['total_attributes']
            
            return {
                'vector_database': {
                    'collection_name': self.VECTOR_COLLECTION,
                    'total_vectors': vector_count,
                    'status': 'active'
                },
                'postgresql': {
                    'total_clauses': clause_count,
                    'unique_clauses': unique_clauses,
                    'total_attributes': attribute_count
                },
                'optimization': {
                    'chunk_size': self.MAX_CHUNK_TOKENS,
                    'min_chunk_size': self.MIN_CHUNK_TOKENS,
                    'overlap_size': self.OVERLAP_TOKENS,
                    'semantic_chunking': True,
                    'contextual_filtering': True
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå [STATS] Error getting statistics: {e}")
            return {'error': str(e)}
