import os
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import tiktoken
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –æ—Ç–ª–∞–¥–∫–æ–π
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/app/rag_service.log')
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
logging.getLogger('qdrant_client').setLevel(logging.DEBUG)
logging.getLogger('sentence_transformers').setLevel(logging.DEBUG)
logging.getLogger('psycopg2').setLevel(logging.DEBUG)

app = FastAPI(title="RAG Service for Norms", version="2.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://norms_user:norms_password@norms-db:5432/norms_db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")

logger.info(f"üîß [CONFIG] RAG Service Configuration:")
logger.info(f"üîß [CONFIG] POSTGRES_URL: {POSTGRES_URL}")
logger.info(f"üîß [CONFIG] QDRANT_URL: {QDRANT_URL}")
logger.info(f"üîß [CONFIG] OLLAMA_URL: {OLLAMA_URL}")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
CHUNK_SIZE = 500  # ~400-600 —Ç–æ–∫–µ–Ω–æ–≤
CHUNK_OVERLAP = 75  # ~50-100 —Ç–æ–∫–µ–Ω–æ–≤
MAX_TOKENS = 1000

# –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π
VECTOR_COLLECTION = "normative_documents"
BM25_COLLECTION = "normative_bm25"

logger.info(f"üîß [CONFIG] CHUNK_SIZE: {CHUNK_SIZE}")
logger.info(f"üîß [CONFIG] CHUNK_OVERLAP: {CHUNK_OVERLAP}")
logger.info(f"üîß [CONFIG] VECTOR_COLLECTION: {VECTOR_COLLECTION}")

class ChunkType(Enum):
    TEXT = "text"
    TABLE = "table"
    FIGURE = "figure"
    HEADER = "header"

@dataclass
class NormChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–∞–Ω–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    chunk_id: str
    clause_id: str
    document_id: int
    document_title: str
    chapter: str
    section: str
    page_number: int
    chunk_type: ChunkType
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

class NormRAGService:
    def __init__(self):
        logger.info("üöÄ [INIT] Initializing NormRAGService...")
        self.db_conn = None
        self.qdrant_client = None
        self.embedding_model = None
        self.tokenizer = None
        self.text_splitter = None
        self.connect_services()
        self.initialize_models()
        logger.info("‚úÖ [INIT] NormRAGService initialized successfully")
    
    def connect_services(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîå [CONNECT] Connecting to services...")
        try:
            # PostgreSQL
            logger.info(f"üîå [CONNECT] Connecting to PostgreSQL: {POSTGRES_URL}")
            self.db_conn = psycopg2.connect(POSTGRES_URL)
            logger.info("‚úÖ [CONNECT] Connected to PostgreSQL")
            
            # Qdrant
            logger.info(f"üîå [CONNECT] Connecting to Qdrant: {QDRANT_URL}")
            self.qdrant_client = qdrant_client.QdrantClient(url=QDRANT_URL)
            logger.info("‚úÖ [CONNECT] Connected to Qdrant")
            
        except Exception as e:
            logger.error(f"‚ùå [CONNECT] Service connection error: {e}")
            raise
    
    def initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        logger.info("ü§ñ [MODELS] Initializing models...")
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            logger.info("ü§ñ [MODELS] Initializing tokenizer...")
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
            logger.info("‚úÖ [MODELS] Tokenizer initialized")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å BGE-M3 –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            logger.info("ü§ñ [MODELS] Loading BGE-M3 embedding model...")
            try:
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer('BAAI/bge-m3', device='cpu')
                logger.info("‚úÖ [MODELS] BGE-M3 model loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå [MODELS] Failed to load BGE-M3 model: {e}")
                logger.info("ü§ñ [MODELS] Falling back to simple hash embedding")
                self.embedding_model = None
            
        except Exception as e:
            logger.error(f"‚ùå [MODELS] Model initialization error: {e}")
            raise
    
    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text.split())
    
    def simple_text_split(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π —Å–ø–ª–∏—Ç—Ç–µ—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        if not text.strip():
            return []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_tokens = self.count_tokens(sentence)
            
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
            if sentence_tokens > CHUNK_SIZE:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_tokens = 0
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
                words = sentence.split()
                temp_chunk = ""
                temp_tokens = 0
                
                for word in words:
                    word_tokens = self.count_tokens(word + " ")
                    if temp_tokens + word_tokens > CHUNK_SIZE:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = word + " "
                        temp_tokens = word_tokens
                    else:
                        temp_chunk += word + " "
                        temp_tokens += word_tokens
                
                if temp_chunk:
                    current_chunk = temp_chunk
                    current_tokens = temp_tokens
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > CHUNK_SIZE:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
                    current_tokens = sentence_tokens
                else:
                    current_chunk += sentence + ". "
                    current_tokens += sentence_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def create_chunk_id(self, document_id: int, clause_id: str, chunk_index: int) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID —á–∞–Ω–∫–∞"""
        return f"doc_{document_id}_clause_{clause_id}_chunk_{chunk_index}"
    
    def extract_clause_id(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ clause_id –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–ø. 1.2.3", "—Å—Ç–∞—Ç—å—è 5", "–ì–û–°–¢ 12345-2020 –ø.4.1"
        import re
        patterns = [
            r'–ø\.\s*(\d+(?:\.\d+)*)',  # –ø. 1.2.3
            r'—Å—Ç–∞—Ç—å—è\s*(\d+)',  # —Å—Ç–∞—Ç—å—è 5
            r'–ì–û–°–¢\s*(\d+(?:-\d+)?)\s*–ø\.(\d+(?:\.\d+)*)',  # –ì–û–°–¢ 12345-2020 –ø.4.1
            r'–°–ü\s*(\d+(?:\.\d+)?)\s*–ø\.(\d+(?:\.\d+)*)',  # –°–ü 20.13330.2016 –ø.4.1
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º —Ö–µ—à
        return hashlib.md5(text[:100].encode()).hexdigest()[:8]
    
    def chunk_document(self, document_id: int, document_title: str, content: str, 
                      chapter: str = "", section: str = "", page_number: int = 1) -> List[NormChunk]:
        """–ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üìÑ [CHUNK] Starting document chunking for document ID: {document_id}")
        logger.info(f"üìÑ [CHUNK] Document title: {document_title}")
        logger.info(f"üìÑ [CHUNK] Content length: {len(content)} characters")
        logger.info(f"üìÑ [CHUNK] Page number: {page_number}")
        
        chunks = []
        
        # –ü—Ä–æ—Å—Ç–æ–π —á–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤
        logger.info(f"üìÑ [CHUNK] Performing text splitting...")
        text_chunks = self.simple_text_split(content)
        logger.info(f"üìÑ [CHUNK] Text split into {len(text_chunks)} raw chunks")
        
        for i, chunk_text in enumerate(text_chunks):
            if not chunk_text.strip():
                logger.debug(f"üìÑ [CHUNK] Skipping empty chunk {i}")
                continue
            
            logger.debug(f"üìÑ [CHUNK] Processing chunk {i}: {len(chunk_text)} characters")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
            chunk_type = self.detect_chunk_type(chunk_text)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} type: {chunk_type.value}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º clause_id
            clause_id = self.extract_clause_id(chunk_text)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} clause_id: {clause_id}")
            
            # –°–æ–∑–¥–∞–µ–º ID —á–∞–Ω–∫–∞
            chunk_id = self.create_chunk_id(document_id, clause_id, i)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} ID: {chunk_id}")
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            token_count = self.count_tokens(chunk_text)
            metadata = {
                "document_id": document_id,
                "document_title": document_title,
                "chapter": chapter,
                "section": section,
                "page_number": page_number,
                "chunk_type": chunk_type.value,
                "clause_id": clause_id,
                "chunk_index": i,
                "token_count": token_count,
                "created_at": datetime.now().isoformat()
            }
            
            chunk = NormChunk(
                chunk_id=chunk_id,
                clause_id=clause_id,
                document_id=document_id,
                document_title=document_title,
                chapter=chapter,
                section=section,
                page_number=page_number,
                chunk_type=chunk_type,
                content=chunk_text,
                metadata=metadata
            )
            
            chunks.append(chunk)
            logger.debug(f"üìÑ [CHUNK] Created chunk {i}: {chunk_id} ({token_count} tokens)")
        
        logger.info(f"‚úÖ [CHUNK] Document chunking completed. Created {len(chunks)} chunks")
        return chunks
    
    def detect_chunk_type(self, text: str) -> ChunkType:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —á–∞–Ω–∫–∞"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
        if any(keyword in text_lower for keyword in ['|', '\t', '—Ç–∞–±–ª–∏—Ü–∞', 'table']):
            return ChunkType.TABLE
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if len(text.strip()) < 100 and any(keyword in text_lower for keyword in ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', 'chapter', 'section']):
            return ChunkType.HEADER
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–∏—Å—É–Ω–æ–∫
        if any(keyword in text_lower for keyword in ['—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∏—Å.', 'figure', '–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è']):
            return ChunkType.FIGURE
        
        return ChunkType.TEXT
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        logger.debug(f"üî¢ [EMBED] Creating embedding for text: {len(text)} characters")
        
        if self.embedding_model:
            try:
                logger.debug(f"üî¢ [EMBED] Using BGE-M3 model for embedding")
                # BGE-M3 —Å–æ–∑–¥–∞–µ—Ç 1024-–º–µ—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
                embedding = self.embedding_model.encode(text, normalize_embeddings=True)
                embedding_list = embedding.tolist()
                logger.debug(f"üî¢ [EMBED] BGE-M3 embedding created: {len(embedding_list)} dimensions")
                return embedding_list
            except Exception as e:
                logger.error(f"‚ùå [EMBED] BGE-M3 embedding creation error: {e}")
                logger.info(f"üî¢ [EMBED] Falling back to simple hash embedding")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥—É
        logger.debug(f"üî¢ [EMBED] Using simple hash embedding fallback")
        return self.create_simple_embedding(text)
    
    def create_simple_embedding(self, text: str) -> List[float]:
        """–ü—Ä–æ—Å—Ç–æ–π —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è fallback"""
        logger.debug(f"üî¢ [EMBED] Creating simple hash embedding")
        hash_obj = hashlib.sha256(text.encode())
        hash_bytes = hash_obj.digest()
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 1024-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–∫–∞–∫ —É BGE-M3)
        embedding = []
        for i in range(1024):
            embedding.append(float(hash_bytes[i % len(hash_bytes)]) / 255.0)
        logger.debug(f"üî¢ [EMBED] Simple hash embedding created: {len(embedding)} dimensions")
        return embedding
    
    def index_chunks(self, chunks: List[NormChunk]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏ BM25"""
        logger.info(f"üîó [INDEX] Indexing {len(chunks)} chunks...")
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            for chunk in chunks:
                chunk.embedding = self.create_embedding(chunk.content)
                logger.debug(f"üîó [INDEX] Created embedding for chunk {chunk.chunk_id}")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)
            self.index_to_qdrant(chunks)
            logger.info(f"‚úÖ [INDEX] Qdrant indexing successful")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ PostgreSQL (BM25/keyword –ø–æ–∏—Å–∫)
            self.index_to_postgres(chunks)
            logger.info(f"‚úÖ [INDEX] PostgreSQL indexing successful")
            
            logger.info(f"‚úÖ [INDEX] Successfully indexed {len(chunks)} chunks")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX] Indexing error: {e}")
            return False
    
    def index_to_qdrant(self, chunks: List[NormChunk]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            import requests
            response = requests.get(f"{QDRANT_URL}/collections")
            if response.status_code == 200:
                collections_data = response.json()
                collection_names = [col['name'] for col in collections_data.get('result', {}).get('collections', [])]
                if VECTOR_COLLECTION not in collection_names:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                else:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists.")
            else:
                logger.warning(f"‚ö†Ô∏è [QDRANT] Could not check collections: {response.status_code}")
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
                try:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                except Exception as e:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists or error: {e}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            points = []
            for chunk in chunks:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ö–µ—à –¥–ª—è ID —Ç–æ—á–∫–∏
                point_id = abs(hash(chunk.chunk_id))
                point = PointStruct(
                    id=point_id,
                    vector=chunk.embedding,
                    payload={
                        "chunk_id": chunk.chunk_id,
                        "clause_id": chunk.clause_id,
                        "document_id": chunk.document_id,
                        "document_title": chunk.document_title,
                        "chapter": chunk.chapter,
                        "section": chunk.section,
                        "page_number": chunk.page_number,
                        "chunk_type": chunk.chunk_type.value,
                        "content": chunk.content,
                        "metadata": chunk.metadata
                    }
                )
                points.append(point)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ—á–∫–∏
            self.qdrant_client.upsert(
                collection_name=VECTOR_COLLECTION,
                points=points
            )
            logger.info(f"‚úÖ [QDRANT] Upserted {len(points)} points to Qdrant")
            
        except Exception as e:
            logger.error(f"‚ùå [QDRANT] Qdrant indexing error: {e}")
            raise
    
    def index_to_postgres(self, chunks: List[NormChunk]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ PostgreSQL –¥–ª—è BM25/keyword –ø–æ–∏—Å–∫–∞"""
        logger.info(f"üóÑÔ∏è [POSTGRES] Starting PostgreSQL indexing for {len(chunks)} chunks")
        try:
            with self.db_conn.cursor() as cursor:
                # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —á–∞–Ω–∫–æ–≤ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                logger.info(f"üóÑÔ∏è [POSTGRES] Creating normative_chunks table if not exists...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS normative_chunks (
                        chunk_id VARCHAR(255) PRIMARY KEY,
                        clause_id VARCHAR(255) NOT NULL,
                        document_id INTEGER NOT NULL,
                        document_title VARCHAR(500) NOT NULL,
                        chapter VARCHAR(255),
                        section VARCHAR(255),
                        page_number INTEGER,
                        chunk_type VARCHAR(50) NOT NULL,
                        content TEXT NOT NULL,
                        metadata JSONB,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                logger.info(f"‚úÖ [POSTGRES] Table normative_chunks ready")
                
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                logger.info(f"üóÑÔ∏è [POSTGRES] Creating indexes...")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_document_id ON normative_chunks(document_id);
                    CREATE INDEX IF NOT EXISTS idx_chunks_clause_id ON normative_chunks(clause_id);
                    CREATE INDEX IF NOT EXISTS idx_chunks_chunk_type ON normative_chunks(chunk_type);
                    CREATE INDEX IF NOT EXISTS idx_chunks_content_gin ON normative_chunks USING gin(to_tsvector('russian', content));
                """)
                logger.info(f"‚úÖ [POSTGRES] Indexes created")
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
                logger.info(f"üóÑÔ∏è [POSTGRES] Inserting {len(chunks)} chunks...")
                for i, chunk in enumerate(chunks):
                    logger.debug(f"üóÑÔ∏è [POSTGRES] Inserting chunk {i+1}/{len(chunks)}: {chunk.chunk_id}")
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, document_id, document_title, chapter, section, 
                         page_number, chunk_type, content, metadata)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (chunk_id) DO UPDATE SET
                        content = EXCLUDED.content,
                        metadata = EXCLUDED.metadata
                    """, (
                        chunk.chunk_id, chunk.clause_id, chunk.document_id, 
                        chunk.document_title, chunk.chapter, chunk.section,
                        chunk.page_number, chunk.chunk_type.value, chunk.content,
                        json.dumps(chunk.metadata)
                    ))
                
                self.db_conn.commit()
                logger.info(f"‚úÖ [POSTGRES] Successfully inserted {len(chunks)} chunks into PostgreSQL")
                
        except Exception as e:
            self.db_conn.rollback()
            logger.error(f"‚ùå [POSTGRES] PostgreSQL indexing error: {e}")
            logger.error(f"‚ùå [POSTGRES] Error details: {type(e).__name__}: {str(e)}")
            raise
    
    def hybrid_search(self, query: str, k: int = 8, 
                     document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None,
                     chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25"""
        logger.info(f"üîç [SEARCH] Performing hybrid search for query: '{query}' with k={k}")
        try:
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_results = self.vector_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] Vector search completed. Found {len(vector_results)} results.")
            
            # BM25 –ø–æ–∏—Å–∫
            bm25_results = self.bm25_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] BM25 search completed. Found {len(bm25_results)} results.")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results = self.combine_search_results(vector_results, bm25_results, k)
            logger.info(f"‚úÖ [SEARCH] Combined search results. Total {len(combined_results)} results.")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filtered_results = self.apply_filters(combined_results, document_filter, chapter_filter, chunk_type_filter)
            logger.info(f"‚úÖ [SEARCH] Applied filters. Final {len(filtered_results)} results.")
            
            return filtered_results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå [SEARCH] Hybrid search error: {e}")
            return []
    
    def vector_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant"""
        logger.info(f"üîç [VECTOR] Performing vector search for query: '{query}' with k={k}")
        try:
            query_embedding = self.create_embedding(query)
            logger.debug(f"üîç [VECTOR] Query embedding created: {query_embedding}")
            
            results = self.qdrant_client.search(
                collection_name=VECTOR_COLLECTION,
                query_vector=query_embedding,
                limit=k,
                with_payload=True
            )
            logger.info(f"‚úÖ [VECTOR] Vector search completed. Found {len(results)} results.")
            
            return [
                {
                    "chunk_id": result.payload["chunk_id"],
                    "clause_id": result.payload["clause_id"],
                    "content": result.payload["content"],
                    "document_title": result.payload["document_title"],
                    "chapter": result.payload["chapter"],
                    "section": result.payload["section"],
                    "page_number": result.payload["page_number"],
                    "chunk_type": result.payload["chunk_type"],
                    "score": result.score,
                    "search_type": "vector"
                }
                for result in results
            ]
            
        except Exception as e:
            logger.error(f"‚ùå [VECTOR] Vector search error: {e}")
            return []
    
    def bm25_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 –ø–æ–∏—Å–∫ –≤ PostgreSQL"""
        logger.info(f"üîç [BM25] Performing BM25 search for query: '{query}' with k={k}")
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ PostgreSQL
                cursor.execute("""
                    SELECT 
                        chunk_id, clause_id, content, document_title, 
                        chapter, section, page_number, chunk_type,
                        ts_rank(to_tsvector('russian', content), plainto_tsquery('russian', %s)) as score
                    FROM normative_chunks 
                    WHERE to_tsvector('russian', content) @@ plainto_tsquery('russian', %s)
                    ORDER BY score DESC
                    LIMIT %s
                """, (query, query, k))
                
                results = cursor.fetchall()
                logger.info(f"‚úÖ [BM25] BM25 search completed. Found {len(results)} results.")
                
                return [
                    {
                        "chunk_id": row["chunk_id"],
                        "clause_id": row["clause_id"],
                        "content": row["content"],
                        "document_title": row["document_title"],
                        "chapter": row["chapter"],
                        "section": row["section"],
                        "page_number": row["page_number"],
                        "chunk_type": row["chunk_type"],
                        "score": float(row["score"]),
                        "search_type": "bm25"
                    }
                    for row in results
                ]
                
        except Exception as e:
            logger.error(f"‚ùå [BM25] BM25 search error: {e}")
            return []
    
    def combine_search_results(self, vector_results: List[Dict], bm25_results: List[Dict], k: int) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ BM25 –ø–æ–∏—Å–∫–∞"""
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        combined = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(vector_results):
            chunk_id = result["chunk_id"]
            if chunk_id not in combined:
                combined[chunk_id] = result.copy()
                combined[chunk_id]["combined_score"] = result["score"] * 0.6  # –í–µ—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            else:
                combined[chunk_id]["combined_score"] += result["score"] * 0.6
        
        # –î–æ–±–∞–≤–ª—è–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(bm25_results):
            chunk_id = result["chunk_id"]
            if chunk_id not in combined:
                combined[chunk_id] = result.copy()
                combined[chunk_id]["combined_score"] = result["score"] * 0.4  # –í–µ—Å BM25
            else:
                combined[chunk_id]["combined_score"] += result["score"] * 0.4
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
        sorted_results = sorted(combined.values(), key=lambda x: x["combined_score"], reverse=True)
        
        return sorted_results[:k]
    
    def apply_filters(self, results: List[Dict], document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        filtered_results = results
        
        if document_filter:
            filtered_results = [r for r in filtered_results if document_filter.lower() in r["document_title"].lower()]
            logger.info(f"‚úÖ [FILTER] Document filter applied. {len(filtered_results)} results remaining.")
        
        if chapter_filter:
            filtered_results = [r for r in filtered_results if chapter_filter.lower() in r["chapter"].lower()]
            logger.info(f"‚úÖ [FILTER] Chapter filter applied. {len(filtered_results)} results remaining.")
        
        if chunk_type_filter:
            filtered_results = [r for r in filtered_results if r["chunk_type"] == chunk_type_filter]
            logger.info(f"‚úÖ [FILTER] Chunk type filter applied. {len(filtered_results)} results remaining.")
        
        return filtered_results
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        logger.info("üìä [STATS] Getting service statistics...")
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL
            with self.db_conn.cursor() as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã normative_chunks
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'normative_chunks'
                    );
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    logger.info("üìä [STATS] Table normative_chunks does not exist, returning empty stats")
                    total_chunks = 0
                    total_documents = 0
                    total_clauses = 0
                    chunk_types = {}
                else:
                    cursor.execute("SELECT COUNT(*) FROM normative_chunks")
                    total_chunks = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total chunks in PostgreSQL: {total_chunks}")
                    
                    cursor.execute("SELECT COUNT(DISTINCT document_id) FROM normative_chunks")
                    total_documents = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total unique documents in PostgreSQL: {total_documents}")
                    
                    cursor.execute("SELECT COUNT(DISTINCT clause_id) FROM normative_chunks")
                    total_clauses = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total unique clauses in PostgreSQL: {total_clauses}")
                    
                    cursor.execute("""
                        SELECT chunk_type, COUNT(*) 
                        FROM normative_chunks 
                        GROUP BY chunk_type
                    """)
                    chunk_types = dict(cursor.fetchall())
                    logger.info(f"‚úÖ [STATS] Chunk types distribution: {chunk_types}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ - —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã,
            # –ø–æ—Å–∫–æ–ª—å–∫—É –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            vector_count = total_chunks
            logger.info(f"‚úÖ [STATS] Total vectors available in Qdrant: {vector_count}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ–∫–∞ –ø—É—Å—Ç–∞—è, —Ç–∞–∫ –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ uploaded_documents –≤ –¥—Ä—É–≥–æ–º —Å–µ—Ä–≤–∏—Å–µ)
            category_distribution = {}
            logger.info(f"‚úÖ [STATS] Category distribution: {category_distribution}")
            
            return {
                "total_chunks": total_chunks,
                "total_documents": total_documents,
                "total_clauses": total_clauses,
                "vector_indexed": vector_count,
                "chunk_types": chunk_types,
                "category_distribution": category_distribution,
                "collection_name": VECTOR_COLLECTION,
                "indexing_status": "active" if total_chunks > 0 else "empty"
            }
            
        except Exception as e:
            logger.error(f"‚ùå [STATS] Stats error: {e}")
            return {"error": str(e)}

    def delete_document_indexes(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üóëÔ∏è [DELETE_DOC] Deleting indexes for document ID: {document_id}")
        try:
            deleted_count = 0
            
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º chunk_id –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ Qdrant
            chunk_ids = []
            with self.db_conn.cursor() as cursor:
                cursor.execute("SELECT chunk_id FROM normative_chunks WHERE document_id = %s", (document_id,))
                chunk_ids = [row[0] for row in cursor.fetchall()]
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ Qdrant
            if chunk_ids:
                try:
                    # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
                    self.qdrant_client.delete(
                        collection_name=VECTOR_COLLECTION,
                        points_selector=chunk_ids
                    )
                    qdrant_deleted = len(chunk_ids)
                    deleted_count += qdrant_deleted
                    logger.info(f"‚úÖ [DELETE_DOC] Deleted {qdrant_deleted} vectors from Qdrant for document {document_id}")
                except Exception as e:
                    logger.error(f"‚ùå [DELETE_DOC] Error deleting from Qdrant: {e}")
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ PostgreSQL
            with self.db_conn.cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks WHERE document_id = %s", (document_id,))
                postgresql_deleted = cursor.rowcount
                self.db_conn.commit()
                deleted_count += postgresql_deleted
                logger.info(f"‚úÖ [DELETE_DOC] Deleted {postgresql_deleted} chunks from PostgreSQL for document {document_id}")
            
            logger.info(f"‚úÖ [DELETE_DOC] Deleted {deleted_count} indexes for document {document_id}")
            return deleted_count > 0
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_DOC] Delete document indexes error: {e}")
            return False

    def delete_all_indexes(self) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ (–æ—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã)"""
        logger.info("üóëÔ∏è [DELETE_ALL] Clearing all indexes...")
        try:
            deleted_count = 0
            
            # –û—á–∏—â–∞–µ–º PostgreSQL
            with self.db_conn.cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks")
                postgresql_deleted = cursor.rowcount
                self.db_conn.commit()
                deleted_count += postgresql_deleted
                logger.info(f"‚úÖ [DELETE_ALL] Deleted {postgresql_deleted} chunks from PostgreSQL")
            
            # –û—á–∏—â–∞–µ–º Qdrant
            try:
                self.qdrant_client.delete(
                    collection_name=VECTOR_COLLECTION,
                    points_selector="all"
                )
                logger.info("‚úÖ [DELETE_ALL] Cleared all vectors from Qdrant")
            except Exception as e:
                logger.error(f"‚ùå [DELETE_ALL] Error clearing Qdrant: {e}")
            
            logger.info(f"‚úÖ [DELETE_ALL] Deleted {deleted_count} indexes from system")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_ALL] Delete all indexes error: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
rag_service = NormRAGService()

@app.post("/index")
async def index_document(
    document_id: int = Form(...),
    document_title: str = Form(...),
    content: str = Form(...),
    chapter: str = Form(""),
    section: str = Form(""),
    page_number: int = Form(1)
):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG-—Å–∏—Å—Ç–µ–º—É"""
    logger.info(f"üìÑ [INDEX_DOC] Indexing document ID: {document_id}")
    try:
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
        chunks = rag_service.chunk_document(
            document_id=document_id,
            document_title=document_title,
            content=content,
            chapter=chapter,
            section=section,
            page_number=page_number
        )
        logger.info(f"‚úÖ [INDEX_DOC] Document {document_id} chunked into {len(chunks)} chunks.")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        success = rag_service.index_chunks(chunks)
        
        if success:
            return {
                "status": "success",
                "document_id": document_id,
                "chunks_created": len(chunks),
                "message": f"Successfully indexed {len(chunks)} chunks"
            }
        else:
            raise HTTPException(status_code=500, detail="Indexing failed")
            
    except Exception as e:
        logger.error(f"‚ùå [INDEX_DOC] Indexing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_norms(
    query: str = Form(...),
    k: int = Form(8),
    document_filter: Optional[str] = Form(None),
    chapter_filter: Optional[str] = Form(None),
    chunk_type_filter: Optional[str] = Form(None)
):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    logger.info(f"üîç [SEARCH_NORM] Performing hybrid search for query: '{query}' with k={k}")
    try:
        results = rag_service.hybrid_search(
            query=query,
            k=k,
            document_filter=document_filter,
            chapter_filter=chapter_filter,
            chunk_type_filter=chunk_type_filter
        )
        logger.info(f"‚úÖ [SEARCH_NORM] Hybrid search completed. Found {len(results)} results.")
        
        return {
            "query": query,
            "results_count": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"‚ùå [SEARCH_NORM] Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
    logger.info("üìä [GET_STATS] Getting service statistics...")
    try:
        stats = rag_service.get_stats()
        logger.info(f"‚úÖ [GET_STATS] Service statistics retrieved: {stats}")
        return stats
    except Exception as e:
        logger.error(f"‚ùå [GET_STATS] Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/indexes/document/{document_id}")
async def delete_document_indexes(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üóëÔ∏è [DELETE_DOC_INDEXES] Deleting indexes for document ID: {document_id}")
    try:
        success = rag_service.delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Indexes for document {document_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Document indexes not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOC_INDEXES] Delete document indexes error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/indexes/all")
async def delete_all_indexes():
    """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ (–æ—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã)"""
    logger.info("üóëÔ∏è [DELETE_ALL_INDEXES] Clearing all indexes...")
    try:
        success = rag_service.delete_all_indexes()
        
        if success:
            return {
                "status": "success",
                "message": "All indexes deleted successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to delete indexes")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_ALL_INDEXES] Delete all indexes error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    logger.info("üí™ [HEALTH] Performing health check...")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        rag_service.db_conn.cursor().execute("SELECT 1")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Qdrant —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å
        import requests
        response = requests.get(f"{QDRANT_URL}/collections")
        if response.status_code != 200:
            raise Exception("Qdrant connection failed")
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "embedding_model": "BGE-M3" if rag_service.embedding_model else "simple_hash",
            "services": {
                "postgresql": "connected",
                "qdrant": "connected"
            }
        }
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
