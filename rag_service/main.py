import os
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import tiktoken
import re

# –ò–º–ø–æ—Ä—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω)
# from optimized_indexer import OptimizedNormativeIndexer, DocumentType, DocumentStage, DocumentMark, ContentTag

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –æ—Ç–ª–∞–¥–∫–æ–π
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/app/rag_service.log')
    ]
)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –º–æ–¥–µ–ª—è–º
model_logger = logging.getLogger('rag_model_requests')
model_logger.setLevel(logging.INFO)

# –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
logging.getLogger('qdrant_client').setLevel(logging.INFO)
logging.getLogger('sentence_transformers').setLevel(logging.INFO)
logging.getLogger('psycopg2').setLevel(logging.INFO)

app = FastAPI(title="RAG Service for Norms", version="2.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://norms_user:norms_password@norms-db:5432/norms_db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")

logger.info(f"üîß [CONFIG] RAG Service Configuration:")
logger.info(f"üîß [CONFIG] POSTGRES_URL: {POSTGRES_URL}")
logger.info(f"üîß [CONFIG] QDRANT_URL: {QDRANT_URL}")
logger.info(f"üîß [CONFIG] OLLAMA_URL: {OLLAMA_URL}")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
CHUNK_SIZE = 500  # ~400-600 —Ç–æ–∫–µ–Ω–æ–≤
CHUNK_OVERLAP = 75  # ~50-100 —Ç–æ–∫–µ–Ω–æ–≤
MAX_TOKENS = 1000

# –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π
VECTOR_COLLECTION = "normative_documents"
CHECKABLE_COLLECTION = "checkable_documents"
BM25_COLLECTION = "normative_bm25"

logger.info(f"üîß [CONFIG] CHUNK_SIZE: {CHUNK_SIZE}")
logger.info(f"üîß [CONFIG] CHUNK_OVERLAP: {CHUNK_OVERLAP}")
logger.info(f"üîß [CONFIG] VECTOR_COLLECTION: {VECTOR_COLLECTION}")

class ChunkType(Enum):
    TEXT = "text"
    TABLE = "table"
    FIGURE = "figure"
    HEADER = "header"

@dataclass
class NormChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–∞–Ω–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    chunk_id: str
    clause_id: str
    document_id: int
    document_title: str
    chapter: str
    section: str
    page_number: int
    chunk_type: ChunkType
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

class NormRAGService:
    def __init__(self):
        logger.info("üöÄ [INIT] Initializing NormRAGService...")
        self.db_conn = None
        self.qdrant_client = None
        self._embedding_model = None  # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        self._tokenizer = None  # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        self.text_splitter = None
        self.optimized_indexer = None  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
        self.model_loaded = False
        self.model_load_start_time = None
        self.connect_services()
        self.initialize_optimized_indexer()
        logger.info("‚úÖ [INIT] NormRAGService initialized successfully (models will be loaded on demand)")
    
    def connect_services(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîå [CONNECT] Connecting to services...")
        try:
            # PostgreSQL
            logger.info(f"üîå [CONNECT] Connecting to PostgreSQL: {POSTGRES_URL}")
            self.db_conn = psycopg2.connect(POSTGRES_URL)
            logger.info("‚úÖ [CONNECT] Connected to PostgreSQL")
            
            # Qdrant
            logger.info(f"üîå [CONNECT] Connecting to Qdrant: {QDRANT_URL}")
            self.qdrant_client = qdrant_client.QdrantClient(url=QDRANT_URL)
            logger.info("‚úÖ [CONNECT] Connected to Qdrant")
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            self._create_collections()
            
        except Exception as e:
            logger.error(f"‚ùå [CONNECT] Service connection error: {e}")
            raise
    
    def _create_collections(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        try:
            import requests
            response = requests.get(f"{QDRANT_URL}/collections")
            if response.status_code == 200:
                collections_data = response.json()
                collection_names = [col['name'] for col in collections_data.get('result', {}).get('collections', [])]
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if VECTOR_COLLECTION not in collection_names:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                else:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists.")
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if CHECKABLE_COLLECTION not in collection_names:
                    self.qdrant_client.create_collection(
                        collection_name=CHECKABLE_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {CHECKABLE_COLLECTION}")
                else:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {CHECKABLE_COLLECTION} already exists.")
            else:
                logger.warning(f"‚ö†Ô∏è [QDRANT] Could not check collections: {response.status_code}")
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
                try:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                except Exception as e:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists or error: {e}")
                
                try:
                    self.qdrant_client.create_collection(
                        collection_name=CHECKABLE_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {CHECKABLE_COLLECTION}")
                except Exception as e:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {CHECKABLE_COLLECTION} already exists or error: {e}")
                    
        except Exception as e:
            logger.error(f"‚ùå [QDRANT] Error creating collections: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π
    
    @property
    def embedding_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self._embedding_model is None:
            self._load_embedding_model()
        return self._embedding_model
    
    @property
    def tokenizer(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        if self._tokenizer is None:
            self._load_tokenizer()
        return self._tokenizer
    
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        if self._embedding_model is not None:
            return
        
        self.model_load_start_time = datetime.now()
        logger.info("ü§ñ [MODELS] Loading BGE-M3 embedding model (lazy loading)...")
        
        try:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            import os
            os.environ['TRANSFORMERS_CACHE'] = '/app/models'
            os.environ['HF_HOME'] = '/app/models'
            
            from sentence_transformers import SentenceTransformer
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
            self._embedding_model = SentenceTransformer(
                'BAAI/bge-m3', 
                device='cpu',
                cache_folder='/app/models'
            )
            
            load_time = (datetime.now() - self.model_load_start_time).total_seconds()
            logger.info(f"‚úÖ [MODELS] BGE-M3 model loaded successfully in {load_time:.2f} seconds")
            self.model_loaded = True
            
        except Exception as e:
            load_time = (datetime.now() - self.model_load_start_time).total_seconds()
            logger.error(f"‚ùå [MODELS] Failed to load BGE-M3 model after {load_time:.2f} seconds: {e}")
            logger.info("ü§ñ [MODELS] Falling back to simple hash embedding")
            self._embedding_model = None
    
    def _load_tokenizer(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        if self._tokenizer is not None:
            return
        
        logger.info("ü§ñ [MODELS] Loading tokenizer (lazy loading)...")
        try:
            self._tokenizer = tiktoken.get_encoding("cl100k_base")
            logger.info("‚úÖ [MODELS] Tokenizer loaded successfully")
        except Exception as e:
            logger.error(f"‚ùå [MODELS] Failed to load tokenizer: {e}")
            self._tokenizer = None
    
    def initialize_optimized_indexer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–∞)"""
        logger.info("üîß [OPTIMIZED_INDEXER] Optimized indexer temporarily disabled")
        self.optimized_indexer = None
    
    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text.split())
    
    def simple_text_split(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π —Å–ø–ª–∏—Ç—Ç–µ—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        if not text.strip():
            return []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_tokens = self.count_tokens(sentence)
            
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
            if sentence_tokens > CHUNK_SIZE:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_tokens = 0
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
                words = sentence.split()
                temp_chunk = ""
                temp_tokens = 0
                
                for word in words:
                    word_tokens = self.count_tokens(word + " ")
                    if temp_tokens + word_tokens > CHUNK_SIZE:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = word + " "
                        temp_tokens = word_tokens
                    else:
                        temp_chunk += word + " "
                        temp_tokens += word_tokens
                
                if temp_chunk:
                    current_chunk = temp_chunk
                    current_tokens = temp_tokens
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_tokens + sentence_tokens > CHUNK_SIZE:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
                    current_tokens = sentence_tokens
                else:
                    current_chunk += sentence + ". "
                    current_tokens += sentence_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def create_chunk_id(self, document_id: int, clause_id: str, chunk_index: int) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID —á–∞–Ω–∫–∞"""
        return f"doc_{document_id}_clause_{clause_id}_chunk_{chunk_index}"
    
    def extract_clause_id(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ clause_id –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–ø. 1.2.3", "—Å—Ç–∞—Ç—å—è 5", "–ì–û–°–¢ 12345-2020 –ø.4.1"
        import re
        patterns = [
            r'–ø\.\s*(\d+(?:\.\d+)*)',  # –ø. 1.2.3
            r'—Å—Ç–∞—Ç—å—è\s*(\d+)',  # —Å—Ç–∞—Ç—å—è 5
            r'–ì–û–°–¢\s*(\d+(?:-\d+)?)\s*–ø\.(\d+(?:\.\d+)*)',  # –ì–û–°–¢ 12345-2020 –ø.4.1
            r'–°–ü\s*(\d+(?:\.\d+)?)\s*–ø\.(\d+(?:\.\d+)*)',  # –°–ü 20.13330.2016 –ø.4.1
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º —Ö–µ—à
        return hashlib.md5(text[:100].encode()).hexdigest()[:8]
    
    def chunk_document(self, document_id: int, document_title: str, content: str, 
                      chapter: str = "", section: str = "", page_number: int = 1) -> List[NormChunk]:
        """–ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üìÑ [CHUNK] Starting document chunking for document ID: {document_id}")
        logger.info(f"üìÑ [CHUNK] Document title: {document_title}")
        logger.info(f"üìÑ [CHUNK] Content length: {len(content)} characters")
        logger.info(f"üìÑ [CHUNK] Page number: {page_number}")
        
        chunks = []
        
        # –ü—Ä–æ—Å—Ç–æ–π —á–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤
        logger.info(f"üìÑ [CHUNK] Performing text splitting...")
        text_chunks = self.simple_text_split(content)
        logger.info(f"üìÑ [CHUNK] Text split into {len(text_chunks)} raw chunks")
        
        for i, chunk_text in enumerate(text_chunks):
            if not chunk_text.strip():
                logger.debug(f"üìÑ [CHUNK] Skipping empty chunk {i}")
                continue
            
            logger.debug(f"üìÑ [CHUNK] Processing chunk {i}: {len(chunk_text)} characters")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
            chunk_type = self.detect_chunk_type(chunk_text)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} type: {chunk_type.value}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º clause_id
            clause_id = self.extract_clause_id(chunk_text)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} clause_id: {clause_id}")
            
            # –°–æ–∑–¥–∞–µ–º ID —á–∞–Ω–∫–∞
            chunk_id = self.create_chunk_id(document_id, clause_id, i)
            logger.debug(f"üìÑ [CHUNK] Chunk {i} ID: {chunk_id}")
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            token_count = self.count_tokens(chunk_text)
            metadata = {
                "document_id": document_id,
                "document_title": document_title,
                "chapter": chapter,
                "section": section,
                "page_number": page_number,
                "chunk_type": chunk_type.value,
                "clause_id": clause_id,
                "chunk_index": i,
                "token_count": token_count,
                "created_at": datetime.now().isoformat()
            }
            
            chunk = NormChunk(
                chunk_id=chunk_id,
                clause_id=clause_id,
                document_id=document_id,
                document_title=document_title,
                chapter=chapter,
                section=section,
                page_number=page_number,
                chunk_type=chunk_type,
                content=chunk_text,
                metadata=metadata
            )
            
            chunks.append(chunk)
            logger.debug(f"üìÑ [CHUNK] Created chunk {i}: {chunk_id} ({token_count} tokens)")
        
        logger.info(f"‚úÖ [CHUNK] Document chunking completed. Created {len(chunks)} chunks")
        return chunks
    
    def detect_chunk_type(self, text: str) -> ChunkType:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —á–∞–Ω–∫–∞"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
        if any(keyword in text_lower for keyword in ['|', '\t', '—Ç–∞–±–ª–∏—Ü–∞', 'table']):
            return ChunkType.TABLE
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if len(text.strip()) < 100 and any(keyword in text_lower for keyword in ['–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', 'chapter', 'section']):
            return ChunkType.HEADER
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–∏—Å—É–Ω–æ–∫
        if any(keyword in text_lower for keyword in ['—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∏—Å.', 'figure', '–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è']):
            return ChunkType.FIGURE
        
        return ChunkType.TEXT
    
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        start_time = datetime.now()
        text_length = len(text)
        
        model_logger.info(f"ü§ñ [EMBEDDING_MODEL] Creating embedding for text ({text_length} chars)")
        logger.debug(f"üî¢ [EMBED] Creating embedding for text: {text_length} characters")
        
        if self.embedding_model:
            try:
                model_logger.info(f"ü§ñ [EMBEDDING_MODEL] Using BGE-M3 model for embedding")
                logger.debug(f"üî¢ [EMBED] Using BGE-M3 model for embedding")
                
                # BGE-M3 —Å–æ–∑–¥–∞–µ—Ç 1024-–º–µ—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
                embedding = self.embedding_model.encode(text, normalize_embeddings=True)
                embedding_list = embedding.tolist()
                
                embedding_time = (datetime.now() - start_time).total_seconds()
                model_logger.info(f"‚úÖ [EMBEDDING_MODEL] BGE-M3 embedding created in {embedding_time:.3f}s: {len(embedding_list)} dimensions")
                logger.debug(f"üî¢ [EMBED] BGE-M3 embedding created: {len(embedding_list)} dimensions")
                
                return embedding_list
            except Exception as e:
                embedding_time = (datetime.now() - start_time).total_seconds()
                model_logger.error(f"‚ùå [EMBEDDING_MODEL] BGE-M3 embedding error after {embedding_time:.3f}s: {type(e).__name__}: {str(e)}")
                logger.error(f"‚ùå [EMBED] BGE-M3 embedding creation error: {e}")
                logger.info(f"üî¢ [EMBED] Falling back to simple hash embedding")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥—É
        model_logger.info(f"ü§ñ [EMBEDDING_FALLBACK] Using simple hash embedding fallback")
        logger.debug(f"üî¢ [EMBED] Using simple hash embedding fallback")
        return self.create_simple_embedding(text)
    
    def create_simple_embedding(self, text: str) -> List[float]:
        """–ü—Ä–æ—Å—Ç–æ–π —Ö–µ—à-—ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è fallback"""
        logger.debug(f"üî¢ [EMBED] Creating simple hash embedding")
        hash_obj = hashlib.sha256(text.encode())
        hash_bytes = hash_obj.digest()
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 1024-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–∫–∞–∫ —É BGE-M3)
        embedding = []
        for i in range(1024):
            embedding.append(float(hash_bytes[i % len(hash_bytes)]) / 255.0)
        logger.debug(f"üî¢ [EMBED] Simple hash embedding created: {len(embedding)} dimensions")
        return embedding
    
    def index_chunks(self, chunks: List[NormChunk]) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏ BM25"""
        logger.info(f"üîó [INDEX] Indexing {len(chunks)} chunks...")
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            for chunk in chunks:
                chunk.embedding = self.create_embedding(chunk.content)
                logger.debug(f"üîó [INDEX] Created embedding for chunk {chunk.chunk_id}")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)
            self.index_to_qdrant(chunks)
            logger.info(f"‚úÖ [INDEX] Qdrant indexing successful")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤ PostgreSQL (BM25/keyword –ø–æ–∏—Å–∫)
            self.index_to_postgres(chunks)
            logger.info(f"‚úÖ [INDEX] PostgreSQL indexing successful")
            
            logger.info(f"‚úÖ [INDEX] Successfully indexed {len(chunks)} chunks")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [INDEX] Indexing error: {e}")
            return False
    
    def index_to_qdrant(self, chunks: List[NormChunk]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            import requests
            response = requests.get(f"{QDRANT_URL}/collections")
            if response.status_code == 200:
                collections_data = response.json()
                collection_names = [col['name'] for col in collections_data.get('result', {}).get('collections', [])]
                if VECTOR_COLLECTION not in collection_names:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                else:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists.")
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                if CHECKABLE_COLLECTION not in collection_names:
                    self.qdrant_client.create_collection(
                        collection_name=CHECKABLE_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {CHECKABLE_COLLECTION}")
                else:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {CHECKABLE_COLLECTION} already exists.")
            else:
                logger.warning(f"‚ö†Ô∏è [QDRANT] Could not check collections: {response.status_code}")
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
                try:
                    self.qdrant_client.create_collection(
                        collection_name=VECTOR_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {VECTOR_COLLECTION}")
                except Exception as e:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {VECTOR_COLLECTION} already exists or error: {e}")
                
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                try:
                    self.qdrant_client.create_collection(
                        collection_name=CHECKABLE_COLLECTION,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
                    )
                    logger.info(f"‚úÖ [QDRANT] Created Qdrant collection: {CHECKABLE_COLLECTION}")
                except Exception as e:
                    logger.info(f"‚úÖ [QDRANT] Qdrant collection {CHECKABLE_COLLECTION} already exists or error: {e}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            points = []
            for chunk in chunks:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ö–µ—à –¥–ª—è ID —Ç–æ—á–∫–∏
                point_id = abs(hash(chunk.chunk_id))
                point = PointStruct(
                    id=point_id,
                    vector=chunk.embedding,
                    payload={
                        "chunk_id": chunk.chunk_id,
                        "clause_id": chunk.clause_id,
                        "document_id": chunk.document_id,
                        "document_title": chunk.document_title,
                        "chapter": chunk.chapter,
                        "section": chunk.section,
                        "page_number": chunk.page_number,
                        "chunk_type": chunk.chunk_type.value,
                        "content": chunk.content,
                        "metadata": chunk.metadata
                    }
                )
                points.append(point)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ—á–∫–∏
            self.qdrant_client.upsert(
                collection_name=VECTOR_COLLECTION,
                points=points
            )
            logger.info(f"‚úÖ [QDRANT] Upserted {len(points)} points to Qdrant")
            
        except Exception as e:
            logger.error(f"‚ùå [QDRANT] Qdrant indexing error: {e}")
            raise
    
    def index_to_postgres(self, chunks: List[NormChunk]):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ PostgreSQL –¥–ª—è BM25/keyword –ø–æ–∏—Å–∫–∞"""
        logger.info(f"üóÑÔ∏è [POSTGRES] Starting PostgreSQL indexing for {len(chunks)} chunks")
        try:
            with self.db_conn.cursor() as cursor:
                # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —á–∞–Ω–∫–æ–≤ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                logger.info(f"üóÑÔ∏è [POSTGRES] Creating normative_chunks table if not exists...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS normative_chunks (
                        chunk_id VARCHAR(255) PRIMARY KEY,
                        clause_id VARCHAR(255) NOT NULL,
                        document_id INTEGER NOT NULL,
                        document_title VARCHAR(500) NOT NULL,
                        chapter VARCHAR(255),
                        section VARCHAR(255),
                        page_number INTEGER,
                        chunk_type VARCHAR(50) NOT NULL,
                        content TEXT NOT NULL,
                        metadata JSONB,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                logger.info(f"‚úÖ [POSTGRES] Table normative_chunks ready")
                
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                logger.info(f"üóÑÔ∏è [POSTGRES] Creating indexes...")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_document_id ON normative_chunks(document_id);
                    CREATE INDEX IF NOT EXISTS idx_chunks_clause_id ON normative_chunks(clause_id);
                    CREATE INDEX IF NOT EXISTS idx_chunks_chunk_type ON normative_chunks(chunk_type);
                    CREATE INDEX IF NOT EXISTS idx_chunks_content_gin ON normative_chunks USING gin(to_tsvector('russian', content));
                """)
                logger.info(f"‚úÖ [POSTGRES] Indexes created")
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
                logger.info(f"üóÑÔ∏è [POSTGRES] Inserting {len(chunks)} chunks...")
                for i, chunk in enumerate(chunks):
                    logger.debug(f"üóÑÔ∏è [POSTGRES] Inserting chunk {i+1}/{len(chunks)}: {chunk.chunk_id}")
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, document_id, document_title, chapter, section, 
                         page_number, chunk_type, content, metadata)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (chunk_id) DO UPDATE SET
                        content = EXCLUDED.content,
                        metadata = EXCLUDED.metadata
                    """, (
                        chunk.chunk_id, chunk.clause_id, chunk.document_id, 
                        chunk.document_title, chunk.chapter, chunk.section,
                        chunk.page_number, chunk.chunk_type.value, chunk.content,
                        json.dumps(chunk.metadata)
                    ))
                
                self.db_conn.commit()
                logger.info(f"‚úÖ [POSTGRES] Successfully inserted {len(chunks)} chunks into PostgreSQL")
                
        except Exception as e:
            self.db_conn.rollback()
            logger.error(f"‚ùå [POSTGRES] PostgreSQL indexing error: {e}")
            logger.error(f"‚ùå [POSTGRES] Error details: {type(e).__name__}: {str(e)}")
            raise
    
    def hybrid_search(self, query: str, k: int = 8, 
                     document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None,
                     chunk_type_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25"""
        logger.info(f"üîç [SEARCH] Performing hybrid search for query: '{query}' with k={k}")
        try:
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_results = self.vector_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] Vector search completed. Found {len(vector_results)} results.")
            
            # BM25 –ø–æ–∏—Å–∫
            bm25_results = self.bm25_search(query, k * 2)
            logger.info(f"‚úÖ [SEARCH] BM25 search completed. Found {len(bm25_results)} results.")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results = self.combine_search_results(vector_results, bm25_results, k)
            logger.info(f"‚úÖ [SEARCH] Combined search results. Total {len(combined_results)} results.")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filtered_results = self.apply_filters(combined_results, document_filter, chapter_filter, chunk_type_filter)
            logger.info(f"‚úÖ [SEARCH] Applied filters. Final {len(filtered_results)} results.")
            
            return filtered_results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå [SEARCH] Hybrid search error: {e}")
            return []
    
    def vector_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant"""
        start_time = datetime.now()
        logger.info(f"üîç [VECTOR] Performing vector search for query: '{query}' with k={k}")
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            model_logger.info(f"ü§ñ [EMBEDDING_CREATE] Creating embedding for query: '{query[:100]}...'")
            embedding_start = datetime.now()
            
            query_embedding = self.create_embedding(query)
            
            embedding_time = (datetime.now() - embedding_start).total_seconds()
            model_logger.info(f"‚úÖ [EMBEDDING_CREATE] Embedding created in {embedding_time:.3f}s, dimension: {len(query_embedding)}")
            logger.debug(f"üîç [VECTOR] Query embedding created: {query_embedding[:5]}...")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant
            search_start = datetime.now()
            model_logger.info(f"üîç [QDRANT_SEARCH] Searching in Qdrant collection: {VECTOR_COLLECTION}")
            
            results = self.qdrant_client.search(
                collection_name=VECTOR_COLLECTION,
                query_vector=query_embedding,
                limit=k,
                with_payload=True
            )
            
            search_time = (datetime.now() - search_start).total_seconds()
            total_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"‚úÖ [VECTOR] Vector search completed in {total_time:.3f}s. Found {len(results)} results.")
            model_logger.info(f"‚úÖ [QDRANT_SEARCH] Qdrant search completed in {search_time:.3f}s, found {len(results)} results")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if results:
                top_result = results[0]
                model_logger.info(f"üìä [VECTOR_TOP] Top vector result: {top_result.payload.get('document_title', 'Unknown')} - Score: {top_result.score:.3f}")
                model_logger.debug(f"üìä [VECTOR_TOP] Top result content: {top_result.payload.get('content', '')[:200]}...")
            
            return [
                {
                    "chunk_id": result.payload["chunk_id"],
                    "clause_id": result.payload["clause_id"],
                    "content": result.payload["content"],
                    "document_title": result.payload["document_title"],
                    "chapter": result.payload["chapter"],
                    "section": result.payload["section"],
                    "page_number": result.payload["page_number"],
                    "chunk_type": result.payload["chunk_type"],
                    "score": result.score,
                    "search_type": "vector"
                }
                for result in results
            ]
            
        except Exception as e:
            total_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå [VECTOR] Vector search error after {total_time:.3f}s: {type(e).__name__}: {str(e)}")
            model_logger.error(f"‚ùå [EMBEDDING_ERROR] Failed to create embedding for query: '{query[:100]}...' - {str(e)}")
            return []
    
    def bm25_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 –ø–æ–∏—Å–∫ –≤ PostgreSQL"""
        logger.info(f"üîç [BM25] Performing BM25 search for query: '{query}' with k={k}")
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ PostgreSQL
                cursor.execute("""
                    SELECT 
                        chunk_id, clause_id, content, document_title, 
                        chapter, section, page_number, chunk_type,
                        ts_rank(to_tsvector('russian', content), plainto_tsquery('russian', %s)) as score
                    FROM normative_chunks 
                    WHERE to_tsvector('russian', content) @@ plainto_tsquery('russian', %s)
                    ORDER BY score DESC
                    LIMIT %s
                """, (query, query, k))
                
                results = cursor.fetchall()
                logger.info(f"‚úÖ [BM25] BM25 search completed. Found {len(results)} results.")
                
                return [
                    {
                        "chunk_id": row["chunk_id"],
                        "clause_id": row["clause_id"],
                        "content": row["content"],
                        "document_title": row["document_title"],
                        "chapter": row["chapter"],
                        "section": row["section"],
                        "page_number": row["page_number"],
                        "chunk_type": row["chunk_type"],
                        "score": float(row["score"]),
                        "search_type": "bm25"
                    }
                    for row in results
                ]
                
        except Exception as e:
            logger.error(f"‚ùå [BM25] BM25 search error: {e}")
            return []
    
    def combine_search_results(self, vector_results: List[Dict], bm25_results: List[Dict], k: int) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ BM25 –ø–æ–∏—Å–∫–∞"""
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        combined = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(vector_results):
            chunk_id = result["chunk_id"]
            if chunk_id not in combined:
                combined[chunk_id] = result.copy()
                combined[chunk_id]["combined_score"] = result["score"] * 0.6  # –í–µ—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            else:
                combined[chunk_id]["combined_score"] += result["score"] * 0.6
        
        # –î–æ–±–∞–≤–ª—è–µ–º BM25 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(bm25_results):
            chunk_id = result["chunk_id"]
            if chunk_id not in combined:
                combined[chunk_id] = result.copy()
                combined[chunk_id]["combined_score"] = result["score"] * 0.4  # –í–µ—Å BM25
            else:
                combined[chunk_id]["combined_score"] += result["score"] * 0.4
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
        sorted_results = sorted(combined.values(), key=lambda x: x["combined_score"], reverse=True)
        
        return sorted_results[:k]
    
    def apply_filters(self, results: List[Dict], document_filter: Optional[str] = None,
                     chapter_filter: Optional[str] = None, chunk_type_filter: Optional[str] = None) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        filtered_results = results
        
        if document_filter:
            filtered_results = [r for r in filtered_results if document_filter.lower() in r["document_title"].lower()]
            logger.info(f"‚úÖ [FILTER] Document filter applied. {len(filtered_results)} results remaining.")
        
        if chapter_filter:
            filtered_results = [r for r in filtered_results if chapter_filter.lower() in r["chapter"].lower()]
            logger.info(f"‚úÖ [FILTER] Chapter filter applied. {len(filtered_results)} results remaining.")
        
        if chunk_type_filter:
            filtered_results = [r for r in filtered_results if r["chunk_type"] == chunk_type_filter]
            logger.info(f"‚úÖ [FILTER] Chunk type filter applied. {len(filtered_results)} results remaining.")
        
        return filtered_results
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.info("üìÑ [GET_DOCUMENTS] Getting documents list...")
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã normative_chunks
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'normative_chunks'
                    );
                """)
                table_exists = cursor.fetchone()['exists']
                
                if not table_exists:
                    logger.info("üìÑ [GET_DOCUMENTS] Table normative_chunks does not exist, returning empty list")
                    return []
                
                # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                cursor.execute("""
                    SELECT DISTINCT 
                        nc.document_id,
                        nc.document_title,
                        COUNT(*) as chunks_count,
                        MIN(nc.page_number) as first_page,
                        MAX(nc.page_number) as last_page,
                        STRING_AGG(DISTINCT nc.chunk_type, ', ') as chunk_types,
                        ud.category
                    FROM normative_chunks nc
                    LEFT JOIN uploaded_documents ud ON nc.document_id = ud.id
                    GROUP BY nc.document_id, nc.document_title, ud.category
                    ORDER BY nc.document_title ASC
                """)
                
                documents = cursor.fetchall()
                logger.info(f"‚úÖ [GET_DOCUMENTS] Retrieved {len(documents)} documents")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                result = []
                for doc in documents:
                    result.append({
                        "id": doc['document_id'],
                        "title": doc['document_title'] or f"–î–æ–∫—É–º–µ–Ω—Ç {doc['document_id']}",
                        "chunks_count": doc['chunks_count'],
                        "first_page": doc['first_page'],
                        "last_page": doc['last_page'],
                        "chunk_types": doc['chunk_types'].split(', ') if doc['chunk_types'] else [],
                        "category": doc['category'] or 'other',
                        "status": "indexed"
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENTS] Error getting documents: {e}")
            return []
    
    def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üìÑ [GET_DOCUMENT_CHUNKS] Getting chunks for document ID: {document_id}")
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã normative_chunks
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'normative_chunks'
                    );
                """)
                table_exists = cursor.fetchone()['exists']
                
                if not table_exists:
                    logger.info("üìÑ [GET_DOCUMENT_CHUNKS] Table normative_chunks does not exist, returning empty list")
                    return []
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute("""
                    SELECT 
                        chunk_id,
                        clause_id,
                        chapter,
                        section,
                        page_number,
                        chunk_type,
                        LENGTH(content) as content_length,
                        created_at
                    FROM normative_chunks 
                    WHERE document_id = %s
                    ORDER BY page_number ASC, chunk_id ASC
                """, (document_id,))
                
                chunks = cursor.fetchall()
                logger.info(f"‚úÖ [GET_DOCUMENT_CHUNKS] Retrieved {len(chunks)} chunks for document {document_id}")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                result = []
                for chunk in chunks:
                    result.append({
                        "chunk_id": chunk['chunk_id'],
                        "clause_id": chunk['clause_id'],
                        "chapter": chunk['chapter'] or "–ù–µ —É–∫–∞–∑–∞–Ω–æ",
                        "section": chunk['section'] or "–ù–µ —É–∫–∞–∑–∞–Ω–æ",
                        "page_number": chunk['page_number'],
                        "chunk_type": chunk['chunk_type'],
                        "content_length": chunk['content_length'],
                        "created_at": chunk['created_at'].isoformat() if chunk['created_at'] else None
                    })
                
                return result
                
        except Exception as e:
            logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Error getting document chunks: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        logger.info("üìä [STATS] Getting service statistics...")
        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL
            with self.db_conn.cursor() as cursor:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã normative_chunks
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'normative_chunks'
                    );
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    logger.info("üìä [STATS] Table normative_chunks does not exist, returning empty stats")
                    total_chunks = 0
                    total_documents = 0
                    total_clauses = 0
                    chunk_types = {}
                else:
                    cursor.execute("SELECT COUNT(*) FROM normative_chunks")
                    total_chunks = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total chunks in PostgreSQL: {total_chunks}")
                    
                    cursor.execute("SELECT COUNT(DISTINCT document_id) FROM normative_chunks")
                    total_documents = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total unique documents in PostgreSQL: {total_documents}")
                    
                    cursor.execute("SELECT COUNT(DISTINCT clause_id) FROM normative_chunks")
                    total_clauses = cursor.fetchone()[0]
                    logger.info(f"‚úÖ [STATS] Total unique clauses in PostgreSQL: {total_clauses}")
                    
                    cursor.execute("""
                        SELECT chunk_type, COUNT(*) 
                        FROM normative_chunks 
                        GROUP BY chunk_type
                    """)
                    chunk_types = dict(cursor.fetchall())
                    logger.info(f"‚úÖ [STATS] Chunk types distribution: {chunk_types}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Qdrant
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ - —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã,
            # –ø–æ—Å–∫–æ–ª—å–∫—É –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            vector_count = total_chunks
            logger.info(f"‚úÖ [STATS] Total vectors available in Qdrant: {vector_count}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ–∫–∞ –ø—É—Å—Ç–∞—è, —Ç–∞–∫ –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ uploaded_documents –≤ –¥—Ä—É–≥–æ–º —Å–µ—Ä–≤–∏—Å–µ)
            category_distribution = {}
            logger.info(f"‚úÖ [STATS] Category distribution: {category_distribution}")
            
            return {
                "total_chunks": total_chunks,
                "total_documents": total_documents,
                "total_clauses": total_clauses,
                "vector_indexed": vector_count,
                "chunk_types": chunk_types,
                "category_distribution": category_distribution,
                "collection_name": VECTOR_COLLECTION,
                "indexing_status": "active" if total_chunks > 0 else "empty"
            }
            
        except Exception as e:
            logger.error(f"‚ùå [STATS] Stats error: {e}")
            return {"error": str(e)}

    def delete_document_indexes(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info(f"üóëÔ∏è [DELETE_DOC] Deleting indexes for document ID: {document_id}")
        try:
            deleted_count = 0
            
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º chunk_id –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ Qdrant
            chunk_ids = []
            with self.db_conn.cursor() as cursor:
                cursor.execute("SELECT chunk_id FROM normative_chunks WHERE document_id = %s", (document_id,))
                chunk_ids = [row[0] for row in cursor.fetchall()]
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ Qdrant
            if chunk_ids:
                try:
                    # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏ –∏–∑ Qdrant
                    self.qdrant_client.delete(
                        collection_name=VECTOR_COLLECTION,
                        points_selector=chunk_ids
                    )
                    qdrant_deleted = len(chunk_ids)
                    deleted_count += qdrant_deleted
                    logger.info(f"‚úÖ [DELETE_DOC] Deleted {qdrant_deleted} vectors from Qdrant for document {document_id}")
                except Exception as e:
                    logger.error(f"‚ùå [DELETE_DOC] Error deleting from Qdrant: {e}")
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ PostgreSQL
            with self.db_conn.cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks WHERE document_id = %s", (document_id,))
                postgresql_deleted = cursor.rowcount
                self.db_conn.commit()
                deleted_count += postgresql_deleted
                logger.info(f"‚úÖ [DELETE_DOC] Deleted {postgresql_deleted} chunks from PostgreSQL for document {document_id}")
            
            logger.info(f"‚úÖ [DELETE_DOC] Deleted {deleted_count} indexes for document {document_id}")
            return deleted_count > 0
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_DOC] Delete document indexes error: {e}")
            return False

    def delete_all_indexes(self) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ (–æ—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã)"""
        logger.info("üóëÔ∏è [DELETE_ALL] Clearing all indexes...")
        try:
            deleted_count = 0
            
            # –û—á–∏—â–∞–µ–º PostgreSQL
            with self.db_conn.cursor() as cursor:
                cursor.execute("DELETE FROM normative_chunks")
                postgresql_deleted = cursor.rowcount
                self.db_conn.commit()
                deleted_count += postgresql_deleted
                logger.info(f"‚úÖ [DELETE_ALL] Deleted {postgresql_deleted} chunks from PostgreSQL")
            
            # –û—á–∏—â–∞–µ–º Qdrant
            try:
                self.qdrant_client.delete(
                    collection_name=VECTOR_COLLECTION,
                    points_selector="all"
                )
                logger.info("‚úÖ [DELETE_ALL] Cleared all vectors from Qdrant")
            except Exception as e:
                logger.error(f"‚ùå [DELETE_ALL] Error clearing Qdrant: {e}")
            
            logger.info(f"‚úÖ [DELETE_ALL] Deleted {deleted_count} indexes from system")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [DELETE_ALL] Delete all indexes error: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
rag_service = NormRAGService()

@app.post("/index")
async def index_document(
    document_id: int = Form(...),
    document_title: str = Form(...),
    content: str = Form(...),
    chapter: str = Form(""),
    section: str = Form(""),
    page_number: int = Form(1)
):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG-—Å–∏—Å—Ç–µ–º—É"""
    logger.info(f"üìÑ [INDEX_DOC] Indexing document ID: {document_id}")
    try:
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
        chunks = rag_service.chunk_document(
            document_id=document_id,
            document_title=document_title,
            content=content,
            chapter=chapter,
            section=section,
            page_number=page_number
        )
        logger.info(f"‚úÖ [INDEX_DOC] Document {document_id} chunked into {len(chunks)} chunks.")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        success = rag_service.index_chunks(chunks)
        
        if success:
            return {
                "status": "success",
                "document_id": document_id,
                "chunks_created": len(chunks),
                "message": f"Successfully indexed {len(chunks)} chunks"
            }
        else:
            raise HTTPException(status_code=500, detail="Indexing failed")
            
    except Exception as e:
        logger.error(f"‚ùå [INDEX_DOC] Indexing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    category: str = Form("other"),
    description: str = Form("")
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üì§ [UPLOAD_DOC] Uploading document: {file.filename}")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        if not file.filename.lower().endswith(('.pdf', '.docx', '.txt')):
            raise HTTPException(status_code=400, detail="Unsupported file type. Only PDF, DOCX, and TXT files are allowed.")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = await file.read()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–µ–±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –¥–ª—è PostgreSQL)
        import hashlib
        import time
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–æ–π —Ö–µ—à + timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        hash_part = int(hashlib.md5(f"{file.filename}_{content[:100]}".encode()).hexdigest()[:3], 16)
        time_part = int(time.time()) % 100000  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Ü–∏—Ñ—Ä –≤—Ä–µ–º–µ–Ω–∏
        document_id = time_part * 1000 + hash_part  # –ü–æ–ª—É—á–∏—Ç—Å—è —á–∏—Å–ª–æ –¥–æ 8 —Ü–∏—Ñ—Ä (–º–∞–∫—Å. 99999999)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        if file.filename.lower().endswith('.txt'):
            text_content = content.decode('utf-8', errors='ignore')
        else:
            # –î–ª—è PDF –∏ DOCX –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
            text_content = f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file.filename}"
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
        chunks = rag_service.chunk_document(
            document_id=document_id,
            document_title=file.filename,
            content=text_content,
            chapter="",
            section="",
            page_number=1
        )
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        success = rag_service.index_chunks(chunks)
        
        if success:
            return {
                "status": "success",
                "document_id": document_id,
                "filename": file.filename,
                "chunks_created": len(chunks),
                "message": f"Document uploaded and indexed successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to index document")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [UPLOAD_DOC] Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_norms(
    query: str = Form(...),
    k: int = Form(8),
    document_filter: Optional[str] = Form(None),
    chapter_filter: Optional[str] = Form(None),
    chunk_type_filter: Optional[str] = Form(None)
):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    start_time = datetime.now()
    logger.info(f"üîç [SEARCH_NORM] Performing hybrid search for query: '{query}' with k={k}")
    logger.info(f"üîç [SEARCH_NORM] Filters: document={document_filter}, chapter={chapter_filter}, chunk_type={chunk_type_filter}")
    
    try:
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model_logger.info(f"ü§ñ [EMBEDDING_REQUEST] Generating embeddings for query: '{query[:100]}...'")
        
        results = rag_service.hybrid_search(
            query=query,
            k=k,
            document_filter=document_filter,
            chapter_filter=chapter_filter,
            chunk_type_filter=chunk_type_filter
        )
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ [SEARCH_NORM] Hybrid search completed in {execution_time:.2f}s. Found {len(results)} results.")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        if results:
            top_result = results[0]
            model_logger.info(f"üìä [SEARCH_RESULTS] Top result: {top_result.get('document_title', 'Unknown')} - Score: {top_result.get('score', 0):.3f}")
            model_logger.debug(f"üìä [SEARCH_RESULTS] Top result content preview: {top_result.get('content', '')[:200]}...")
        
        return {
            "query": query,
            "results_count": len(results),
            "execution_time": execution_time,
            "results": results
        }
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.error(f"‚ùå [SEARCH_NORM] Search error after {execution_time:.2f}s: {type(e).__name__}: {str(e)}")
        model_logger.error(f"‚ùå [EMBEDDING_ERROR] Failed to process query: '{query[:100]}...' - {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents")
async def get_documents():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    logger.info("üìÑ [GET_DOCUMENTS] Getting documents list...")
    try:
        documents = rag_service.get_documents()
        logger.info(f"‚úÖ [GET_DOCUMENTS] Documents list retrieved: {len(documents)} documents")
        return {"documents": documents}
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENTS] Documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
    logger.info("üìä [GET_STATS] Getting service statistics...")
    try:
        stats = rag_service.get_stats()
        logger.info(f"‚úÖ [GET_STATS] Service statistics retrieved: {stats}")
        return stats
    except Exception as e:
        logger.error(f"‚ùå [GET_STATS] Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{document_id}/chunks")
async def get_document_chunks(document_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üìÑ [GET_DOCUMENT_CHUNKS] Getting chunks for document ID: {document_id}")
    try:
        chunks_info = rag_service.get_document_chunks(document_id)
        logger.info(f"‚úÖ [GET_DOCUMENT_CHUNKS] Chunks info retrieved for document {document_id}")
        return {"chunks": chunks_info}
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENT_CHUNKS] Chunks error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/stats")
async def get_documents_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
    logger.info("üìä [GET_DOCUMENTS_STATS] Getting documents statistics...")
    try:
        stats = rag_service.get_stats()
        documents = rag_service.get_documents()
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        adapted_stats = {
            "statistics": {
                "total_documents": len(documents),
                "indexed_documents": len(documents),
                "indexing_progress_percent": 100 if len(documents) > 0 else 0,
                "categories": [
                    {"category": "–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", "count": len(documents)}
                ]
            }
        }
        
        logger.info(f"‚úÖ [GET_DOCUMENTS_STATS] Documents statistics retrieved: {adapted_stats}")
        return adapted_stats
    except Exception as e:
        logger.error(f"‚ùå [GET_DOCUMENTS_STATS] Documents stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}")
async def delete_document(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å–æ–≤"""
    logger.info(f"üóëÔ∏è [DELETE_DOCUMENT] Deleting document ID: {document_id}")
    try:
        success = rag_service.delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Document {document_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOCUMENT] Delete document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/indexes/document/{document_id}")
async def delete_document_indexes(document_id: int):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    logger.info(f"üóëÔ∏è [DELETE_DOC_INDEXES] Deleting indexes for document ID: {document_id}")
    try:
        success = rag_service.delete_document_indexes(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Indexes for document {document_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Document indexes not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_DOC_INDEXES] Delete document indexes error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/indexes/all")
async def delete_all_indexes():
    """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ (–æ—á–∏—Å—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã)"""
    logger.info("üóëÔ∏è [DELETE_ALL_INDEXES] Clearing all indexes...")
    try:
        success = rag_service.delete_all_indexes()
        
        if success:
            return {
                "status": "success",
                "message": "All indexes deleted successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to delete indexes")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [DELETE_ALL_INDEXES] Delete all indexes error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ RAG-—Å–µ—Ä–≤–∏—Å–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus"""
    logger.info("üìä [METRICS] Getting service metrics...")
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ RAG —Å–µ—Ä–≤–∏—Å–∞
        stats = rag_service.get_stats()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        metrics_lines = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        metrics_lines.append(f"# HELP rag_service_collections_total Total number of collections")
        metrics_lines.append(f"# TYPE rag_service_collections_total gauge")
        metrics_lines.append(f"rag_service_collections_total 2")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        metrics_lines.append(f"# HELP rag_service_chunk_size Chunk size for text processing")
        metrics_lines.append(f"# TYPE rag_service_chunk_size gauge")
        metrics_lines.append(f"rag_service_chunk_size {CHUNK_SIZE}")
        
        metrics_lines.append(f"# HELP rag_service_chunk_overlap Chunk overlap for text processing")
        metrics_lines.append(f"# TYPE rag_service_chunk_overlap gauge")
        metrics_lines.append(f"rag_service_chunk_overlap {CHUNK_OVERLAP}")
        
        metrics_lines.append(f"# HELP rag_service_max_tokens Maximum tokens for processing")
        metrics_lines.append(f"# TYPE rag_service_max_tokens gauge")
        metrics_lines.append(f"rag_service_max_tokens {MAX_TOKENS}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
        metrics_lines.append(f"# HELP rag_service_connections_status Connection status")
        metrics_lines.append(f"# TYPE rag_service_connections_status gauge")
        metrics_lines.append(f'rag_service_connections_status{{service="postgresql"}} {1 if rag_service.db_conn else 0}')
        metrics_lines.append(f'rag_service_connections_status{{service="qdrant"}} {1 if rag_service.qdrant_client else 0}')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if stats:
            metrics_lines.append(f"# HELP rag_service_total_chunks Total number of chunks")
            metrics_lines.append(f"# TYPE rag_service_total_chunks counter")
            metrics_lines.append(f"rag_service_total_chunks {stats.get('total_chunks', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_total_documents Total number of documents")
            metrics_lines.append(f"# TYPE rag_service_total_documents counter")
            metrics_lines.append(f"rag_service_total_documents {stats.get('total_documents', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_total_clauses Total number of clauses")
            metrics_lines.append(f"# TYPE rag_service_total_clauses counter")
            metrics_lines.append(f"rag_service_total_clauses {stats.get('total_clauses', 0)}")
            
            metrics_lines.append(f"# HELP rag_service_vector_indexed Total vector indexed")
            metrics_lines.append(f"# TYPE rag_service_vector_indexed counter")
            metrics_lines.append(f"rag_service_vector_indexed {stats.get('vector_indexed', 0)}")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º —á–∞–Ω–∫–æ–≤
            chunk_types = stats.get('chunk_types', {})
            for chunk_type, count in chunk_types.items():
                metrics_lines.append(f'rag_service_chunks_by_type{{type="{chunk_type}"}} {count}')
        
        logger.info(f"‚úÖ [METRICS] Service metrics retrieved successfully")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
        from fastapi.responses import Response
        return Response(
            content="\n".join(metrics_lines),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
        
    except Exception as e:
        logger.error(f"‚ùå [METRICS] Metrics error: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        error_metrics = [
            "# HELP rag_service_up Service is up",
            "# TYPE rag_service_up gauge",
            "rag_service_up 0"
        ]
        from fastapi.responses import Response
        return Response(
            content="\n".join(error_metrics),
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    logger.info("üí™ [HEALTH] Performing health check...")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        rag_service.db_conn.cursor().execute("SELECT 1")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Qdrant —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å
        import requests
        response = requests.get(f"{QDRANT_URL}/collections")
        if response.status_code != 200:
            raise Exception("Qdrant connection failed")
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "embedding_model": "BGE-M3" if rag_service.embedding_model else "simple_hash",
            "optimized_indexer": "not_available",  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω
            "services": {
                "postgresql": "connected",
                "qdrant": "connected"
            }
        }
    except Exception as e:
        logger.error(f"‚ùå [HEALTH] Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# @app.post("/optimized/index")
# async def optimized_index_document(
#     document_info: Dict[str, Any],
#     content: str = Form(...)
# ):
#     """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
#     logger.info("üìÑ [OPTIMIZED_INDEX] Optimized indexing requested")
#     
#     try:
#         if not rag_service.optimized_indexer:
#             raise HTTPException(status_code=503, detail="Optimized indexer not available")
#         
#         # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
#         success = rag_service.optimized_indexer.index_document(document_info, content)
#         
#         if success:
#             logger.info("‚úÖ [OPTIMIZED_INDEX] Document indexed successfully")
#             return {
#                 "status": "success",
#                 "message": "Document indexed with optimized structure",
#                 "timestamp": datetime.now().isoformat()
#             }
#         else:
#             raise HTTPException(status_code=500, detail="Failed to index document")
#             
#     except Exception as e:
#         logger.error(f"‚ùå [OPTIMIZED_INDEX] Error: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.post("/optimized/search")
# async def optimized_search_with_context(
#     query: str = Form(...),
#     document_mark: Optional[str] = Form(None),
#     document_stage: Optional[str] = Form(None),
#     content_tags: Optional[List[str]] = Form(None),
#     limit: int = Form(10)
# ):
#     """–ü–æ–∏—Å–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
#     logger.info(f"üîç [OPTIMIZED_SEARCH] Contextual search requested: {query}")
#     
#     try:
#         if not rag_service.optimized_indexer:
#             raise HTTPException(status_code=503, detail="Optimized indexer not available")
#         
#         # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
#         results = rag_service.optimized_indexer.search_with_context_filter(
#             query=query,
#             document_mark=document_mark,
#             document_stage=document_stage,
#             content_tags=content_tags,
#             limit=limit
#         )
#         
#         logger.info(f"‚úÖ [OPTIMIZED_SEARCH] Found {len(results)} results")
#         return {
#             "status": "success",
#             "query": query,
#             "filters": {
#                 "document_mark": document_mark,
#                 "document_stage": document_stage,
#                 "content_tags": content_tags
#             },
#             "results": results,
#             "total_results": len(results),
#             "timestamp": datetime.now().isoformat()
#         }
#         
#     except Exception as e:
#         logger.error(f"‚ùå [OPTIMIZED_SEARCH] Error: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.get("/optimized/statistics")
# async def get_optimized_statistics():
#     """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
#     logger.info("üìä [OPTIMIZED_STATS] Statistics requested")
#     
#     try:
#         if not rag_service.optimized_indexer:
#             raise HTTPException(status_code=503, detail="Optimized indexer not available")
#         
#         stats = rag_service.optimized_indexer.get_statistics()
#         
#         logger.info("‚úÖ [OPTIMIZED_STATS] Statistics retrieved successfully")
#         return {
#             "status": "success",
#             "statistics": stats,
#             "timestamp": datetime.now().isoformat()
#         }
#         
#     except Exception as e:
#         logger.error(f"‚ùå [OPTIMIZED_STATS] Error: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
