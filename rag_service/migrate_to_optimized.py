"""
–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É RAG –±–∞–∑—ã
"""

import os
import json
import logging
from typing import List, Dict, Any
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import qdrant_client

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_URL = os.getenv("POSTGRES_URL", "postgresql://norms_user:norms_password@norms-db:5432/norms_db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")

class OptimizedMigrationService:
    """–°–µ—Ä–≤–∏—Å –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
    
    def __init__(self):
        self.db_conn = None
        self.qdrant_client = None
        self.connect_services()
    
    def connect_services(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–µ—Ä–≤–∏—Å–∞–º"""
        try:
            # PostgreSQL
            self.db_conn = psycopg2.connect(POSTGRES_URL)
            logger.info("‚úÖ Connected to PostgreSQL")
            
            # Qdrant
            self.qdrant_client = qdrant_client.QdrantClient(url=QDRANT_URL)
            logger.info("‚úÖ Connected to Qdrant")
            
        except Exception as e:
            logger.error(f"‚ùå Connection error: {e}")
            raise
    
    def migrate_existing_documents(self):
        """–ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
        logger.info("üöÄ Starting migration to optimized structure...")
        
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                cursor.execute("""
                    SELECT 
                        nd.id,
                        nd.document_type,
                        nd.document_number,
                        nd.document_name,
                        nd.version,
                        nd.publication_date,
                        nd.effective_date,
                        nd.status
                    FROM normative_documents nd
                    ORDER BY nd.id
                """)
                
                documents = cursor.fetchall()
                logger.info(f"üìÑ Found {len(documents)} documents to migrate")
                
                for doc in documents:
                    logger.info(f"üìÑ Migrating document: {doc['document_number']} - {doc['document_name']}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—É–Ω–∫—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    cursor.execute("""
                        SELECT 
                            dc.id,
                            dc.clause_id,
                            dc.clause_number,
                            dc.section_title,
                            dc.clause_title,
                            dc.clause_text,
                            dc.clause_type,
                            dc.importance_level,
                            dc.page_number
                        FROM document_clauses dc
                        WHERE dc.document_id = %s
                        ORDER BY dc.clause_number
                    """, (doc['id'],))
                    
                    clauses = cursor.fetchall()
                    logger.info(f"üìù Found {len(clauses)} clauses for document {doc['document_number']}")
                    
                    # –ú–∏–≥—Ä–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç
                    for clause in clauses:
                        self.migrate_clause_to_optimized(doc, clause)
                
                logger.info("‚úÖ Migration completed successfully")
                
        except Exception as e:
            logger.error(f"‚ùå Migration error: {e}")
            self.db_conn.rollback()
            raise
    
    def migrate_clause_to_optimized(self, document: Dict[str, Any], clause: Dict[str, Any]):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
        try:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            unified_structure = self.create_unified_structure(document, clause)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_tags = self.detect_content_tags(clause['clause_text'])
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
            semantic_chunks = self.create_semantic_chunks(clause['clause_text'], clause['clause_id'])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            self.save_optimized_structure(unified_structure, semantic_chunks, content_tags)
            
            logger.debug(f"‚úÖ Migrated clause: {clause['clause_id']}")
            
        except Exception as e:
            logger.error(f"‚ùå Error migrating clause {clause['clause_id']}: {e}")
            raise
    
    def create_unified_structure(self, document: Dict[str, Any], clause: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        return {
            'clause_id': clause['clause_id'],
            'section_title': clause['section_title'] or '',
            'full_text': clause['clause_text'],
            'document_type': document['document_type'],
            'document_number': document['document_number'],
            'clause_number': clause['clause_number'],
            'importance_level': clause['importance_level'] or 1,
            'metadata': {
                'original_document_id': document['id'],
                'original_clause_id': clause['id'],
                'page_number': clause['page_number'],
                'clause_type': clause['clause_type'],
                'migrated_at': datetime.now().isoformat()
            }
        }
    
    def detect_content_tags(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        tags = []
        text_lower = text.lower()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if any(word in text_lower for word in ['–ø–æ–¥–ø–∏—Å—å', '–ø–æ–¥–ø–∏—Å–∞–Ω', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω', '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω']):
            tags.append('–ø–æ–¥–ø–∏—Å–∏')
        
        if any(word in text_lower for word in ['–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ', '—Ñ–æ—Ä–º–∞—Ç', '—à—Ä–∏—Ñ—Ç', '—Ä–∞–∑–º–µ—Ä']):
            tags.append('–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ')
        
        if any(word in text_lower for word in ['—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '—Å–æ—Å—Ç–∞–≤', '—Ä–∞–∑–¥–µ–ª', '–≥–ª–∞–≤–∞']):
            tags.append('—Å—Ç—Ä—É–∫—Ç—É—Ä–∞')
        
        if any(word in text_lower for word in ['—Ç–∞–±–ª–∏—Ü–∞', '—Ç–∞–±–ª.', '—Ç–∞–±.']):
            tags.append('—Ç–∞–±–ª–∏—Ü—ã')
        
        if any(word in text_lower for word in ['—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∏—Å.', '—á–µ—Ä—Ç–µ–∂', '—Å—Ö–µ–º–∞']):
            tags.append('–≥—Ä–∞—Ñ–∏–∫–∞')
        
        if any(word in text_lower for word in ['—Ä–∞—Å—á–µ—Ç', '–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ', '—Ñ–æ—Ä–º—É–ª–∞']):
            tags.append('—Ä–∞—Å—á–µ—Ç—ã')
        
        if any(word in text_lower for word in ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è']):
            tags.append('—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è')
        
        if any(word in text_lower for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
            tags.append('—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏')
        
        if any(word in text_lower for word in ['–Ω–∞–ø—Ä–∏–º–µ—Ä', '–ø—Ä–∏–º–µ—Ä', '–æ–±—Ä–∞–∑–µ—Ü']):
            tags.append('–ø—Ä–∏–º–µ—Ä—ã')
        
        if any(word in text_lower for word in ['–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '–ø—Ä–∏–º.', '–∑–∞–º–µ—á–∞–Ω–∏–µ']):
            tags.append('–ø—Ä–∏–º–µ—á–∞–Ω–∏—è')
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ "—Ç–µ–∫—Å—Ç"
        if not tags:
            tags.append('—Ç–µ–∫—Å—Ç')
        
        return tags
    
    def create_semantic_chunks(self, text: str, clause_id: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            if not paragraph.strip():
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            chunk_type = self.detect_chunk_type(paragraph)
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
            chunk = {
                'chunk_id': f"{clause_id}_chunk_{i}",
                'clause_id': clause_id,
                'content': paragraph,
                'chunk_type': chunk_type,
                'semantic_context': f"clause_id: {clause_id}, paragraph: {i+1}/{len(paragraphs)}",
                'metadata': {
                    'paragraph_index': i,
                    'total_paragraphs': len(paragraphs),
                    'token_count': len(paragraph.split()),
                    'created_at': datetime.now().isoformat()
                }
            }
            chunks.append(chunk)
        
        return chunks
    
    def detect_chunk_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —á–∞–Ω–∫–∞"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['–¥–æ–ª–∂–µ–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è']):
            return "requirement"
        
        if any(word in text_lower for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ']):
            return "recommendation"
        
        if any(word in text_lower for word in ['–Ω–∞–ø—Ä–∏–º–µ—Ä', '–ø—Ä–∏–º–µ—Ä', '–æ–±—Ä–∞–∑–µ—Ü']):
            return "example"
        
        if any(word in text_lower for word in ['–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '–ø—Ä–∏–º.', '–∑–∞–º–µ—á–∞–Ω–∏–µ']):
            return "note"
        
        return "text"
    
    def save_optimized_structure(self, unified_structure: Dict[str, Any], 
                               semantic_chunks: List[Dict[str, Any]], 
                               content_tags: List[str]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                cursor.execute("""
                    INSERT INTO document_clauses 
                    (clause_id, clause_number, section_title, clause_text, clause_type, importance_level, metadata)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (clause_id) DO UPDATE SET
                    clause_text = EXCLUDED.clause_text,
                    importance_level = EXCLUDED.importance_level,
                    metadata = EXCLUDED.metadata,
                    updated_at = CURRENT_TIMESTAMP
                """, (
                    unified_structure['clause_id'],
                    unified_structure['clause_number'],
                    unified_structure['section_title'],
                    unified_structure['full_text'],
                    'requirement',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    unified_structure['importance_level'],
                    json.dumps(unified_structure['metadata'])
                ))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã (—Ç–µ–≥–∏)
                for tag in content_tags:
                    cursor.execute("""
                        INSERT INTO clause_attributes 
                        (clause_id, attribute_name, attribute_value, attribute_type)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT DO NOTHING
                    """, (
                        unified_structure['clause_id'],
                        'tag',
                        tag,
                        'text'
                    ))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–∞—Ö
                for chunk in semantic_chunks:
                    cursor.execute("""
                        INSERT INTO normative_chunks 
                        (chunk_id, clause_id, content, chunk_type, metadata)
                        VALUES (%s, %s, %s, %s, %s)
                        ON CONFLICT (chunk_id) DO UPDATE SET
                        content = EXCLUDED.content,
                        chunk_type = EXCLUDED.chunk_type,
                        metadata = EXCLUDED.metadata,
                        updated_at = CURRENT_TIMESTAMP
                    """, (
                        chunk['chunk_id'],
                        chunk['clause_id'],
                        chunk['content'],
                        chunk['chunk_type'],
                        json.dumps(chunk['metadata'])
                    ))
                
                self.db_conn.commit()
                logger.debug(f"‚úÖ Saved optimized structure for clause: {unified_structure['clause_id']}")
                
        except Exception as e:
            logger.error(f"‚ùå Error saving optimized structure: {e}")
            self.db_conn.rollback()
            raise
    
    def create_optimized_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        logger.info("üîß Creating optimized tables...")
        
        try:
            with self.db_conn.cursor() as cursor:
                # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —á–∞–Ω–∫–æ–≤
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS normative_chunks (
                        id SERIAL PRIMARY KEY,
                        chunk_id VARCHAR(255) UNIQUE NOT NULL,
                        clause_id VARCHAR(100) NOT NULL,
                        content TEXT NOT NULL,
                        chunk_type VARCHAR(50),
                        semantic_context TEXT,
                        metadata JSONB,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_normative_chunks_clause_id 
                    ON normative_chunks(clause_id)
                """)
                
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_normative_chunks_chunk_type 
                    ON normative_chunks(chunk_type)
                """)
                
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_normative_chunks_metadata 
                    ON normative_chunks USING GIN(metadata)
                """)
                
                # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS document_metadata (
                        id SERIAL PRIMARY KEY,
                        document_id INTEGER REFERENCES normative_documents(id),
                        document_mark VARCHAR(50),
                        document_stage VARCHAR(50),
                        content_tags TEXT[],
                        processing_status VARCHAR(50) DEFAULT 'pending',
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                self.db_conn.commit()
                logger.info("‚úÖ Optimized tables created successfully")
                
        except Exception as e:
            logger.error(f"‚ùå Error creating optimized tables: {e}")
            self.db_conn.rollback()
            raise
    
    def get_migration_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–∏"""
        try:
            with self.db_conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                cursor.execute("SELECT COUNT(*) as total_documents FROM normative_documents")
                total_documents = cursor.fetchone()['total_documents']
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—É–Ω–∫—Ç–æ–≤
                cursor.execute("SELECT COUNT(*) as total_clauses FROM document_clauses")
                total_clauses = cursor.fetchone()['total_clauses']
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–æ–≤
                cursor.execute("SELECT COUNT(*) as total_chunks FROM normative_chunks")
                total_chunks = cursor.fetchone()['total_chunks']
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                cursor.execute("SELECT COUNT(*) as total_attributes FROM clause_attributes")
                total_attributes = cursor.fetchone()['total_attributes']
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                cursor.execute("""
                    SELECT document_type, COUNT(*) as count 
                    FROM normative_documents 
                    GROUP BY document_type
                """)
                document_types = cursor.fetchall()
                
                return {
                    'total_documents': total_documents,
                    'total_clauses': total_clauses,
                    'total_chunks': total_chunks,
                    'total_attributes': total_attributes,
                    'document_types': {row['document_type']: row['count'] for row in document_types},
                    'migration_status': 'completed',
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå Error getting migration statistics: {e}")
            return {'error': str(e)}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏"""
    logger.info("üöÄ Starting RAG Base optimization migration...")
    
    try:
        migration_service = OptimizedMigrationService()
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        migration_service.create_optimized_tables()
        
        # –ú–∏–≥—Ä–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        migration_service.migrate_existing_documents()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = migration_service.get_migration_statistics()
        logger.info(f"üìä Migration statistics: {json.dumps(stats, indent=2, ensure_ascii=False)}")
        
        logger.info("‚úÖ Migration completed successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Migration failed: {e}")
        raise

if __name__ == "__main__":
    main()
