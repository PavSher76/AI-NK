# –ü–ª–∞–Ω –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ VLLM —Å–µ—Ä–≤–∏—Å–∞

## üéØ –¶–µ–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

### 1. –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è LLM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –ø—Ä—è–º—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ Ollama –Ω–∞ VLLM API
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM
- –£–ø—Ä–æ—Å—Ç–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### 2. –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LLM –∞–Ω–∞–ª–∏–∑–∞ –≤ Document Parser
- –£–ª—É—á—à–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π RAG —Å–µ—Ä–≤–∏—Å–∞

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
- Batch processing
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### 1. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   VLLM Adapter  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Ollama        ‚îÇ
‚îÇ   (—É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω)‚îÇ    ‚îÇ   (—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω)‚îÇ    ‚îÇ   (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è
```python
# gateway/app.py - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
SERVICES = {
    "document-parser": "http://document-parser:8001",
    "rag-service": "http://rag-service:8003",
    "rule-engine": "http://rule-engine:8004",
    "calculation-service": "http://calculation-service:8002",
    "vllm": "http://vllm:8000",
    "ollama": "http://ollama:11434"  # —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
}

# –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM endpoint
@app.api_route("/llm/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])
async def proxy_llm(request: Request, path: str):
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM endpoint"""
    service_url = SERVICES["vllm"]
    return await proxy_request(request, service_url, f"/api/{path}")
```

## üìã –ü–ª–∞–Ω –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–æ —Å–µ—Ä–≤–∏—Å–∞–º

### 1. Document Parser - –ù–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—å

#### –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
```python
# document_parser/services/norm_control_service.py
async def perform_norm_control_check_for_page(self, document_id: int, page_data: Dict[str, Any]):
    # ‚ùå –ó–∞–≥–ª—É—à–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    findings = []
    # ... —Ñ–∏–∫—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
```

#### –¶–µ–ª–µ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
```python
# document_parser/services/norm_control_service.py
async def perform_norm_control_check_for_page(self, document_id: int, page_data: Dict[str, Any]):
    """LLM –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        prompt_template = await self.get_normcontrol_prompt_template()
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        formatted_prompt = prompt_template.format(
            page_content=page_data["content"],
            page_number=page_data["page_number"],
            document_id=document_id
        )
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ VLLM —á–µ—Ä–µ–∑ Gateway
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{GATEWAY_URL}/llm/chat/completions",
                headers={"Authorization": f"Bearer {auth_token}"},
                json={
                    "model": "llama3.1:8b",
                    "messages": [
                        {
                            "role": "system",
                            "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."
                        },
                        {
                            "role": "user",
                            "content": formatted_prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 2000
                },
                timeout=120.0
            )
            
            if response.status_code == 200:
                data = response.json()
                llm_response = data["choices"][0]["message"]["content"]
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                findings = self.parse_llm_response(llm_response)
                
                return {
                    "status": "success",
                    "findings": findings,
                    "llm_response": llm_response
                }
            else:
                logger.error(f"LLM request failed: {response.status_code}")
                return {"status": "error", "error": "LLM request failed"}
                
    except Exception as e:
        logger.error(f"LLM analysis error: {e}")
        return {"status": "error", "error": str(e)}
```

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- ‚úÖ –†–µ–∞–ª—å–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ –∑–∞–≥–ª—É—à–µ–∫
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
- ‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π API —á–µ—Ä–µ–∑ VLLM
- ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ —Ç–∞–π–º–∞—É—Ç–æ–≤

### 2. RAG Service - –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏

#### –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
```python
# rag_service/ntd_consultation.py
async def _generate_response(self, question: str, context: str):
    # ‚ùå –ü—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ Ollama
    response = await client.post(
        f"{self.ollama_url}/api/generate",
        json={
            "model": "llama3.2:3b",
            "prompt": prompt,
            # ...
        }
    )
```

#### –¶–µ–ª–µ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
```python
# rag_service/ntd_consultation.py
async def _generate_response(self, question: str, context: str, history: List[Dict[str, str]] = None):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ VLLM"""
    try:
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt = self._build_prompt(question, context, history)
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ VLLM —á–µ—Ä–µ–∑ Gateway
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{GATEWAY_URL}/llm/chat/completions",
                headers={"Authorization": f"Bearer {self.auth_token}"},
                json={
                    "model": "llama3.1:8b",
                    "messages": [
                        {
                            "role": "system",
                            "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.7,
                    "max_tokens": 2000
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                logger.error(f"VLLM request failed: {response.status_code}")
                return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
                
    except Exception as e:
        logger.error(f"VLLM generation error: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
```

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- ‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π API —á–µ—Ä–µ–∑ VLLM
- ‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å (llama3.1:8b –≤–º–µ—Å—Ç–æ llama3.2:3b)
- ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- ‚úÖ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞

### 3. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

#### –ù–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
```python
# document_parser/services/hierarchical_check_service.py
class HierarchicalCheckService:
    async def perform_hierarchical_check(self, document_id: int) -> Dict[str, Any]:
        """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º VLLM"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = await self.get_document(document_id)
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            structure_analysis = await self.analyze_document_structure(document)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–∞–∑–¥–µ–ª–æ–≤
            section_analysis = await self.analyze_sections(document, structure_analysis)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraph_analysis = await self.analyze_paragraphs(document, section_analysis)
            
            # –°–∏–Ω—Ç–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_result = await self.synthesize_results(
                structure_analysis, section_analysis, paragraph_analysis
            )
            
            return final_result
            
        except Exception as e:
            logger.error(f"Hierarchical check error: {e}")
            return {"status": "error", "error": str(e)}
    
    async def analyze_document_structure(self, document: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ VLLM"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—ã–¥–µ–ª–∏—Ç–µ:
        1. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
        2. –ò–µ—Ä–∞—Ä—Ö–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        3. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏
        4. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è
        
        –î–æ–∫—É–º–µ–Ω—Ç: {document['content'][:5000]}
        """
        
        response = await self.llm_request(prompt, "structure_analysis")
        return self.parse_structure_response(response)
    
    async def analyze_sections(self, document: Dict, structure: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        results = []
        for section in structure.get("sections", []):
            prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–¥–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:
            
            –†–∞–∑–¥–µ–ª: {section['title']}
            –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {section['content']}
            
            –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:
            1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫—É
            2. –õ–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            3. –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            4. –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–ª–æ–∂–µ–Ω–∏—è
            """
            
            response = await self.llm_request(prompt, "section_analysis")
            results.append(self.parse_section_response(response, section))
        
        return {"sections": results}
    
    async def llm_request(self, prompt: str, analysis_type: str) -> str:
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ VLLM"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{GATEWAY_URL}/llm/chat/completions",
                headers={"Authorization": f"Bearer {self.auth_token}"},
                json={
                    "model": "llama3.1:8b",
                    "messages": [
                        {
                            "role": "system",
                            "content": f"–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í—ã–ø–æ–ª–Ω—è–π—Ç–µ {analysis_type}."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 3000
                },
                timeout=180.0
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                raise Exception(f"LLM request failed: {response.status_code}")
```

## üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 1. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤

#### Redis –∫—ç—à –¥–ª—è VLLM:
```python
# ollama_adapter/cache.py
import redis
import hashlib
import json
from typing import Optional

class VLLMCache:
    def __init__(self):
        self.redis_client = redis.Redis(
            host='redis',
            port=6379,
            db=1,  # –û—Ç–¥–µ–ª—å–Ω–∞—è –ë–î –¥–ª—è VLLM –∫—ç—à–∞
            decode_responses=True
        )
        self.ttl = 3600  # 1 —á–∞—Å
    
    def get_cache_key(self, model: str, messages: list, temperature: float) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        content = json.dumps({
            "model": model,
            "messages": messages,
            "temperature": temperature
        }, sort_keys=True)
        return f"vllm:{hashlib.md5(content.encode()).hexdigest()}"
    
    async def get_cached_response(self, model: str, messages: list, temperature: float) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        cache_key = self.get_cache_key(model, messages, temperature)
        cached = self.redis_client.get(cache_key)
        return cached
    
    async def cache_response(self, model: str, messages: list, temperature: float, response: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∫—ç—à"""
        cache_key = self.get_cache_key(model, messages, temperature)
        self.redis_client.setex(cache_key, self.ttl, response)
```

#### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ VLLM Adapter:
```python
# ollama_adapter/ollama_adapter.py
from cache import VLLMCache

cache = VLLMCache()

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    body = await request.json()
    messages = body.get("messages", [])
    model = body.get("model", "llama2:7b")
    temperature = body.get("temperature", 0.7)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    cached_response = await cache.get_cached_response(model, messages, temperature)
    if cached_response:
        logger.info("Cache hit for LLM request")
        return json.loads(cached_response)
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    await cache.cache_response(model, messages, temperature, json.dumps(openai_response))
    
    return openai_response
```

### 2. Batch Processing

#### Batch API endpoint:
```python
# ollama_adapter/ollama_adapter.py
@app.post("/v1/chat/completions/batch")
async def chat_completions_batch(request: Request):
    """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    body = await request.json()
    requests = body.get("requests", [])
    
    results = []
    for req in requests:
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            response = await process_single_request(req)
            results.append({
                "status": "success",
                "response": response
            })
        except Exception as e:
            results.append({
                "status": "error",
                "error": str(e)
            })
    
    return {"results": results}
```

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤

#### –ü—Ä–æ–º–ø—Ç –º–µ–Ω–µ–¥–∂–µ—Ä:
```python
# ollama_adapter/prompt_manager.py
class PromptManager:
    def __init__(self):
        self.prompt_templates = {
            "norm_control": {
                "system": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                "user_template": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: {content}"
            },
            "document_analysis": {
                "system": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                "user_template": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: {content}"
            },
            "consultation": {
                "system": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
                "user_template": "–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {question}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}"
            }
        }
    
    def get_optimized_prompt(self, prompt_type: str, **kwargs) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        template = self.prompt_templates.get(prompt_type)
        if not template:
            raise ValueError(f"Unknown prompt type: {prompt_type}")
        
        return {
            "system": template["system"],
            "user": template["user_template"].format(**kwargs)
        }
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### 1. VLLM –º–µ—Ç—Ä–∏–∫–∏
```python
# ollama_adapter/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# –ú–µ—Ç—Ä–∏–∫–∏
vllm_requests_total = Counter('vllm_requests_total', 'Total VLLM requests', ['model', 'endpoint'])
vllm_request_duration = Histogram('vllm_request_duration_seconds', 'VLLM request duration', ['model'])
vllm_cache_hits = Counter('vllm_cache_hits_total', 'VLLM cache hits')
vllm_cache_misses = Counter('vllm_cache_misses_total', 'VLLM cache misses')
vllm_active_requests = Gauge('vllm_active_requests', 'Active VLLM requests')

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    if request.url.path.startswith("/v1/"):
        vllm_active_requests.inc()
        start_time = time.time()
        
        response = await call_next(request)
        
        duration = time.time() - start_time
        vllm_request_duration.observe(duration)
        vllm_active_requests.dec()
        
        return response
    
    return await call_next(request)
```

### 2. Health Check —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
```python
# ollama_adapter/ollama_adapter.py
@app.get("/health")
async def health():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π health check"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama
        async with httpx.AsyncClient() as client:
            ollama_response = await client.get(f"{OLLAMA_URL}/api/tags", timeout=5.0)
            ollama_healthy = ollama_response.status_code == 200
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Redis –∫—ç—à–∞
        redis_healthy = cache.redis_client.ping()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models = await get_available_models()
        
        return {
            "status": "healthy" if ollama_healthy and redis_healthy else "unhealthy",
            "components": {
                "ollama": "healthy" if ollama_healthy else "unhealthy",
                "redis_cache": "healthy" if redis_healthy else "unhealthy"
            },
            "models": models,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
```

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

### 1. –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Docker Compose
```yaml
# docker-compose.yaml
vllm:
  build: 
    context: ./ollama_adapter
    dockerfile: Dockerfile.optimized
  ports:
    - "8000:8000"
  environment:
    - OLLAMA_BASE_URL=http://ollama:11434
    - OLLAMA_MODEL=llama3.1:8b
    - REDIS_URL=redis://redis:6379/1
    - CACHE_TTL=3600
    - ENABLE_CACHE=true
    - ENABLE_METRICS=true
    - LOG_LEVEL=INFO
  volumes:
    - ./ollama_adapter/prompts:/app/prompts:ro
  depends_on:
    - ollama
    - redis
  restart: unless-stopped
  deploy:
    resources:
      limits:
        memory: 2G
        cpus: '1.5'
      reservations:
        memory: 1G
        cpus: '0.75'
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
```

### 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
# .env
VLLM_MODEL=llama3.1:8b
VLLM_CACHE_TTL=3600
VLLM_ENABLE_CACHE=true
VLLM_ENABLE_METRICS=true
VLLM_REQUEST_TIMEOUT=120
VLLM_MAX_TOKENS=2000
VLLM_TEMPERATURE=0.3
```

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### 1. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ:** 40-60% —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- **Batch processing:** 30-50% —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤:** 20-30% —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

### 2. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
- **Document Parser:** –†–µ–∞–ª—å–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ –∑–∞–≥–ª—É—à–µ–∫
- **RAG Service:** –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
- **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞:** –ù–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

### 3. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:** –ü–æ–ª–Ω–∞—è –≤–∏–¥–∏–º–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã VLLM
- **Health checks:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–±–æ–µ–≤

## üöÄ –ü–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è

### –≠—Ç–∞–ø 1: –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (1-2 –Ω–µ–¥–µ–ª–∏)
1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ VLLM Adapter —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ Document Parser
3. –ë–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –≠—Ç–∞–ø 2: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (2-3 –Ω–µ–¥–µ–ª–∏)
1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ RAG Service
2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
3. Batch processing

### –≠—Ç–∞–ø 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (1-2 –Ω–µ–¥–µ–ª–∏)
1. –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
3. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –≠—Ç–∞–ø 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ (1 –Ω–µ–¥–µ–ª—è)
1. –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
2. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
3. –ü—Ä–æ–¥–∞–∫—à–Ω —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è VLLM —Å–µ—Ä–≤–∏—Å–∞ –≤ –ø—Ä–æ–µ–∫—Ç AI-NK –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:

1. **–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è** - –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö LLM –æ–ø–µ—Ä–∞—Ü–∏–π
2. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** - –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
3. **–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å** - —Ä–µ–∞–ª—å–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –≤–æ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–∞—Ö
4. **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
5. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ç–æ–≥–æ –ø–ª–∞–Ω–∞ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç VLLM –∏–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞.
